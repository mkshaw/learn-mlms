[["index.html", "Introduction to Multilevel Modelling Chapter 1 Introduction 1.1 Overview 1.2 Goals 1.3 Prerequisites 1.4 Materials", " Introduction to Multilevel Modelling Mairead Shaw and Jessica Kay Flake Chapter 1 Introduction 1.1 Overview These materials focus on conceptual foundations of multilevel models (MLMs), specifiying them, and interpreting the results. Topics include multilevel data and approaches to dependence, specifying and interpreting fixed and random effects, model estimation, centering, repeated measures and longitudinal models, assumptions testing, and effect sizes in MLMs. 1.2 Goals These materials are intended for students and instructors. By the end of this course, students will be able to: Estimate variance components and interpret the intraclass correlation coefficient Decide if and when a multilevel model is needed Specify and build multilevel models with covariates at level 1 and 2 with both cross-sectional and repeated measures designs Interpret regression coefficients and variance components from multilevel models Assess the assumptions of multilevel models Calculate effect sizes for multilevel models For instructors, all of the chapters, datasets, demonstration steps, and code are available for reuse under a CC BY 4.0 license. 1.3 Prerequisites Readers should be comfortable with multiple linear regression, including building regression models, interpreting regression output, and testing for and interpreting regression coefficients including interactions. The first module reviews multiple regression and can be used to gauge your preparedness for continuing. For those wishing to brush up their regression skills before working through these materials, we recommend UCLAs Statistical Methods and Data Analytics resources and online seminars: https://stats.oarc.ucla.edu/other/mult-pkg/seminars/ The worked examples will be conducted using lme4 in R. The lme4 documentation provides details of the workings of lme4, for interested readers. 1.4 Materials If you are a student interested in following along with the data analysis demonstrations, you can download the data files here. If you are an instructor interested in using these files to create your own teaching materials, you can find data, blank worksheets, and R scripts here. "],["04-module-4.html", "Chapter 4 Our First Multilevel Models 4.1 Learning Objectives 4.2 Data Demonstration 4.3 Conclusion", " Chapter 4 Our First Multilevel Models 4.1 Learning Objectives In this chapter, we will run our first multilevel models to account for the clustered nature of our data and begin asking multilevel research questions. The learning objectives for this chapter are: Visualize clustering in data structures; Recognize when to use a multilevel model over cluster-robust standard errors; Explain the difference between fixed and random effects in MLMs; Code and interpret the null model; Determine how variance is distributed in a dataset. All materials for this chapter are available for download here. 4.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the dataset. 4.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for visualizations library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for intraclass correlation And the same dataset of students math achievement from chapters 2 and 3: data &lt;- read.csv(&#39;heck2011.csv&#39;) 4.2.2 Why Multilevel Models? In chapter 3 we talked about cluster-robust standard errors, which handle clustering as a nuisance, but doesnt investigate it as interesting or enable multilevel research questions. But if youre interested in the clustered structure of the data, and you want to know how clusters differ or ask questions at multiple levels, youll need a multilevel model. To get a sense of how the outcome is clustered, lets start by making some graphs. Our data set contains students clustered into 419 schools. For demonstration, we will take a subset of 10 schools. data_sub &lt;- data %&gt;% filter(schcode &lt;= 10) In chapter 2, we created a scatterplot with math achievement (math) on the y-axis and socioeconomic status (ses) on the x-axis. Lets re-create that graph and overlay a line of best fit, first ignoring the clustering. data_sub %&gt;% ggplot(mapping = aes(x = ses, y = math)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; Now, lets create the same scatterplot, this time colouring the points by school (schcode). data_sub %&gt;% ggplot(mapping = aes(x = ses, y = math, colour = factor(schcode))) + geom_point() + geom_smooth(mapping = aes(group = schcode), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + labs(colour = &quot;schcode&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; When we run a regular regression without accounting for the clustered structure of the data, we treat all schools as though they have the same intercept and slope. What do you notice about the intercepts and slopes for different schools? The intercepts and slopes vary widely! For example, school 4 has an intercept around 62, compared to the intercept of school 3 around 49. School 8 has a relatively large positive slope, while school 6 has a slightly negative slope. With multilevel models, we can quantify this variance and ask questions about it. 4.2.3 Fixed vs Random Effects Multilevel models have two main ingredients: fixed and random effects. For our purposes of executing and interpreting MLMs, a fixed effect is an average effect across all clusters and a random effect is a variance that describes how much an effect differs across clusters. Generally, fixed effects in MLMs capture the mean of an effect and the random effect captures the variance of an effect. For example, we might have a fixed effect for the intercept that describes average math achievement across all schools. Then we have a random effect that describes how intercepts for math achievement vary across schools. Together, the fixed and random effect describe math achievement scores across schools. 4.2.4 The Null Model In the simplest MLM we can run, we let intercepts vary between clusters by estimating random effects for the intercepts in addition to a fixed effect. The random effect allows the intercepts to randomly vary about the fixed effect, the grand mean of the intercepts. As a result, this model is called the random intercept only model, also known as the null model. For our example, math achievement is our outcome variable, and the equations for the null model look like this: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) Combined \\(math_{ij} = \\gamma_{00} + U_{0j} + R_{ij}\\) Lets run the model. null_model &lt;- lmer(math ~ 1 + (1|schcode), data = data) summary(null_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48877.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6336 -0.5732 0.1921 0.6115 5.2989 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 10.64 3.262 ## Residual 66.55 8.158 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6742 0.1883 416.0655 306.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In lme4, the syntax for a two-level model is lmer(DV ~ 1 + IV1 + IV2 + ... + IVp + (random_effect1 + random_effect2 + ... + random_effect3 | grouping_variable), data = dataset). Our dependent variable is math achievement (math), and we have a fixed and random effect of the intercept (represented by the 1s). Our grouping variable is school (schcode). The key output to interpret is: Number of parameters Estimates of fixed effects Estimates of variances, which are the random effects As indicated in our combined equation, we are estimating three parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools around the intercept. Each \\(U_{0j}\\) is the residual of a school around the intercept; that is, it describes how the schools mean math achievement at the intercept deviates from the intercept for the entire sample. Every school has a \\(U_{0j}\\), and the variance of all of the \\(U_{0j}\\)s is \\(\\tau_0^2\\); \\(\\sigma^2\\): a random effect capturing the variance of students around their school mean math achievement. Each student has a residual, \\(R_{ij}\\), and the variance of all the \\(R_{ij}\\)s is \\(\\sigma^2\\). The fixed effect for the intercept is 57.67, representing the average math achievement score across all schools. The variance of schools around the intercept is 10.64, and of students around their schools mean is 66.55. 4.2.5 Understanding Variance 4.2.5.1 Intraclass Correlation Coefficient (ICC) The intraclass correlation coefficient quantifies the extent of clustering in a dataset. It ranges from 0 to 1 and is the quotient of the variance between clusters to the total variance: \\(ICC = \\frac{\\tau_0^2}{\\tau_0^2 + \\sigma^2}\\). The more extensive the impact of clustering, the more variance between clusters, the larger the ICC. The ICCs can also be interpreted as (1) the proportion of variance in the outcome that is attributable to clusters or (2) the expected correlation between the outcome from randomly selected units from a randomly selected cluster. Per our model output, the total variance is \\(\\tau_0^2 + \\sigma^2 = 10.64 + 66.55 = 77.19\\), and the variance between schools is \\(\\tau_0^2\\), 10.64. The ICC is then \\(\\frac{10.64}{77.19} = 0.138\\); 13.8% of the total variance in math achievement can be attributed to school membership. We can use the performance package to calculate this automatically: performance::icc(null_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.138 ## Conditional ICC: 0.138 Dont worry about the adjusted vs conditional ICC here. In short, the adjusted ICC accounts only for the random effect variances, while the conditional ICC accounts for the fixed effect variances, too. You can read more about it here. 4.2.5.2 Plausible Values Range Another way to understand the variance in our data is by calculating a 95% plausible values range for a given effect. For example, the intercept: given the fixed effect for the intercept (\\(\\gamma_{00}\\)) and the variance of residuals around that fixed effect (\\(\\tau_0^2\\)), we can describe how much the schools vary in mean math achievement by calculating a 95% plausible values range. \\(95\\%\\ plausible\\ values\\ range = \\gamma_{00} Â± 1.96\\sqrt{\\tau_0^2}\\). Tau0 &lt;- VarCorr(null_model)$schcode[1] lower_bound &lt;- null_model@beta - 1.96*sqrt(Tau0) upper_bound &lt;- null_model@beta + 1.96*sqrt(Tau0) lower_bound ## [1] 51.28024 upper_bound ## [1] 64.06822 This range gives us a sense of the variance in school intercepts: 95% of intercepts will fall between 51 and 64. 13 points of variance is a fair amount for a scale from 0 to 100! It seems good that were accounting for that variance with our multilevel model, rather than treating all schools like they have the same intercept. 4.2.5.3 Empirical Bayes Estimates As noted, every school has its own intercept residual, \\(U_{0j}\\). Were not usually interested in individual residuals; rather, were interested in the variance of those residuals to understand the clustering in our data. But we can visualize at the individual residuals as a third way of understanding that variation. We can extract and plot the residuals as Empirical Bayes estimates, which are weighted. The random effects for the intercept (the \\(U_{0j}\\)s) are latent variables rather than statistical parameters, but we can estimate them to visualize how much they vary (and thus how much schools vary around the grand mean intercept). We can estimate a weighted intercept for a given group with the following equation: \\(\\hat\\beta_{0j}^{EB} = \\lambda_j\\hat\\beta_{0j} + (1 - \\lambda_j)\\hat\\gamma_{00}\\) To calculate the weighted intercept, we use the following information: Group mean information (\\(\\hat{\\beta}\\)); Population mean information (\\(\\hat{\\gamma_{00}}\\)); Weight \\(\\lambda_j = \\frac{\\tau_0^2}{\\tau_0^2 + \\frac{\\sigma^2}{n_j}}\\). The larger a cluster, the closer the denominator is to the numerator, the larger the weight. The Empirical Bayes estimate of a residuals, \\(U_{0j}\\), is then the difference between the fixed effect \\(\\gamma_{0j}\\) and the EB estimate \\(\\hat\\beta_{0j}^{EB}\\). To develop an intuition about EB estimates, lets manually calculate the residual for the intercept for school 1, or \\(U_{01}\\). First, we need the group mean math achievement, \\(\\hat{\\beta_{01}}\\): data %&gt;% filter(schcode == 1) %&gt;% # select only school code 1 summarize( mean(math) ) ## mean(math) ## 1 58.99677 Next, we need the estimated population mean, \\(\\hat{\\gamma_{00}}\\). We have that from our earlier MLM: the intercept of 47.6742. Finally, we need the weight: \\(\\lambda_j = \\frac{\\tau_0^2}{\\tau_0^2 + \\frac{\\sigma^2}{n_j}}\\). We also have \\(\\tau_0^2 = 10.64\\) and \\(\\sigma^2 = 66.55\\) from our earlier MLM. We need the group sample size, \\(n_1\\): data %&gt;% filter(schcode == 1) %&gt;% count() ## n ## 1 12 So \\(\\lambda_j = \\frac{\\tau_0^2}{\\tau_0^2 + \\frac{\\sigma^2}{n_j}} = \\frac{10.64}{10.64 + \\frac{66.55}{12}} = 0.657\\). Combining this information into our Empirical Bayes formula, \\(\\hat\\beta_{01}^{EB} = 58.545\\). The residual between the intercept from our MLM  57.6742  and our Empirical Bayes estimate  58.545  is 0.87. We could repeat this manual calculation process and get an Empirical Bayes residual for every school, and then plot those residuals to visualize their distribution. Luckily, we dont need to do this manual process 419 times; we can extract the Empirical Bayes estimates of the residuals using code: empirical_bayes_data &lt;- as_tibble(ranef(null_model)) We can double-check our manual calculation: head(empirical_bayes_data, 1) ## # A tibble: 1 x 5 ## grpvar term grp condval condsd ## &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 schcode (Intercept) 1 0.869 1.91 Looks like the residual for school 1 (grp 1) is 0.87, just like we manually calculated! Then, we can plot the residuals to visualize their distribution. ggplot(data = empirical_bayes_data, mapping = aes(x = condval)) + # &quot;condval&quot; is the name of the EB estimates returned by the ranef function above geom_histogram() + labs(x = &quot;EB estimate of U0j&quot;) As we would expect, the residuals have a mean of 0 because the process of estimating the model is a process of minimizing the residuals. It looks like they mostly range from -5 to 5 in a normal distribution. Again, looks like our intercepts vary fairly widely between schools, so its a good thing were modelling that variation. 4.3 Conclusion In this chapter, we discussed why and when one should use multilevel models, reviewed different ways to visualize and understand the variance in your data at different levels, and estimated our first multilevel model: the random-intercept-only model (also called the null model). In the next chapter, well start adding more fixed effects. "],["05-module-5.html", "Chapter 5 Adding Fixed Predictors to MLMs 5.1 Learning Objectives 5.2 Data Demonstration 5.3 Conclusion", " Chapter 5 Adding Fixed Predictors to MLMs 5.1 Learning Objectives In this chapter, we will introduce fixed predictors at both level-1 and level-2. The learning objectives for this chapter are: Code and interpret fixed effects in multilevel models; Explain the difference between conditional and unconditional effects; Evaluate the utility of predictors in a model by considering the information from regression coefficients and variance reduced. All materials for this chapter are available for download here. 5.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the data. 5.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for visualizations library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for intraclass correlation And the same dataset of students math achievement: data &lt;- read.csv(&#39;heck2011.csv&#39;) 5.2.2 MLM with Level-1 Predictor As a reminder, in Chapter 4 we estimated the random-intercept-only model, also called the null model: null_model &lt;- lmer(math ~ 1 + (1|schcode), data = data) summary(null_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48877.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6336 -0.5732 0.1921 0.6115 5.2989 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 10.64 3.262 ## Residual 66.55 8.158 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6742 0.1883 416.0655 306.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now that weve explored the null model and variance decomposition it gives us access to, lets practice adding a level-1 predictor to our model. Level-1 predictors vary at level-1, which in our example is the student level, meaning that students have different values for a variable. In our data, socioeconomic status (ses) and sex (female) vary across students, at level-1. Lets add a fixed effect for ses as a predictor to our model. The following equations describe this model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{1j}ses_{ij} + U_{0j} + R_{ij}\\) Well be estimating four parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses; \\(\\gamma_{10}\\): the fixed effect for the slope of ses; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools around the intercept, controlling for ses; \\(\\sigma^2\\): a random effect capturing the variance of students around their school mean math achievement, controlling for ses. Notice that the parameters are now conditional on ses. The intercept is no longer interpreted as the intercept across all schools; its the intercept across all schools conditional on ses being equal to 0, or at the mean ses level for the sample given that ses is z-scored in these data. Additionally, note that there is no \\(U_j\\) term associated with the coefficient for ses; thats because were only adding a fixed effect for ses right now. This implies that the relationship between ses and math achievement is the same across all schools (i.e., the slope is fixed, not randomly varying). Well look at adding random slope effects in the next chapter. For now, lets run our model. ses_l1 &lt;- lmer(math ~ ses + (1|schcode), data = data, REML = TRUE) summary(ses_l1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ ses + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48215.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7733 -0.5540 0.1303 0.6469 5.6908 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.469 1.863 ## Residual 62.807 7.925 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5960 0.1329 375.6989 433.36 &lt;2e-16 *** ## ses 3.8739 0.1366 3914.6382 28.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.025 Per the intercept, the average math achievement across all schools at mean ses is 57.596. A one-standard-deviation increase in ses is associated with a 3.87-point increase in math achievement. The variance term describing how schools vary around the intercept is 3.469, whereas the variance term describing how the students vary within schools, about their schools mean, is 62.807. These variance terms are different from our null model that had no predictors; we can quantify that difference in at least two ways. One option is to calculate how much level-1 variance was reduced by adding ses as a level-1 predictor. If we divide the difference between our null models level-1 variance and this new models (l1) level-1 variance by the null model variance, we can see what proportion of variance was reduced. null &lt;- sigma(null_model)^2 l1 &lt;- sigma(ses_l1)^2 (null - l1) / null ## [1] 0.05624991 So we reduced about 5.6% of level-1 variance by adding ses as a level-1 predictor. Another way of stating this is that we reduced the unexplained within school variance by 5.6%. Another option is to calculate the conditional ICC, or the proportion of variance explained by clustering after we account for ses. Recall from last chapter that the adjusted ICC accounts only for random effects, while the conditional ICC accounts for both random effects and fixed effects. With the null model, the adjusted and conditional ICC values from performance are the same because there are no predictors in the model, but with a fixed level-1 predictor in the model, we should reference the conditional ICC. performance::icc(ses_l1) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.052 ## Conditional ICC: 0.046 After accounting for the effect of socioeconomic status, 4.6% of the variance in math achievement is accounted for by school membership. 5.2.3 Compare Regular and Multilevel Regression In the previous chapter, we compared a regular regression to a cluster-robust standard error regression. Now, lets compare those two with a multilevel model. The regular regression from Chapter 4: model &lt;- lm(math ~ ses, data = data) summary(model) ## ## Call: ## lm(formula = math ~ ses, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.459 -4.678 1.144 5.355 47.560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 57.59817 0.09819 586.61 &lt;2e-16 *** ## ses 4.25468 0.12566 33.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.132 on 6869 degrees of freedom ## Multiple R-squared: 0.143, Adjusted R-squared: 0.1429 ## F-statistic: 1146 on 1 and 6869 DF, p-value: &lt; 2.2e-16 The cluster-robust standard error regression from Chapter 4: model_crse &lt;- lmtest::coeftest(model, vcov = sandwich::vcovCL, cluster = ~ schcode) model_crse ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 57.59817 0.13020 442.378 &lt; 2.2e-16 *** ## ses 4.25468 0.14981 28.401 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 These two models had the same coefficients, with different significance values. This is our multilevel model: summary(ses_l1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ ses + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48215.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7733 -0.5540 0.1303 0.6469 5.6908 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.469 1.863 ## Residual 62.807 7.925 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5960 0.1329 375.6989 433.36 &lt;2e-16 *** ## ses 3.8739 0.1366 3914.6382 28.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.025 The intercepts are the same between the MLM and regular regressions, but the coefficient for ses is not. Why? The coefficient for ses represents the mean relationship between SES and math achievement across all schools, weighted by the reliability of the cluster. The weighting reflects cluster-level sample size, and thus varies from the regular regression estimates that treat all observations equally. 5.2.4 MLM with Level-2 Predictor We added ses as a level-1 predictor to explain some of the student-level variance in math achievement. Now, lets add a predictor that varies at level-2, meaning that the value is different across level 2 units, which is the school level. Level-2 predictors are different across schools but the same for all students within a school. There are three possible level-2 predictors: ses_mean: the mean SES per school (this variable is centered, well discuss centering more in Chapter 9) pro4yc: the percentage of students at a school who intend to study at a 4-year college/university public: whether the school is private (0) or public (1) This is where we begin to unlock the potential of MLMs, to ask questions about both individual differences (level-1 variables) and school differences (level-2 variables) at the same time while accounting for clustered data structures. Lets consider the role of school type in our model by adding a fixed effect for public as a predictor of our intercept. The following equations describe this model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{10}public_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}public_{j} + \\gamma_{10}ses_{ij} + U_{0j} + R_{ij}\\) A few things to note here: first, public_j only has a j subscript because only different schools (js) have different values of public. All students (is) within a school have the same value. Second, public is currently only a predictor for the intercept. In Chapter 6 well look at using level-2 variables as predictors of level-1 slopes and the cross-level interactions that result. Well be estimating five parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses and public; \\(\\gamma_{01}\\): the fixed effect for the slope of public controlling for ses \\(\\gamma_{10}\\): the fixed effect for the slope of ses controlling for public; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools around the intercept, controlling for ses and public; \\(\\sigma^2\\): a random effect capturing the variance of students around their school mean math achievement, controlling for ses and public. Notice that the parameters are conditional on both ses and on public now. Lets run our model. ses_l1_public_l2 &lt;- lmer(math ~ 1 + ses + public + (1|schcode), data = data, REML = TRUE) summary(ses_l1_public_l2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + public + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48216 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7718 -0.5541 0.1309 0.6477 5.6916 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.486 1.867 ## Residual 62.807 7.925 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.63143 0.25535 381.81733 225.693 &lt;2e-16 *** ## ses 3.87338 0.13673 3928.37427 28.329 &lt;2e-16 *** ## public -0.04859 0.29862 385.93649 -0.163 0.871 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ses ## ses 0.013 ## public -0.854 -0.031 Lets look at our fixed effects, which describes the conditional mean effect of a variable on the outcome, across all schools. Per the intercept, the average math achievement across all private schools (public = 0) at mean SES (ses = 0) is 57.70. A one-standard-deviation increase in ses across all private schools is associated with a 3.87-point increase in math achievement. Public schools at mean ses have a -0.14-point decrease on average in math achievement relative to private schools. From our random effects, the variance term describing how schools vary around the intercept (at mean SES at private schools) is 3.48, and the variance term describing how students vary around their school means is 62.81. Lets calculate variance reduced at level 1 and level 2 by adding school type as a predictor. # level-1 variance reduced sigma2_null &lt;- sigma(null_model)^2 sigma2_public &lt;- sigma(ses_l1_public_l2)^2 (sigma2_null - sigma2_public) / sigma2_null ## [1] 0.05624525 # level-2 variance reduced tau2_null &lt;- VarCorr(null_model)$schcode[1] tau2_public &lt;- VarCorr(ses_l1_public_l2)$schcode[1] (tau2_null - tau2_public) / tau2_null ## [1] 0.6724414 We reduced around 5.6% of variance in math achievement at level-1 and 67.2% of variance at level-2 by adding public as a level-2 predictor. It makes sense that the variance at level-2 was reduced by so much more, because we added a level-2 predictor that varies at level-2. So, does it seem like school type is related to math achievement? We have two sources of information to consider so far: the regression coefficient and the variance reduced. While the regression coefficient is relatively small, the intercept variance reduced at level-2 is quite large (67%!), so it seems like school type is a valuable predictor in our model. 5.3 Conclusion In this chapter, we added level-1 and level-2 fixed effects to our models, considered the difference between conditional and unconditional effects, and used regression coefficients and variance reduced to make a decision about retaining model parameters. In Chapter 6, well work with random slopes and explain cross-level interactions. "],["06-module-6.html", "Chapter 6 Random Effects and Cross-level Interactions 6.1 Learning Objectives 6.2 Data Demonstration 6.3 Conclusion", " Chapter 6 Random Effects and Cross-level Interactions 6.1 Learning Objectives In this chapter, we will introduce cross-level interactions and random effects. The learning objectives for this chapter are: Code and interpret models with random slope effects and cross-level interactions. Interpret meaning of different elements of a Tau matrix. Visualize a random effect covariance using Empirical Bayes estimates. All materials for this chapter are available for download here. 6.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the data. 6.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for visualizations library(lme4) # for multilevel models library(lmerTest) # for p-values And the same dataset of students math achievement: data &lt;- read.csv(&#39;heck2011.csv&#39;) 6.2.2 MLM with Random Slope Effect As a reminder, in Chapter 4 we made the following scatterplot visualizing the relationship between math achievement and socioeconomic status across different schools: data %&gt;% filter(schcode &lt;= 10) %&gt;% ggplot(mapping = aes(x = ses, y = math, colour = factor(schcode))) + geom_point() + geom_smooth(mapping = aes(group = schcode), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + labs(colour = &quot;schcode&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; As we can see, the intercept and slope values are quite different across schools. For example, school 3 has an intercept around 38 and a small positive slope, whereas school 8 has an intercept around 55 and a larger positive slope. In Chapter 5, we modelled the relationship between math achievement and SES as follows: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{1j}ses_{ij} + U_{0j} + R_{ij}\\) We did not assume all schools had the same mean math achievement: we modelled the variation in intercepts by adding a random intercept term (U_{0j}) to our model, which estimated the variances in intercepts across schools. However, we assumed that all schools had the same slope by only estimating the average effect of SES, and not a variance around that slope. That doesnt seem accurate; look at our scatterplot and the variability in slopes! We can model this variance in slopes between schools by adding a random slope term to our model. The following equations describe this model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + U_{1j}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{1j}ses_{ij} + U_{0j} + U_{1j}ses_{ij} + R_{ij}\\) With this model, well now be estimating 6 parameters  2 fixed effects, 3 random effects, and a random effect covariance: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses; \\(\\gamma_{10}\\): the fixed effect for the slope of ses; \\(\\sigma^2\\): a random effect capturing the variance of students around their schools mean math achievement, controlling for ses; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools around the intercept, controlling for ses; \\(\\tau_1^2\\): a random effect for the slope capturing variance of school slopes around the grand mean slope, controlling for ses; \\(\\tau_{01}\\): a random effect covariance capturing how the intercept variance and slope variance relate to each other. The random effect covariance \\(\\tau_{01}\\) quantifies the relationship between \\(\\tau_0^2\\) and \\(\\tau_1^2\\). It is interpreted like any other covariance, as the unstandardized relationship, but lme4 also outputs the standardized form, the correlation. If the covariance is positive, then a higher intercept value is associated with a higher slope. If negative, a higher intercept value is associated with a lower slope. If near-zero, there is minimal/no relationship between the intercept and slope values. In our example, that would indicate schools with higher mean levels of math achievement at the intercept of ses = 0 would also have a larger slope of ses. If the covariance is negative, then a higher intercept value is associated with a lower slope. In our example, schools with higher intercepts of math achievement at ses = 0 would have lower slopes for ses. Well look at the actual random effect covariance in a moment. To estimate a random slope effect in lme4, you place the predictor for which you want a random slope before the | in the code as follows: ses_l1_random &lt;- lmer(math ~ ses + (1 + ses|schcode), data = data, REML = TRUE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(ses_l1_random) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ ses + (1 + ses | schcode) ## Data: data ## ## REML criterion at convergence: 48190.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8578 -0.5553 0.1290 0.6437 5.7098 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schcode (Intercept) 3.2042 1.7900 ## ses 0.7794 0.8828 -1.00 ## Residual 62.5855 7.9111 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6959 0.1315 378.6378 438.78 &lt;2e-16 *** ## ses 3.9602 0.1408 1450.7730 28.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.284 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Note that the 1 indicates a random intercept term, and is a default setting so you can also estimate both a random intercept and slope with just (ses|schcode). If you want to exclude the random intercept from the model you need to write (0 + ses|schcode) to override the default. Lets look at our fixed effects. Per the intercept, the average math achievement at mean SES (ses = 0) is 57.70. A one-standard-deviation increase in ses across all private schools is associated with a 3.96-point increase in math achievement. From our random effects, the variance term describing how schools vary around the intercept (at mean ses) is 3.20 (\\(\\tau_0^2\\)), the variance term describing how school SES slopes vary around the grand mean slope is 0.78 (\\(\\tau_1^2\\)), and the variance term describing how students vary around their schools mean math achievement is 62.59 (\\(\\sigma^2\\)). We can find our random effect covariance by examining our Tau matrix. The Tau matrix is called a Tau matrix because it contains the estimates for our random effects, or Taus: \\(\\tau_0^2\\), \\(\\tau_1^2\\), etc. We have always been estimating a Tau matrix, but when we only had a random intercept it was just a 1-by-1 matrix of the random intercept term \\(\\tau_0^2\\). Matrix::bdiag(VarCorr(ses_l1_random)) ## 2 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## (Intercept) ses ## (Intercept) 3.204184 -1.5802590 ## ses -1.580259 0.7793617 The code looks a little busy, but there are two steps. First, we extract our random effects variance-covariance matrix (Tau matrix) with VarCorr(ses_l1_random). Then, we use the bdiag() function from the Matrix package to construct a matrix thats easy for us to read at a glance. In the first row and first column, we have our intercept variance term \\(\\tau_0^2\\), 3.20. In the second row and second column, we have our slope variance term, \\(\\tau_1^2\\), 0.78. In the second row and first column OR in the first row and second column, we have our random effect covariance, -1.58. This negative covariance indicates that for higher intercepts, the slope value is lower: the relationship between SES and math achievement decreases as mean math achievement increases. The matrix weve shown here only includes the level-2 random effects organized in matrix form, which you may see in other software programs and which can be easier to read. Information about the relationship between random effects is also output by lme4 under the Random effects: section. The lme4 output includes all random effects in variance and standard deviation units, as well as the correlation (not covariance) between the intercept and slope variances. We can convert the covariance between \\(\\tau_0^2\\) and \\(\\tau_1^2\\) to the correlation using the standard deviations of each: \\[corr = \\frac{cov(X, Y)}{sd_x*sd_y}\\] We have our covariance from our Tau matrix: -1.58. We can see the standard deviations in the lme4 output: the standard deviation of the intercept variance is 1.79, the standard deviation of the slope variance 0.88. We can then compute the correlation: -1.58/(1.79*0.88) ## [1] -1.003047 So there is a correlation of -1 between the intercept variance and slope variance, which matches the printed output of -1.00 under the Corr column in the Random effects section of the lme4 output. Lets visualize the relationship using Empirical Bayes estimates (see Chapter 4 for more on EB estimates) of the intercepts and slopes for each school; we expect to see a negative relationship between them. empirical_bayes_data &lt;- ranef(ses_l1_random) # extract random effects for each school empirical_bayes_intercepts &lt;- empirical_bayes_data$schcode[&quot;(Intercept)&quot;] empirical_bayes_slopes &lt;- empirical_bayes_data$schcode[&quot;ses&quot;] # extracts the SES/slope EB estimates from the list bind_cols(empirical_bayes_intercepts, empirical_bayes_slopes) %&gt;% # combine EB slopes and intercepts into a useable dataframe for graphing ggplot(mapping = aes(x = ses, y = `(Intercept)`)) + geom_point() Looks like we expect! Thats a covariance of -1.58 visualized. Finally, note that we get a convergence issue with this model: boundary (singular) fit: see help('isSingular'). For now, were going to ignore that. In Chapter 7 we will focus on estimation issues and troubleshooting. 6.2.3 MLM with Crosslevel Effect In Chapter 5, we added the level-2 variable of school type (public = 0 for public schools, public = 1 for private schools) as a predictor of the intercept to answer the question: how does school type affect math achievement scores when ses = 0? Do public schools have higher or lower intercepts than private schools? The following equations described that model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{10}public_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}public_{j} + \\gamma_{10}ses_{ij} + U_{0j} + R_{ij}\\) What if we want to know how school type affects the slope of ses, though? In other words, is there a difference in the effect of SES on math achievement in a private or public school? We can answer this question by adding school type as a predictor of SES slopes and create a cross-level interaction. This is an interaction because it allows us to estimate a different slope based on school type, whereas our previous model assumed the relationship between SES and math achievement was the same for both school types. The logic is the same as in regular regression. We can describe this model with the following equations. Note that we are also including our slope random effect (\\(\\tau_1^2\\) / \\(U_{1j}\\)), which allows the slopes to vary across schools. This is logically consistent with the idea that slopes might vary due to school type, but not required. We could run this model without random slopes. Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{10}public_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}public_j + U_{1j}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}public_{j} + \\gamma_{10}ses_{ij} + \\gamma_{11}ses_{ij}*public_j + U_{0j} + U_{1j}public_{j} + R_{ij}\\) With this model, we will be estimating 8 parameters  4 fixed effects, 3 random effects, and a random effect covariance: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses and public; \\(\\gamma_{01}\\): the fixed effect for the slope of public, controlling for ses; \\(\\gamma_{10}\\): the fixed effect for the slope of ses, controlling for public; \\(\\gamma_{11}\\): the fixed effect for the effect of public on the slope of ses; \\(\\sigma^2\\): a random effect capturing the variance of students around their schools mean math achievement, controlling for ses and public; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools around the intercept, controlling for ses and public; \\(\\tau_1^2\\): a random effect for the slope capturing variance of school slopes around the grand mean slope, controlling for ses and public; \\(\\tau_{01}\\): a random effect covariance capturing how the intercept variance and slope variance relate to each other. A cross-level interaction is interpreted like an interaction in regular regression: the effect of school type on the effect of SES on math achievement. Like in regular regression interactions, it can also be interpreted as the effect of SES on school type on math achievement. And as in regular regression, either interpretation is accurate, but one of the other might be more intuitive for a specific research question. Lets run our model: crosslevel_model &lt;- lmer(math ~ 1 + ses + public + ses:public + (1 + ses|schcode), data = data, REML = TRUE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(crosslevel_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + public + ses:public + (1 + ses | schcode) ## Data: data ## ## REML criterion at convergence: 48187.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8509 -0.5593 0.1294 0.6412 5.6998 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schcode (Intercept) 3.2144 1.7929 ## ses 0.8013 0.8951 -1.00 ## Residual 62.5555 7.9092 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.72440 0.25183 382.39815 229.216 &lt;2e-16 *** ## ses 4.42383 0.27427 1283.55622 16.130 &lt;2e-16 *** ## public -0.02632 0.29472 387.41741 -0.089 0.9289 ## ses:public -0.62520 0.31957 1363.95273 -1.956 0.0506 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ses public ## ses -0.232 ## public -0.852 0.197 ## ses:public 0.198 -0.858 -0.250 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Note that we can include interactions in two ways. Here, we are using verbose code, listing the individual effects (ses and public) and indicating their interaction with ses:public. You can also capture all of this information with an asterisk: ses*public is equivalent to ses + public + ses:public. We have a convergence warning again: boundary (singular) fit: see help('isSingular'), and again were going to ignore it for now (see Chapter 7 for a deeper dive into these issues). Lets look at our fixed effects. Per the intercept, the average math achievement across all private schools (public = 0) at mean SES (ses = 0) is 57.72. A one-standard-deviation increase in ses across all private schools is associated with a 4.42-point increase in math achievement. Public schools (public = 1) at mean ses have a -0.02-point decrease on average in math achievement relative to private schools. The effect of ses on math achievement is lower in public schools by -0.63 points on average, which quantifies the interaction. We can calculate the expected slope for SES in public schools by using these coefficients: 4.42 - 0.63 = 3.79, so a one-unit increase in SES in public schools is associated with a 3.79-unit increase in math achievement, less of an affect than at private schools. From our random effects, the variance term describing how schools vary around the intercept (at mean SES at public schools) is 3.21, the variance of school slopes around the grand mean is 0.80, and the variance term describing how students vary around their school means is 62.56. We can see our random effect covariance of -1.6 with our Tau matrix, indicating that schools with higher values of mean math achievement at the intercept of ses = 0 have lower slopes of ses. Matrix::bdiag(VarCorr(crosslevel_model)) ## 2 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## (Intercept) ses ## (Intercept) 3.214435 -1.6048897 ## ses -1.604890 0.8012825 Like in other chapters, you can also calculate variance reduced at level-1 and level-2 to examine the impact of adding our cross-level effect, which we leave as an exercise to the reader. 6.3 Conclusion In this chapter, we added random slope effects at level-1 and a cross-level interaction to our model, examined the Tau matrix, and interpreted random effect covariances. In doing so, we ran into some convergence issues (that ?isSingular warning). In Chapter 7, well delve into model estimation options, problems, and troubleshooting. "],["07-module-7.html", "Chapter 7 Model Estimation Options, Problems, and Troubleshooting 7.1 Learning Objectives 7.2 Data Demonstration 7.3 Conclusion 7.4 Further Reading", " Chapter 7 Model Estimation Options, Problems, and Troubleshooting 7.1 Learning Objectives In this chapter, we will review common estimation options, problems that can arise, and how to troubleshoot those problems. The learning objectives for this chapter are: Differentiate between restricted maximum likelihood and full information maximum likelihood estimation options; Describe common causes of estimation errors; Understand the components of optimizer functions; Recognize estimation errors in R output and examine output to identify error sources; Build and compare models to address errors. All materials for this chapter are available for download here. 7.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the data. 7.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for graphing library(lme4) # for multilevel models library(lmerTest) # for p-values And the same dataset of students math achievement: data &lt;- read.csv(&#39;heck2011.csv&#39;) 7.2.2 Introduction to Estimation Problems In Chapter 6, we modelled the relationship between SES and math achievement with a random intercept and random slope as follows: ses_l1_random &lt;- lmer(math ~ 1 + ses + (1 + ses|schcode), data = data, REML = TRUE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) As indicated by the warning message from R, our model is singular (which well define in a moment). In this chapter, we will examine estimation issues like this and how to troubleshoot them. This is one of the less interactive chapters in these materials, but if you want a reason to stick around, there is a fun puzzle analogy. Well begin with some notes on model estimation and then move onto possible issues and how to address them. 7.2.3 Estimation and Optimizers In linear regression, Ordinary Least Squares estimation is used to find a combination of parameters (intercepts and slopes) that minimize the residual sum of squares. If we imagine a simple linear regression with math achievement as an outcome and SES as a predictor, we have our regression line (line of best fit) and our actual data points around that line. data %&gt;% filter(schcode &lt;= 10) %&gt;% # subset data to make it easier to see ggplot(mapping = aes(x = ses, y = math)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; For a given value of SES on the x-axis, the distance between our prediction (regression line) and our actual observation (data point) is our residual, and if we sum all of the residuals (after squaring them so the negative residuals below the line and positive residuals above it dont cancel out), we get our residual sum of squares. OLS regression will select the regression line with the smallest residuals, which is the line that is as close as possible to the data points. You can see this process and play around with it on this interactive website: https://seeing-theory.brown.edu/regression-analysis/index.html#section1 In multilevel modelling, we use maximum likelihood (ML) estimation instead of OLS estimation. In ML estimation, we have our data points and we want to find the combination of parameters (intercepts and slopes) that maximize the likelihood that we observed that data. This is an iterative process, where we select parameters that maximize the probability of getting our data (i.e., that maximize the likelihood). We select set after set of parameters, and eventually stop when the parameter sets arent getting better. You can play around with likelihood here: https://seeing-theory.brown.edu/bayesian-inference/index.html#section2 This video from Stat Quest walks through the concept: We have two options for ML estimation in multilevel modelling: restricted maximum likelihood (REML) and full information maximum likelihood (FIML or ML). The key difference between them is how the estimation methods handle the variance components. When using REML, there is a penalty applied to the degrees of freedom when estimating the variance components \\(\\sigma^2\\), \\(\\tau_1^2\\), etc. When using FIML, there is no such penalty and as a result the variance components are usually underestimated. A linear regression analogy might help clarify this point: the formula for population variance is \\(S = \\frac{\\Sigma(x_i - \\overline{x})^2}{n}\\). The formula for sample variance is \\(s = \\frac{\\Sigma(x_i - \\overline{x})^2}{n - 1}\\). The sample variance imposes a penalty of n - 1 and is a REML estimator, while the population variance formula is the corresponding FIML estimator. Because we want accurate information about our variance components, we will usually use REML. We will only use FIML when we want to compare two models with different fixed effects. Well discuss model comparison later in this chapter. 7.2.4 Non-Convergence In the embedded Stat Quest video above, the narrator describes the iterative process in ML estimation of finding the maximum likelihood estimate of a parameter, trying multiple different options before settling on one as the value that maximizes the likelihood of observing their data about mice weights. When youre working with many predictors at once  for example, an intercept and a slope for SES and a slope for school type and variance terms for all of those fixed effects  it is harder to try all possible combinations. So, optimization algorithms (AKA optimizers) are used to try to find the ML estimates by examining a subset of possible combinations. However, these optimizers cannot always find the combination of parameters that maximizes the likelihood of observing your data; they cant find a solution to the problem of what paramaters maximize the likelihood of observing this data?. When the optimizers cannot find a solution, the result is called non-convergence: the model did not converge on a solution. You should not use the parameter estimates from a non-converged solution. A non-convergence warning is the computing equivalent of being unable to put a puzzle together, jamming the pieces in where you can, and saying I dont know, this is my best guess about where these pieces go. Sure, the puzzle might sort of look like the image on the box, but it doesnt really match, a bunch of the pieces have been contorted and bent to fit. (That was the puzzle analogy, was it worth sticking around?) There are two main strategies to solve a non-convergence problem: change your optimizer or change your model. You can manipulate a few characteristics of your optimizer to try to get convergence: Number of iterations. If you increase the number of iterations, the algorithm will search for longer. This is the equivalent of getting our puzzle-doer to sit at the table for longer trying to assemble the puzzle, trying out different and more pieces. Algorithm: the algorithm determines how the optimizer chooses its next attempted solution. What strategy is our puzzle-doer using to fit pieces into the puzzle? Tolerance: this can get a bit technical, so we suggest Brauer and Curtin, 2018 for more. You can alter these elements of your optimizer to see if giving it more time, a different strategy, or more leeway to say yes, this converged will lead to convergence. Alternatively, you can trim your model, removing variables you think are less likely to matter. We will discuss some approaches to doing this below. 7.2.5 Singularity Singularity occurs when an element of your variance-covariance matrix is estimated as essentially zero as a result of extreme multicollinearity or because the parameter is actually essentially zero. You can find singularity by examining your variance-covariance estimates and the correlations between them. It will often show up as co/variances near zero or correlations between variances at -1 or 1. Lets return to our example from Chapter 6, predicting math achievement from SES with a random slope: ses_l1_random &lt;- lmer(math ~ 1 + ses + (1 + ses|schcode), data = data, REML = TRUE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) As we can see, our output contains a helpful warning message notifying us that the model is singular. We can investigate this issue in three ways. First, we can look at our Tau matrix: Matrix::bdiag(VarCorr(ses_l1_random)) ## 2 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## (Intercept) ses ## (Intercept) 3.204184 -1.5802590 ## ses -1.580259 0.7793617 Things look okay here, no elements appear to be close to or zero. Our second method of investigation is looking at our overall output: summary(ses_l1_random) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + (1 + ses | schcode) ## Data: data ## ## REML criterion at convergence: 48190.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8578 -0.5553 0.1290 0.6437 5.7098 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schcode (Intercept) 3.2042 1.7900 ## ses 0.7794 0.8828 -1.00 ## Residual 62.5855 7.9111 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6959 0.1315 378.6378 438.78 &lt;2e-16 *** ## ses 3.9602 0.1408 1450.7730 28.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.284 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Here, in our random effects section, we can see that the correlation between our random effects is -1.00, a sign of perfect multicollinearity. We can dig into the confidence intervals of our estimates up close to confirm this: confint(ses_l1_random, oldNames = FALSE) ## Computing profile confidence intervals ... ## Warning in FUN(X[[i]], ...): non-monotonic profile for cor_ses.(Intercept)|schcode ## Warning in confint.thpr(pp, level = level, zeta = zeta): bad spline fit for cor_ses.(Intercept)|schcode: falling back to ## linear interpolation ## 2.5 % 97.5 % ## sd_(Intercept)|schcode 1.4944476 2.077295 ## cor_ses.(Intercept)|schcode -1.0000000 1.000000 ## sd_ses|schcode 0.5400973 1.262130 ## sigma 7.7760573 8.049130 ## (Intercept) 57.4362972 57.956350 ## ses 3.6716229 4.252084 Note that oldNames = FALSE just makes the output easier to read. This will take a moment to run, but when it does we can see that the 95% confidence interval for the correlation between our random effects spans -1 to 1 (i.e.Â the entire possible range). Our singularity issue started when we added the random slope effect, which added both a random slope variance \\(\\tau_1^2\\) and the random intercept-slope covariance \\(\\tau_{01}\\). Lets see if we can fix the issue by removing that problematic covariance. ses_l1_random_cov0 &lt;- lmer(math ~ 1 + ses + (1|schcode) + (0 + ses|schcode), data = data, REML = TRUE) summary(ses_l1_random_cov0) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + (1 | schcode) + (0 + ses | schcode) ## Data: data ## ## REML criterion at convergence: 48213.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7791 -0.5526 0.1327 0.6466 5.7089 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.3222 1.8227 ## schcode.1 ses 0.7205 0.8488 ## Residual 62.5213 7.9070 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5888 0.1328 374.9738 433.55 &lt;2e-16 *** ## ses 3.8803 0.1435 377.2408 27.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.023 Here, we specify our random intercept (1|schcode) and random slope with no covariance (0 + ses|schcode) separately, and that fixed the singularity issue! If we print our Tau matrix we can see that the covariance is fixed to 0. Matrix::bdiag(VarCorr(ses_l1_random_cov0)) ## 2 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] 3.322166 . ## [2,] . 0.7204535 In general, it is best practice to build a maximal multilevel model, one that includes all possible fixed and random effects that are not zero (Barr et al., 2013). This maximal model will produce parameter estimates with the least amount of bias and provide the best shot at your model fitting the data. However, the maximal model that tries to estimate extreme random effects (those near zero or with high multicollinearity) will have trouble converging and produce estimation errors. When this happens, often an inspection of the random effects will reveal which parameters need to be removed from the model. It can be helpful, ahead of running your MLMs, to consider the key variables of interest, their random effects, and plan, if the maximal model has errors, which parameters should be removed and in what order. Overall, building MLMs is about balancing complexity with utility. Sometimes we do not have enough information in our data to estimate the complex model we planned, so having a plan for how to decrease complexity ahead of time can prevent getting lost in the garden of forking paths. 7.2.6 Deviance Testing for Model Comparison We removed the random effect covariance and our model is no longer singular (i.e., suffering from multicollinearity). That seems better! Now that we have a model without an error, lets look at comparing the model with the random slope for SES (but no covariance, as we just removed that) and the model without the random slope for SES. If we want to formally test if a model fit is better or at least not worse, we can conduct a deviance test. You can find the deviance for your model under the REML criterion at convergence in your summary output. In short, deviance is bad and we dont want more of it, so when we compare the model with and without the random slope for SES, we dont want the model with the random slope to have more deviance. We want the same or less deviance. Note that deviance is based on the likelihood function for your model. Unlike probability, likelihood is not bound at 0 and 1. It can be any number. As a result, looking at likelihood or deviance in isolation is not informative, because it has no bounds. It is only useful for comparison between models, where less deviance indicates a better model (compared to the reference model). Here, were comparing models with the same fixed effects but different random effects so we can still use REML estimator that more accurately estimates random effects. We have our two model terms, ses_l1_random and ses_l1_random_cov0, and we can compare the deviance of each using the built-in ANOVA function. Specifying refit = FALSE stops the function from refitting the models with FIML. If we were comparing models with different fixed effects, we would use FIML to estimate our models. # models ses_l1 &lt;- lmer(math ~ 1 + ses + (1|schcode), data = data, REML = TRUE) ses_l1_random_cov0 &lt;- lmer(math ~ 1 + ses + (1|schcode) + (0 + ses|schcode), data = data, REML = TRUE) # deviance test to compare model fit anova(ses_l1, ses_l1_random_cov0, refit = FALSE) ## Data: data ## Models: ## ses_l1: math ~ 1 + ses + (1 | schcode) ## ses_l1_random_cov0: math ~ 1 + ses + (1 | schcode) + (0 + ses | schcode) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## ses_l1 4 48223 48251 -24108 48215 ## ses_l1_random_cov0 5 48223 48258 -24107 48213 2.0825 1 0.149 Lets read our output. We have seven columns: npar is the number of parameters estimated in the models. The only difference between the models is one has a random slope for SES and the other doesnt, and you can see that one model estimates 4 parameters and the other 5 parameters. AIC: Akaikes Information Criterion, one measure of goodness of fit BIC: Bayesian Information Criterion, another measure of goodness of fit logLik: log likelihood deviance: -2*logLik Chisq: the difference betwen our models deviances df: the degrees of freedom for the test, calculated as the difference in number of parameters between the models Pr(&gt;Chisq): the probability that we would find our chi-square value or greater if the null hypothesis that the models were the same was true There is no significant difference between our models deviance statistics: the model without the random slope has a deviance of 48215 and the model with the covariance has a deviance of 48213. The difference between these numbers is not significant, p = 0.149. Thus, there is no significant different in model fits and adding a random slope does not compromise model fit so we can add it if we think its informative. Well discuss model specification, fit, and comparison more in Chapter 11 when discussing effect sizes. In closing, when assessing model fit or troubleshooting estimation problems, it is preferable to pre-register what troubleshooting you expect to try or models you expect to estimate. At minimum, you should keep a record of changes you make and report all of them. 7.3 Conclusion In this chapter, we considered convergence options, how to diagnose and troubleshoot issues, and comparing model fits using deviance testing. In Chapter 8, well consider different centering options in MLMs. 7.4 Further Reading Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 10.1016/j.jml.2012.11.001. https://doi.org/10.1016/j.jml.2012.11.001 Brauer, M., &amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389411. "],["08-module-8.html", "Chapter 8 Centering Options and Interpretations 8.1 Learning Objectives 8.2 Data Demonstration 8.3 Conclusion 8.4 Further Reading", " Chapter 8 Centering Options and Interpretations 8.1 Learning Objectives In this chapter, we will review options for and interpretations of centering variables in multilevel models. The examples are adapted from Dr.Â Dan McNeishs lecture (thanks Dan!), and you can see an overview of his other work here. The learning objectives for this chapter are: Review centering options and interpretation in linear regression. Differentiate between total, within, between, and contextual effects. Understand the difference between within-cluster and grand-mean centering and when to use each strategy. Estimate and interpret models using both within-cluster and grand-mean centering. All materials for this chapter are available for download here. 8.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the data. 8.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(magrittr) # for assignment pipe %&lt;&gt;% library(lme4) # for multilevel models library(lmerTest) # for p-values And the same dataset of students math achievement: data &lt;- read.csv(&#39;heck2011.csv&#39;) 8.2.2 Why Center Variables? If youve worked with single-level regression before, youre probably already familiar with centering variables. Centering in regression facilitates interpretation of the intercept, which is the average value of your outcome variable when all predictors are set to zero. If your predictors do not have meaningful zero points, then the intercept can be non-sensical. For example, imagine we were predicting the number of goals scored by players in an adult hockey league based on their age in a simple regression: \\(goals_{i} = \\beta_{0} + \\beta_{1}age_{i} + \\epsilon_{i}\\) If we had age in years without centering it, the intercept would represent the average number of goals scored by players 0 years old. There are no players in adult hockey leagues that are zero years old, so this intercept is not really useful to us. If we instead centered age, the intercept would represent the average number of goals scored by players at the average league age (maybe something like 38 years old). This is a more meaningful interpretation that lies within the range of our data. Centering in multilevel models is also used to make coefficients more meaningful, but also changes the interpretation of coefficients and can be used to decompose total effects into estimates of the within, between, and contextual effects of a variable. Lets dig more into those different effects. 8.2.3 Within, Between, and Contextual Effects To define within, between, and contextual effects, lets think about Marshs Big Fish-Little Pond effect. This effect describes the situation in which high achieving students in a school that is low achieving on average, will feel better about their abilities than high achieving students in a school with higher average achievement. You may have experienced this feeling when you went from being one of the best undergraduates at your institution (i.e., you were the big fish in the little pond) to feeling less confident in your abilities when you went to graduate school (i.e., when you became a small or medium sized fish in a big pond). In the example data we have students in different schools and have measured their levels of academic achievement (their grades, for example) and academic self-concept (do they feel like theyre succeeding in school?). The within effect describes the relationship between students grades and their feelings of succeeding in school within a given school: how does a students grades affect their feelings of success? We might expect that higher grades are associated with stronger feelings of yes, I am succeeding. The between effect describes the relationship between a schools average grades and the average of students feelings of success: how does a schools average grade affect students average feelings of success? We might expect that in higher-performing schools, the students actually feel less successful on average. The contextual effect describes the difference in feelings of success for students with the same grades in schools with different average grades. What would happen if we plopped the same student into a different context (i.e., cluster)? We might expect that Student A with a grade of 80% in a school with an average grade of 60% feels great about their success, but what if we take that student with a grade of 80% and put them into a school with an average grade of 99%? They probably dont feel as successful. This is the Big Fish-Little Pond effect, and it is a contextual effect: what effect does context have on the outcome variable? We can see this illustrated in the following graph adapted from Dan McNeishs slides: Note how the effects relate to one another: \\(between\\ effect = within\\ effect + contextual\\ effect\\). One way of thinking of this is that the between effect shows the overall, average relationship, but using MLMs, we can decompose it into a within and contextual effect. To estimate these effects, We use different centering options! 8.2.4 Options for Centering in MLMs Lets return to our example of SES predicting math achievement to understand if there is a contextual effect of students achievement from being in higher or lower on average SES schools. So far, weve been using the following model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{10}ses_{ij} + U_{0j} + R_{ij}\\) model &lt;- lmer(math ~ 1 + ses + (1|schcode), data = data, REML = TRUE) summary(model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48215.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7733 -0.5540 0.1303 0.6469 5.6908 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.469 1.863 ## Residual 62.807 7.925 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5960 0.1329 375.6989 433.36 &lt;2e-16 *** ## ses 3.8739 0.1366 3914.6382 28.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.025 Weve been using the variable ses which is Z-scored so 0 is equal to the mean across all students. This is a type of centering that also standardizes the units in our model. From this model, weve seen in previous chapters that SES has a 3.87-unit effect on achievement, meaning that math achievement is expected to increase by 3.87 units as SES increases by 1 standard deviation. This estimate cannot tell us about the effect within a school, or the contextual effect, it is an uninterpretable blend of both. We will walk through how to tease these different effects out using centering. If we want to get our within, between, and contextual effects, we have two options (Enders and Tofighi, 2007): Centering within cluster (CWC) Centering around grand mean (CGM). 8.2.4.1 Centering Within Cluster (CWC) Centering a variable within a cluster means each cluster will have a mean of zero. So, each school will have a mean of zero, and students scores on ses_cwc will reflect their variance around their school mean, not the grand mean of the whole dataset. To center SES within cluster, we first group our dataset by school and calculate the mean SES for each cluster: data %&lt;&gt;% # this symbol is an assignment operator and pipe, equivalent to data &lt;- data %&gt;% group_by(schcode) %&gt;% mutate(ses_mean = mean(ses)) Then, we subtract this cluster mean from every individual students SES value: \\(ses_{cwc} = ses - ses_{mean}\\). For the students at the mean, the resulting value will be 0. Students above the mean will have positive values, and students below the mean will have negative values. data %&lt;&gt;% mutate(ses_cwc = ses - ses_mean) The values of ses_cwc are distributed around the mean for the cluster. The mean of the centered values for each school is now (essentially) zero: data %&gt;% group_by(schcode) %&gt;% summarize( mean(ses_cwc) ) ## # A tibble: 419 x 2 ## schcode `mean(ses_cwc)` ## &lt;int&gt; &lt;dbl&gt; ## 1 1 3.01e-17 ## 2 2 -1.28e-17 ## 3 3 4.32e-17 ## 4 4 5.55e-17 ## 5 5 -3.28e-18 ## 6 6 -3.47e-18 ## 7 7 8.67e-19 ## 8 8 1.07e-17 ## 9 9 1.14e-17 ## 10 10 -5.56e-18 ## # ... with 409 more rows If we estimate a model with just ses_cwc, it would look like this: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses\\_cwc_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{10}ses\\_cwc_{ij} + U_{0j} + R_{ij}\\) Here, \\(\\gamma_{10}\\) represents how students vary around their school mean, which is our within effect. It only captures the effect of SES on achievement within schools, if we run this model we get an intercept of 57.67 and an effect for ses_cwc of 3.19 indicating that on average, within a school an increase in SES to one standard deviation above the mean is associated with an increase in math achievement of 3.19. model_cwc &lt;- lmer(math ~ 1 + ses_cwc + (1|schcode), data = data, REML = TRUE) summary(model_cwc) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses_cwc + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48482.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6436 -0.5628 0.1413 0.6375 5.6268 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 10.89 3.300 ## Residual 62.59 7.912 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6732 0.1883 416.1017 306.33 &lt;2e-16 *** ## ses_cwc 3.1903 0.1578 6451.7013 20.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses_cwc 0.000 This model is incomplete. If we want our between effect (i.e., how the school averages differ from each other), we can add the aggregate back in at level 2, which is the value we calculated for each schools mean, ses_mean: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses\\_cwc_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}ses\\_mean_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}ses\\_mean_j + \\gamma_{10}ses\\_cwc_{ij} + U_{0j} + R_{ij}\\) With this model, we are estimating 5 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses_cwc and ses_mean (i.e., a student with school-average SES at the average school); \\(\\gamma_{01}\\): the fixed effect for ses_mean controlling for ses_cwc. This is our between effect, indicating the effect of a schools average SES on a students math achievement; \\(\\gamma_{10}\\): the fixed effect for the slope of ses_cwc controlling for ses_mean. This is our within effect, indicating the effect of a students SES compared to the school average (at the average school, ses_mean = 0); \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools around the intercept, controlling for ses_cwc and ses_mean; \\(\\sigma^2\\): a random effect capturing the variance of students around their school mean math achievement, controlling for ses_cwc and ses_mean. Lets estimate it: model_cwc_l2 &lt;- lmer(math ~ 1 + ses_cwc + ses_mean + (1|schcode), data = data, REML = TRUE) summary(model_cwc_l2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses_cwc + ses_mean + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48137.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7545 -0.5575 0.1312 0.6619 5.7512 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 2.511 1.585 ## Residual 62.628 7.914 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5467 0.1239 401.3223 464.54 &lt;2e-16 *** ## ses_cwc 3.1903 0.1578 6448.7759 20.22 &lt;2e-16 *** ## ses_mean 5.8920 0.2529 385.3371 23.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ss_cwc ## ses_cwc 0.000 ## ses_mean -0.052 0.000 The average student math achievement at the average school is 57.55. A one-unit increase in SES within a school is associated with a 3.19-unit increase in math achievement. A one-unit increase in the schools average SES is associated with a 5.89-unit increase in math achievement. Recall our earlier formula detailing the relationship between the within, between, and contextual effects: \\(between\\ effect = within\\ effect + contextual\\ effect\\). Armed with our between effect (\\(\\gamma_{01}\\)) and within effect (\\(\\gamma_{10}\\)), we can reorganize this equation and calculate our contextual effect: \\(contextual = between - within = \\gamma_{01} - \\gamma_{10} = 5.89 - 3.19 = 2.70\\), so with two hypothetical students with the same level of SES, the one in the school with higher average SES has 2.70-unit higher math achievement. This represents the contextual effect of a schools SES on math achievement. 8.2.4.2 Centering Grand Mean (CGM) When we center a variable at the grand mean, we have information about how individuals vary around the mean of all individuals. So students scores on ses_cgm will reflect their variance around the mean of all students. To center SES at the grand mean, we first calculate the grand mean: data %&lt;&gt;% ungroup() %&gt;% # remove the grouping by school that we added in the CWC section mutate(ses_grand_mean = mean(ses)) Then, we subtract this grand mean from every individual students SES value: \\(ses_{cgm} = ses - ses\\_grand\\_mean\\). For the students at the grand mean, the resulting value will be 0. Students above the mean will have positive values, and students below the mean will have negative values. data %&lt;&gt;% mutate(ses_cgm = ses - ses_grand_mean) The values of ses_cgm are distributed around the grand mean. The mean of the students values of ses_cgm is now (essentially) zero. data %&gt;% summarize( mean(ses_cgm) ) ## # A tibble: 1 x 1 ## `mean(ses_cgm)` ## &lt;dbl&gt; ## 1 3.15e-18 If we estimate a model with just ses_cgm, it would look like this: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses\\_cgm_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{10}ses\\_cgm_{ij} + U_{0j} + R_{ij}\\) Here, \\(\\gamma_{10}\\) represents how students vary around the grand mean. If we dont factor out school means, then this value is an uninterpretable blend of within- and between-effects, sometimes referred to as the total effect. When we center at the grand mean, we must add the cluster mean back into into the model. (This is in contrast to centering within cluster, when we can just estimate the within effect, but to get the between effect we must add the cluster mean back into the model). Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses\\_cgm_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}ses\\_mean_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}ses\\_mean_j + \\gamma_{10}ses\\_cgm_{ij} + U_{0j} + R_{ij}\\) With this model, we are estimating 5 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses_cgm and ses_mean (i.e., a student with school-average SES at the average school); \\(\\gamma_{01}\\): the fixed effect for ses_mean controlling for ses_cgm. This is our contextual effect, indicating the effect of school average SES on student math achievement at the same level of student SES; \\(\\gamma_{10}\\): the fixed effect for the slope of ses_cgm controlling for ses_mean. This is our within effect, indicating the effect of a students SES compared to the school average (at the average school, ses_mean = 0); \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools around the intercept, controlling for ses_cgm and ses_mean; \\(\\sigma^2\\): a random effect capturing the variance of students around their school mean math achievement, controlling for ses_cgm and ses_mean. Lets estimate it in R: cgm_model &lt;- lmer(math ~ 1 + ses_cgm + ses_mean + (1|schcode), data = data, REML = TRUE) summary(cgm_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses_cgm + ses_mean + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48137.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7545 -0.5575 0.1312 0.6619 5.7512 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 2.511 1.585 ## Residual 62.628 7.914 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6485 0.1240 402.6508 464.982 &lt;2e-16 *** ## ses_cgm 3.1903 0.1578 6448.7759 20.217 &lt;2e-16 *** ## ses_mean 2.7017 0.2981 738.7254 9.065 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ss_cgm ## ses_cgm 0.041 ## ses_mean -0.066 -0.529 The average student math achievement at the average school is 57.65. For two hypothetical students with the same level of SES, the one in the school with higher average SES has 2.70-unit higher math achievement. Within a school, a one-unit increase in SES relative to the grand mean is associated with a 3.19-unit increase in math achievement. Recall our earlier formula detailing the relationship between the within, between, and contextual effects: \\(between\\ effect = within\\ effect + contextual\\ effect\\). Armed with our contextual effect (\\(\\gamma_{01}\\)) and within effect (\\(\\gamma_{10}\\)), we can calculate our between effect: \\(between = within + contextual = \\gamma_{10} + \\gamma_{01} = 3.19 + 2.70 = 5.89\\), so an increase in a schools average SES by one unit is associated with an increase of 5.89-unit math achievement. 8.2.5 What Kind of Centering Should You Use? Here is a reference table to keep track of what coefficients represent what effects in CWC or CGM models: Centering Option Contextual Parameter Within Parameter Between Parameter Centering Grand Mean (CGM) \\(\\gamma_{01}\\) \\(\\gamma_{10}\\) \\(\\gamma_{01} + \\gamma_{10}\\) Centering Within Cluster (CWC) \\(\\gamma_{01} - \\gamma_{10}\\) \\(\\gamma_{10}\\) \\(\\gamma_{01}\\) Lets look at our example results this way: Centering Option Contextual Parameter Within Parameter Between Parameter Centering Grand Mean (CGM) 2.70 3.19 5.89 Centering Within Cluster (CWC) 2.70 5.89 3.19 If youre not interested in contextual results, you can use the following shorthand for deciding whether to center around the grand mean or within cluster: if youre interested in level-1 predictors, CWC is best because it gives an unbiased estimate of the within cluster effect and produces better estimates of the slope variance, though as we saw you can get an unbiased estimate of the within cluster effect with CGM if you add the aggregate back in at level 2. If youre interested in level-2 predictors, but you have covariates at level-1 you want to control for, CGM is best because it controls for level-1 predictors by producing adjusted means. If you are interested in interactions (at level-1 or cross-level), use CWC to get an unbiased estimate of the within cluster slope and slope variance. See Enders &amp; Tofighi (2007) for a detailed discussion. 8.3 Conclusion In this chapter, we reviewed two options for centering variables in MLMs (centering within cluster and centering grand mean), when to use each option, and how to interpret coefficients under each option. In the first 8 chapters, weve covered a lot of the basics of MLMs. In Chapter 9, well revisit a number of concepts weve already seen, but in the context of repeated-measure rather than the cross-sectional data weve been using to this point. 8.4 Further Reading Enders, C. K., &amp; Tofighi, D. (2007). Centering predictor variables in cross-sectional multilevel models: A new look at an old issue. Psychological Methods, 12(2), 121138. https://doi.org/10.1037/1082-989X.12.2.121 "],["09-module-9.html", "Chapter 9 Multilevel Modelling with Repeated Measures Data 9.1 Learning Objectives 9.2 Data Demonstration 9.3 Conclusion", " Chapter 9 Multilevel Modelling with Repeated Measures Data 9.1 Learning Objectives JESS/MKS NOTE TO SELF REVIEW THE EQUATIONS IN THIS CHAPTER, THEYRE A LITTLE MESSY In this chapter, we will review fitting MLMs for repeated measures data. The learning objectives for this chapter are: Review multilevel modelling concepts discussed so far; Recognize when data are repeated measures and in the correct format for multilevel modelling; Conduct multilevel modelling on repeated measures data; Interpret coefficients for repeated measures data. All materials for this chapter are available for download here. 9.2 Data Demonstration 9.2.1 Load Dependencies For this data demo, we will use the following packages: library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for ICC 9.2.2 Review of Multilevel Modelling Procedure Multilevel modelling in repeated measures data is a new application of the techniques weve covered so far, so lets briefly review the steps in our modelling framework so far: Establish solid theory and measurement, decide whether you need MLMs for your question Run random-intercept-only (i.e., null) model to calculate ICC and quantify extent of clustering in data Build model incrementally adding fixed and random effects per your theory, considering centering and estimation (REML or FIML) choices Conduct deviance test to compare model fits If you run into estimation issues, change your optimizer or remove problematic effects Report results: coefficients, significance, plausible values ranges, any changes made to address estimation issues 9.2.3 Multilevel Models for Repeated Measures Thus far, weve been using a cross-sectional example of students clustered within schools. Our level-1 variables have been about traits that students vary on (e.g., age, gender, SES) while our level-2 variables have been about traits that schools vary on (e.g., whether they are public or private schools). With repeated measures data, measures are clustered within person rather than having people clustered within some organizational structure. The level-1 variables are about traits that the measures vary on (e.g., experimental manipulations) while level-2 variables are about traits that people vary on (e.g., age, gender, SES). For example, imagine we show participants pictures of lines and ask them to estimate the line length. We measure how long it takes them to rate each line. A level-1 variable would be the line length; each participant sees several lines of different lengths. A level-2 variable would be something demographic like a participants age or gender; each person has the same value on the variable for the entire experiment. In this chapter, well look at repeated measures data without time in the model. In Chapter 10, well look at repeated measures data with time in the model, i.e., longitudinal models. 9.2.3.1 Data Structures: Long vs Wide Imagine you were measuring weight and caloric intake. If you have historically worked with repeated measures data in an ANOVA framework, you are probably used to working with data in a wide format, i.e., one row per participant with different variables for different measurement instances. id weight1 weight2 calories1 calories2 1 200 190 3500 3300 2 150 160 3200 3100 In MLMs, you need to use data in a long format where one row is one measurement occasion: id weight calories measurement_occasion 1 200 3500 1 1 190 3300 2 2 150 3200 1 2 160 3100 2 This requires transposing your data, which you can read more about here. An aside: you might also be used to thinking listwise deletion deletes an entire participant, because listwise deletion deletes rows with any missing data and in wide data one row is one participant. In long data, listwise deletion means deleting one measurement instance, not necessarily an entire participant. For example, if a participant answers a questionnaire a first time, then at one follow-up, but not at the second follow-up, listwise deletion will only remove their third row full of NAs; youll keep their data from the first two questionnaires. 9.2.4 Our Data: Reaction Time The data used in this module are used as an example in Hoffman and Rovine (2007). The article and supporting materials can be found here: http://www.lesahoffman.com/Research/MLM.html data &lt;- read.csv(&#39;hoffman2007.csv&#39;) Lets look at our data: head(data) ## X id sex age NAME rt_sec Item meaning salience lg_rt oldage yrs65 c_mean c_sal ## 1 1 1 1 20 rt_sec1 4.662 1 3.5 4.0 1.5394445 0 0 0.5 1.0 ## 2 2 1 1 20 rt_sec2 6.660 2 0.0 3.0 1.8961195 0 0 -3.0 0.0 ## 3 3 1 1 20 rt_sec3 6.602 3 4.0 2.0 1.8873726 0 0 1.0 -1.0 ## 4 4 1 1 20 rt_sec4 1.332 4 4.0 4.0 0.2866816 0 0 1.0 1.0 ## 5 5 1 1 20 rt_sec5 1.332 5 0.0 5.0 0.2866816 0 0 -3.0 2.0 ## 6 6 1 1 20 rt_sec7 1.302 7 3.5 4.5 0.2639015 0 0 0.5 1.5 For this data demo the outcome of interest is the log of reaction time for participants to detect a change during a picture viewing task (rt_sec). The pictures varied on two dimensions: how meaningful driving was to the picture (meaning) and how salient the change was in the picture (salient). For this analysis we will focus on the variables centered from the midpoint of the rating (3): c_mean and c_sal. One of the primary research questions was how age related to reaction time, given those differences in pictures. Participants were sampled in age categories: younger (oldage = 0, 40 and under) and older (oldage = 1, above 40). Repeated trials are nested within persons. 9.2.5 Random-Intercept-Only/Null Model Lets estimate our null model with FIML as our estimator and calculate the ICC: null_model &lt;- lmer(lg_rt ~ 1 + (1|id), data = data, REML = FALSE) # note that REML = FALSE performance::icc(null_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.252 ## Conditional ICC: 0.252 With repeated measures data, the ICC is interpreted as the proportion of variance between people: How much of the variance stems from people being different from one another versus fluctuating within themselves? A large ICC means that most of the variability is between people, not from people varying in their answers to a set of questions (or in this case, reaction time). The ICC is 0.252, indicating that 25.2% of the variance in log reaction time is attributed to a person. (Some bonus fun: when responses to questions are nested within person, Cronbachs alpha is equivalent to the ICC: a high alpha indicates high reliability of the scale because most of the scale variance is between people, individuals have relatively consistent patterns of answering on the scale.) 9.2.6 Adding Level-1 Fixed Effects Lets add our level-1 predictors for picture meaning c_mean and picture salience c_sal to our model. This is represented with the following formulae: Level Equation Level 1 \\(lg\\_rt_{ij} = \\beta_{0j} + \\beta_{1j}c\\_mean_{ij} + \\beta_{2j}c\\_sal_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) \\(\\beta_{2j} = \\gamma_{20}\\) Combined \\(lg\\_rt_{ij} = \\gamma_{00} + \\gamma_{10}c\\_mean_{ij} + \\gamma_{20}c\\_sal_{ij} + U_{0j} + R_{ij}\\) With this model, were estimating 5 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for c_mean and c_sal; \\(\\gamma_{10}\\): the fixed effect for the slope of c_mean, controlling for c_sal. This represents how meaning affects a persons reaction time  do people respond more quickly or slowly to photos with changes that are related to driving a car (perhaps because theyre often driving and are attuned to changes in the environment when operating a car)? \\(\\gamma_{20}\\): the fixed effect for the slope of c_sal, controlling for c_mean. This represents how salience affects a persons reaction time  do people respond more quickly or slowly to photos with more obvious changes? \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of people around the intercept, controlling for c_mean and c_sal; \\(\\sigma^2\\): a random effect capturing the variance of people around their own mean log reaction time, controlling for c_mean and c_sal. Lets run the model with FIML as our estimator: l1_model &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + (1|id), data = data, REML = FALSE) summary(l1_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + (1 | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16525.9 16560.6 -8257.9 16515.9 7641 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5224 -0.7291 -0.1090 0.6177 3.8884 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.1756 0.4191 ## Residual 0.4785 0.6917 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.605e+00 3.483e-02 1.533e+02 46.09 &lt;2e-16 *** ## c_mean -5.153e-02 4.304e-03 7.493e+03 -11.97 &lt;2e-16 *** ## c_sal -1.323e-01 7.435e-03 7.493e+03 -17.80 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean ## c_mean 0.044 ## c_sal -0.015 -0.219 The intercept of 1.61 is the mean log reaction time across all people at average values of meaning and salience. A one-unit increase in meaning is associated with a decrease in log reaction time of 0.05 (i.e., a faster reaction time), at the average level of salience. A one-unit increase in salience is associated with a decrease in log reaction time of 0.13 at the average level of meaning. All coefficients are significant. The term describing how people vary around the grand mean intercept is 0.18. The term describing how people vary around their own intercept is 0.48. Does this model have significantly less deviance (i.e., better fit) than the null model alone? Lets use a deviance test to check. Note that we can compare the null and level-1 models because we used FIML as our estimator and they are nested (i.e., all variables in the level-1 model are in the null model). anova(null_model, l1_model) ## Data: data ## Models: ## null_model: lg_rt ~ 1 + (1 | id) ## l1_model: lg_rt ~ 1 + c_mean + c_sal + (1 | id) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## null_model 3 17082 17103 -8537.9 17076 ## l1_model 5 16526 16561 -8257.9 16516 559.94 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The level-1 model does have significantly less deviance (16516 compared to 17076 for the null model), so is a better model. Hooray! 9.2.7 Adding Random Slopes Lets try adding random slopes for our level-1 variables of meaning and salience. This allows slopes to vary across people. Maybe some people have stronger relationships between salience and reaction time  such that when the change is more salient they really notice it, and when its less salient they notice it less  while other people have eagle eyes and notice the changes no matter their salience. Level Equation Level 1 \\(lg\\_rt_{ij} = \\beta_{0j} + \\beta_{1j}c\\_mean_{ij} + \\beta_{2j}c\\_sal_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + U_{1j}\\) \\(\\beta_{2j} = \\gamma_{20} + U_{2j}\\) Combined \\(lg\\_rt_{ij} = \\gamma_{00} + \\gamma_{10}c\\_mean_{ij} + \\gamma_{20}c\\_sal_{ij} + U_{0j} + U_{1j}c\\_mean_{ij} + U_{2j}c\\_sal_{ij} + R_{ij}\\) Lets not estimate random effect covariances, so with this model, were estimating 7 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for c_mean and c_sal; \\(\\gamma_{10}\\): the fixed effect for the slope of c_mean, controlling for c_sal; \\(\\gamma_{20}\\): the fixed effect for the slope of c_sal, controlling for c_mean; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of people around the intercept, controlling for c_mean and c_sal; \\(\\tau_1^2\\): a random effect capturing how peoples slopes for c_mean vary around the grand mean slope, controlling for c_sal; \\(\\tau_2^2\\): a random effect capturing how peoples slopes for c_sal vary around the grand mean slope, controlling for c_mean; \\(\\sigma^2\\): a random effect capturing the variance of people around their own mean log reaction time, controlling for c_mean and c_sal. Lets run our model with random slope effects (but no covariances) in R: l1_random &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + (1|id) + (0 + c_mean|id) + (0 + c_sal|id), data = data, REML = FALSE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(l1_random) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + (1 | id) + (0 + c_mean | id) + (0 + c_sal | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16529.4 16578.0 -8257.7 16515.4 7639 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5320 -0.7287 -0.1087 0.6142 3.8842 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 1.756e-01 4.191e-01 ## id.1 c_mean 3.766e-09 6.137e-05 ## id.2 c_sal 6.853e-04 2.618e-02 ## Residual 4.777e-01 6.911e-01 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.605e+00 3.483e-02 1.533e+02 46.09 &lt;2e-16 *** ## c_mean -5.154e-02 4.301e-03 7.337e+03 -11.98 &lt;2e-16 *** ## c_sal -1.323e-01 7.724e-03 1.690e+02 -17.13 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean ## c_mean 0.044 ## c_sal -0.014 -0.211 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) We get an estimation error! Recall that singularity occurs when a variance term is close to zero or a correlation between variance terms is near 1 (high multicollinearity). Looking at our output, the variance terms for meaning and salience both look quite small. Lets try to address our estimation issue by removing the smaller of the two, the random effect for c_mean. l1_random_without_cmean &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + (1|id) + (0 + c_sal|id), data = data, REML = FALSE) summary(l1_random_without_cmean) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + (1 | id) + (0 + c_sal | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16527.4 16569.0 -8257.7 16515.4 7640 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5320 -0.7287 -0.1087 0.6142 3.8842 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.1756384 0.41909 ## id.1 c_sal 0.0006842 0.02616 ## Residual 0.4776817 0.69115 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.605e+00 3.483e-02 1.533e+02 46.09 &lt;2e-16 *** ## c_mean -5.154e-02 4.301e-03 7.352e+03 -11.98 &lt;2e-16 *** ## c_sal -1.323e-01 7.724e-03 1.690e+02 -17.13 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean ## c_mean 0.044 ## c_sal -0.014 -0.211 That took care of our singularity issue. However, the random effect for salience still looks very small. Lets conduct a deviance test to see if including the random effect reduces deviance at all (that is, whether it is worth it to estimate). anova(l1_random, l1_random_without_cmean) ## Data: data ## Models: ## l1_random_without_cmean: lg_rt ~ 1 + c_mean + c_sal + (1 | id) + (0 + c_sal | id) ## l1_random: lg_rt ~ 1 + c_mean + c_sal + (1 | id) + (0 + c_mean | id) + (0 + c_sal | id) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l1_random_without_cmean 6 16527 16569 -8257.7 16515 ## l1_random 7 16529 16578 -8257.7 16515 0 1 1 There is no significant difference between these models, so there doesnt seem to be much benefit to including the random effect for salience. 9.2.8 Adding Level-2 Fixed Effects The level-2 variables in our dataset are demographic variables about participants, which in this dataset is their sex, age in years, whether they 40 or older (oldage = 1) or younger than 40 (oldage = 0), or their age centered at 65 years old for those who are older than 40 (yrs65). Lets add oldage and sex as level-2 predictors of the intercept. Level Equation Level 1 \\(lg\\_rt_{ij} = \\beta_{0j} + \\beta_{1j}c\\_mean_{ij} + \\beta_{2j}c\\_sal_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}oldage_j + \\gamma_{02}sex_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) \\(\\beta_{2j} = \\gamma_{20}\\) Combined \\(lg\\_rt_{ij} = \\gamma_{00} + \\gamma_{01}oldage_j + \\gamma_{02}sex_j + \\gamma_{10}c\\_mean_{ij} + \\gamma_{20}c\\_sal_{ij} + U_{0j} + R_{ij}\\) Were estimating 7 effects: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for c_mean and c_sal; \\(\\gamma_{10}\\): the fixed effect for the slope of c_mean, controlling for c_sal; \\(\\gamma_{20}\\): the fixed effect for the slope of c_sal, controlling for c_mean; \\(\\gamma_{01}\\): the fixed effect for the slope of oldage, controlling for sex, c_mean, and c_sal; \\(\\gamma_{02}\\): the fixed effect for the slope of sex, controlling for oldage, c_mean and c_sal; \\(\\tau_0^2\\): a random effect capturing how peoples mean log reaction times vary around the grand mean log reaction time, controlling for c_mean, c_sal, oldage, and sex; \\(\\sigma^2\\): a random effect capturing the variance of people around their own mean log reaction time, controlling for c_mean, c_sal, oldage, and sex. l2_model &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + oldage + sex + (1|id), data = data, REML = FALSE) summary(l2_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + oldage + sex + (1 | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16269.8 16318.4 -8127.9 16255.8 7639 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6108 -0.7341 -0.1058 0.6151 3.9626 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.02424 0.1557 ## Residual 0.47852 0.6918 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.281e+00 2.566e-02 1.532e+02 49.915 &lt;2e-16 *** ## c_mean -5.150e-02 4.304e-03 7.494e+03 -11.965 &lt;2e-16 *** ## c_sal -1.323e-01 7.434e-03 7.495e+03 -17.801 &lt;2e-16 *** ## oldage 7.994e-01 3.087e-02 1.532e+02 25.891 &lt;2e-16 *** ## sex 4.467e-02 3.045e-02 1.525e+02 1.467 0.144 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean c_sal oldage ## c_mean 0.059 ## c_sal -0.017 -0.219 ## oldage -0.392 -0.002 -0.003 ## sex -0.681 0.001 -0.002 -0.075 The intercept of 1.28 is the mean log reaction time across all people at average values of meaning and salience for men (sex = 0) who are younger than 40 (oldage = 40). A one-unit increase in meaning is associated with a decrease in log reaction time of 0.05 (i.e., a faster reaction time), controlling for other variables. A one-unit increase in salience is associated with a decrease in log reaction time of 0.13 controlling for other variables. People older than 40 have 0.80 units longer of a log reaction time on average, controlling for other variables. Women have 0.04 unit-slower log reaction times on average, controlling for other variables. All coefficients are significant except for sex. The term describing how people vary around the grand mean intercept is 0.02. The term describing how people vary around their own intercept is 0.48. Lets run a model without the non-significant sex predictor and conduct a deviance test to see if the model fit is negatively impacted. # model l2_model_no_sex &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + oldage + (1|id), data = data, REML = FALSE) # deviance test anova(l2_model, l2_model_no_sex) ## Data: data ## Models: ## l2_model_no_sex: lg_rt ~ 1 + c_mean + c_sal + oldage + (1 | id) ## l2_model: lg_rt ~ 1 + c_mean + c_sal + oldage + sex + (1 | id) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l2_model_no_sex 6 16270 16312 -8129.0 16258 ## l2_model 7 16270 16318 -8127.9 16256 2.1363 1 0.1438 There is no significant difference in deviance, so we dont lose on the model fit front if we dont estimate sex. 9.2.9 Adding Cross-Level Interactions For our final model, lets remove the level-2 term for sex and look at a cross-level interaction between oldage and the slope of c_mean to consider the question: does age alter the effect of meaning on reaction times? Level Equation Level 1 \\(lg\\_rt_{ij} = \\beta_{0j} + \\beta_{1j}c\\_mean_{ij} + \\beta_{2j}c\\_sal_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}oldage_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}oldage_j\\) \\(\\beta_{2j} = \\gamma_{20}\\) Combined \\(lg\\_rt_{ij} = \\gamma_{00} + \\gamma_{01}oldage_j + \\gamma_{10}c\\_mean_{ij} + \\gamma_{20}c\\_sal_{ij} + \\gamma_{11}c\\_mean_{ij}*oldage_j + U_{0j} + R_{ij}\\) Were estimating 7 effects: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for c_mean, c_sal, and oldage; \\(\\gamma_{10}\\): the fixed effect for the slope of c_mean, controlling for c_sal and oldage; \\(\\gamma_{20}\\): the fixed effect for the slope of c_sal, controlling for c_mean and oldage; \\(\\gamma_{01}\\): the fixed effect for the slope of oldage, controlling for c_mean and c_sal; \\(\\gamma_{11}\\): the fixed effect for the cross-level interaction of oldage with c_mean, controlling for c_sal; \\(\\tau_0^2\\): a random effect capturing how peoples mean log reaction times vary around the grand mean log reaction time, controlling for c_mean, c_sal, and oldage; \\(\\sigma^2\\): a random effect capturing the variance of people around their own mean log reaction time, controlling for c_mean, c_sal, and `oldage``. crosslevel_model &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + oldage + oldage:c_mean + (1|id), data = data, REML = FALSE) summary(crosslevel_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + oldage + oldage:c_mean + (1 | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16253.8 16302.4 -8119.9 16239.8 7639 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6032 -0.7333 -0.0988 0.6125 3.9049 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.0247 0.1572 ## Residual 0.4774 0.6909 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.301e+00 1.895e-02 1.545e+02 68.674 &lt; 2e-16 *** ## c_mean -6.499e-02 5.337e-03 7.493e+03 -12.176 &lt; 2e-16 *** ## c_sal -1.325e-01 7.425e-03 7.495e+03 -17.838 &lt; 2e-16 *** ## oldage 8.154e-01 3.113e-02 1.561e+02 26.194 &lt; 2e-16 *** ## c_mean:oldage 3.717e-02 8.722e-03 7.495e+03 4.261 2.06e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean c_sal oldage ## c_mean 0.100 ## c_sal -0.025 -0.174 ## oldage -0.608 -0.057 -0.004 ## c_mean:oldg -0.058 -0.593 -0.005 0.095 The interaction between oldage and c_mean is 0.04, suggesting that people older than 40 have 0.04 added to their (log) reaction times for more meaningful photos. That is, older people have slower reaction times even when photos are more related to driving. The other coefficient interpretations are similar to those we discussed in earlier models. 9.3 Conclusion In this chapter, we reviewed our MLM pipeline and applied it to repeated measures without time in the model. In Chapter 10, we will look at longitudinal models, i.e., repeated measures with time in the model. "],["10-module-10.html", "Chapter 10 Multilevel Modelling with Longitudinal Data 10.1 Learning Objectives 10.2 Data Demonstration 10.3 Conclusion", " Chapter 10 Multilevel Modelling with Longitudinal Data 10.1 Learning Objectives In Chapter 9, we discussed how to interpret multilevel models with repeated measures data without time in the model. In this chapter, we will review fitting MLMs for longitudinal data, i.e., repeated measures with time in the model. The learning objectives for this chapter are: Recognize when data are longitudinal and in the correct format for multilevel modelling; Conduct multilevel modelling on longitudinal data; Interpret coefficients for longitudinal data; Review evidence to decide whether to retain effects. All materials for this chapter are available for download here. 10.2 Data Demonstration 10.2.1 Load Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for graphing library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for ICC 10.2.2 Multilevel Models for Longitudinal Data When we use cross-sectional data, level-1 variables vary between individuals (e.g., age, gender, SES) while level-2 variables vary between clusters (e.g., whether schools are public or private, number of students in a classroom, school funding). With repeated measures data without time in the model, level-1 variables vary between measurement occasions (e.g., experimental manipulations, stimulus volume or brightness) while level-2 variables vary between people (e.g., age, gender, SES). In this chapter, were looking at longitudinal models, i.e., repeated measures with time in the model. With longitudinal data, level-1 variables vary over time (e.g., weight at time 1, time 2, time 3; Testosterone levels at 10 minutes into game, 20 minutes into game, 30 minutes into game). Level-1 variables are also called time-varying covariates because they vary with time. Level-2 variables are the same across time (also called time invariant covariates), for example age, gender, SES (assuming these things dont change over the study period, if you wait long enough these will or may change). 10.2.2.1 Data Structures: Long vs Wide As with repeated measures, you need your data in long format to conduct an MLM. Imagine you were measuring weight and caloric intake. If you have historically worked with repeated measures data in an ANOVA framework, you are probably used to working with data in a wide format, i.e., one row per participant with different variables for different measurement instances. id weight1 weight2 calories1 calories2 1 200 190 3500 3300 2 150 160 3200 3100 In MLMs, you need to use data in a long format where one row is one measurement occasion: id weight calories measurement_occasion 1 200 3500 1 1 190 3300 2 2 150 3200 1 2 160 3100 2 10.2.3 Visualizing Testosterone Levels Over Time The data used in this chapter are a simplified version of the data from Casto, K.V., &amp; Edwards, D.A. (2016). Before, during, and after: How phases of competition differentially affect testosterone, cortisol, and estradiol levels in women athletes. Adaptive Human Behavior and Physiology, 2, 11-25.Â https://doi.org/10.1007/s40750-015-0028-2. Dr.Â Kathleen Casto gave us permission to use these data for teaching purposes (thanks Kathleen!); any other use requires additional permission. You can see more of her work here. data &lt;- read.csv(&#39;casto2016.csv&#39;) head(data) ## Code position HormonCont Played minplayed game time Cortisol Testosterone Estradiol time0 ## 1 1224 1 0 1 90 1 2 0.456 84.541 3.706 0 ## 2 1224 1 0 1 90 1 3 0.575 118.942 5.631 1 ## 3 1224 1 0 1 90 1 4 0.303 104.803 3.372 2 ## 4 1348 2 0 1 90 1 2 0.649 39.547 2.620 0 ## 5 1348 2 0 1 90 1 3 0.323 44.291 3.010 1 ## 6 1348 2 0 1 90 1 4 0.879 62.774 3.650 2 These data include hormone levels of female athletes in a competition setting (soccer game), if they played in the game or not (Played), what position they played (position), how long they played (minplayed), their testosterone levels (Testosterone), and if they were taking birth control (HormonCont). We will focus on testosterone, if they played, and if they were taking birth control for this example. Lets start by visualizing testosterone levels over time for the entire sample: data %&gt;% group_by(time0) %&gt;% mutate(tmean = mean(Testosterone)) %&gt;% # mean testosterone per timepoint ggplot(mapping = aes(x = time0, y = tmean)) + geom_line() + labs(title = &quot;Testosterone Over Time for Entire Sample&quot;) The relationship between time and testosterone doesnt look exactly linear. Lets check if there are different relationships between people who played and people who didnt. data %&gt;% group_by(time0, Played) %&gt;% # group by timepoint and played mutate(tmean = mean(Testosterone)) %&gt;% ggplot(mapping = aes(x = time0, y = tmean, colour = factor(Played))) + geom_line() + labs(title = &quot;Testosterone Over Time, Played vs. Did Not Play&quot;) Those who played have a pretty linear increase in testosterone over time, while those who did not play go up from time 0 to 1 and then back down between 1 and 2, so it looks like playing or not might be an important level-2 predictor for our model intercepts and slopes. Finally, lets check whether birth control seems like an important predictor of intercepts and slopes: data %&gt;% group_by(time0, HormonCont) %&gt;% # group by timepoint and birth control mutate(tmean = mean(Testosterone)) %&gt;% ggplot(mapping = aes(x = time0, y = tmean, colour = factor(HormonCont))) + geom_line() + labs(title = &quot;Testosterone Over Time, Birth Control or Not&quot;) Looks like another variable we might want to consider in our model! Lets begin with our null model and incrementally build from there. 10.2.4 Random-Intercept-Only/Null Model Lets estimate our null model with FIML as our estimator and calculate the ICC. Why FIML? Were going to be adding fixed effects (played, birth control) and comparing model fits. Level Equation Level 1 \\(testosterone_{ij} = \\beta_{0j} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) Combined \\(testosterone_{ij} = \\gamma_{00} + U_{0j} + R_{ij}\\) Here, were estimating three parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean testosterone levels across all people; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of peoples average testosterone levels around the intercept; \\(\\sigma^2\\): a random effect capturing the variance of people around their own average testosterone level. null_model &lt;- lmer(Testosterone ~ 1 + (1|Code), data = data, REML = FALSE) summary(null_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Testosterone ~ 1 + (1 | Code) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 634.5 641.4 -314.3 628.5 70 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6055 -0.5563 -0.2220 0.4901 2.8619 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Code (Intercept) 252.9 15.90 ## Residual 186.1 13.64 ## Number of obs: 73, groups: Code, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 59.643 3.569 24.876 16.71 4.94e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 performance::icc(null_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.576 ## Conditional ICC: 0.576 With longitudinal data, as with repeated measures without time, the ICC is interpreted as the proportion of variance between people: How much of the variance stems from people being different from one another versus fluctuating within themselves? A large ICC means that most of the variability is between people, not from people varying in their testosterone levels. The ICC is 0.576, indicating that 57.6% of the variance in testosterone is attributed to a person. 10.2.5 Adding Level-1 Fixed and Random Effects Lets add our level-1 predictor, time0, to our model. Note that time0 is coded such that 0 is the first measurement occasion at the beginning of the game, 1 is mid-game, and 2 is at the end of the game. Level Equation Level 1 \\(testosterone_{ij} = \\beta_{0j} + \\beta_{1j}time0_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + U_{1j}\\) Combined \\(testosterone_{ij} = \\gamma_{00} + \\gamma_{10}time0_{ij} + U_{0j} + U_{1j}time0_{ij} + R_{ij}\\) With this model, were estimating 6 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean testosterone levels at time 0 across all people; \\(\\gamma_{10}\\): the fixed effect for time0, effect of time on testosterone levels; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of peoples average testosterone levels around the intercept; \\(\\tau_1^2\\): a random effect for the slope of time0 capturing the variance of peoples slopes around the grand mean slope; \\(\\sigma^2\\): a random effect capturing the variance of people around their own average testosterone level. \\(\\tau_{01}\\): the covariance between the random intercept and random slope. Do people who have higher values of testosterone at time0 have particularly lower or higher slopes? Lets run the model with FIML as our estimator: l1_model &lt;- lmer(Testosterone ~ 1 + time0 + (time0|Code), data = data, REML = FALSE) summary(l1_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Testosterone ~ 1 + time0 + (time0 | Code) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 601.1 614.8 -294.5 589.1 67 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.65381 -0.55990 -0.07896 0.50762 2.39316 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Code (Intercept) 154.67 12.437 ## time0 24.12 4.911 0.87 ## Residual 78.58 8.865 ## Number of obs: 73, groups: Code, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 50.665 2.971 25.027 17.054 2.73e-15 *** ## time0 9.092 1.625 24.030 5.595 9.26e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## time0 0.114 as.matrix(Matrix::bdiag(VarCorr(l1_model))) ## (Intercept) time0 ## (Intercept) 154.67317 53.07523 ## time0 53.07523 24.11744 Looking at our fixed effects, the intercept of 50.67 is the average testosterone level across people at time 0. The slope of 9.09 indicates the average increase in testosterone levels over one unit of time. The variance term describing how people vary around the intercept is 154.67. The variance term describing how peoples slopes vary is 24.12. The covariance between the random intercept and random slope is 53.07 (correlation of 0.87), indicating that those with higher initial levels of testosterone also have higher slopes (more intense increase in testosterone over time). 10.2.6 Evidence for Retaining Effects The fixed effect for time seems both statistically and practically significant. Variances are difficult to interpret in isolation, so we can consider a few sources of evidence when examining whether to keep the random effect for slope: deviance testing, 95% confidence interval, 95% plausible values range, and visualizing variance. We can use the built-in anova() function to conduct our deviance test for the model with and without the slope covariance: l1_model_no_U1j &lt;- lmer(Testosterone ~ 1 + time0 + (1|Code), data = data, REML = FALSE) anova(l1_model, l1_model_no_U1j) ## Data: data ## Models: ## l1_model_no_U1j: Testosterone ~ 1 + time0 + (1 | Code) ## l1_model: Testosterone ~ 1 + time0 + (time0 | Code) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l1_model_no_U1j 4 607.23 616.39 -299.61 599.23 ## l1_model 6 601.09 614.83 -294.54 589.09 10.14 2 0.006281 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is significantly less deviance in the model with the slope variance term. Lets look at 95% confidence intervals for all effects: confint(l1_model, oldNames = F) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## sd_(Intercept)|Code 8.08885766 18.508662 ## cor_time0.(Intercept)|Code -0.02405199 1.000000 ## sd_time0|Code 1.82944849 9.035275 ## sigma 6.84599241 11.178406 ## (Intercept) 44.60763264 56.714968 ## time0 5.76797005 12.405566 Weve suppressed the warnings in this output for brevity, but if you run this code yourself youll see a lot of warnings! Thats likely because of the covariance term, given that the correlation cor_time0.(Intercept)|Code runs up against the upper bound of 1.00 within the confidence interval. Lets remove that term. l1_model_cov0 &lt;- lmer(Testosterone ~ 1 + time0 + (1|Code) + (0 + time0|Code), data = data, REML = FALSE) anova(l1_model, l1_model_cov0) ## Data: data ## Models: ## l1_model_cov0: Testosterone ~ 1 + time0 + (1 | Code) + (0 + time0 | Code) ## l1_model: Testosterone ~ 1 + time0 + (time0 | Code) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l1_model_cov0 5 602.67 614.12 -296.33 592.67 ## l1_model 6 601.09 614.83 -294.54 589.09 3.5849 1 0.05831 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our model fit is not significantly different after removing the covariance term, so lets proceed without it and look at our confidence intervals again: confint(l1_model_cov0, oldNames = F) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## sd_(Intercept)|Code 10.123069 20.33901 ## sd_time0|Code 2.941503 10.19590 ## sigma 6.549504 10.66364 ## (Intercept) 44.090984 57.21964 ## time0 5.589114 12.80020 The confidence interval for our slope variance, sd_time0|Code, does not contain 0, so it is significant. Finally, one way to visualize variability is with a graph: what are the different slope values across people? This is the same as our Empirical Bayes plotting exercise in Chapter 4. # Extract Empirical Bayes estimates and graph them as_tibble(coef(l1_model)$Code) %&gt;% ggplot(mapping = aes(x = time0)) + geom_histogram(bins = 5) To summarize our evidence: the model with the random slope effect fits better, the random effect confidence interval does not contain zero (i.e., it is significant), and we can see that slopes for time vary, many peoples slopes around 5 or 10 but some as high as ~20. All signs point to keeping the random effect. 10.2.7 Adding Level-2 Fixed Effects Lets start adding the level-2 effects we graphed at the beginning: playing and birth control. First, lets add whether someone played or not. Level Equation Level 1 \\(testosterone_{ij} = \\beta_{0j} + \\beta_{1j}time0_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}played_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}played_j + U_{1j}\\) Combined \\(testosterone_{ij} = \\gamma_{00} + \\gamma_{01}*played_j + \\gamma_{10}time0_{ij} + \\gamma_{11}time0_{ij}*played_j + U_{0j} + U_{1j}time0_{ij} + R_{ij}\\) With this model, were estimating 7 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean testosterone levels at time 0 across those who did not play (played = 0); \\(\\gamma_{10}\\): the fixed effect for time0, effect of time on testosterone levels controlling for playing; \\(\\gamma_{01}\\): the fixed effect for playing, effect of playing on testosterone at time0; \\(\\gamma_{11}\\): interaction term between playing and time, the effect of playing on the effect of time on testosterone levels; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of peoples average testosterone levels around the intercept controlling for time and playing; \\(\\tau_1^2\\): a random effect for the slope of time0 capturing the variance of peoples slopes around the grand mean slope controlling for playing; \\(\\sigma^2\\): a random effect capturing the variance of people around their own average testosterone level, controlling for time and playing; We are not estimating a covariance term. l2_played &lt;- lmer(Testosterone ~ 1 + time0 + Played + Played:time0 + (1|Code) + (0 + time0|Code), data = data, REML = FALSE) summary(l2_played) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Testosterone ~ 1 + time0 + Played + Played:time0 + (1 | Code) + (0 + time0 | Code) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 599.1 615.1 -292.5 585.1 66 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.5068 -0.5964 -0.1015 0.3243 2.4437 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Code (Intercept) 182.24 13.500 ## Code.1 time0 27.22 5.217 ## Residual 67.89 8.239 ## Number of obs: 73, groups: Code, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 44.526 5.479 26.886 8.126 1.03e-08 *** ## time0 3.929 2.942 25.406 1.335 0.1936 ## Played 9.004 6.638 26.811 1.356 0.1863 ## time0:Played 7.389 3.500 25.476 2.111 0.0448 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) time0 Played ## time0 -0.265 ## Played -0.825 0.218 ## time0:Playd 0.222 -0.840 -0.270 Looking at our fixed effects, the intercept of 44.53 is the average testosterone level at time 0 across people who didnt play. The slope of time, 3.93, indicates the average increase in testosterone levels over one unit of time controlling for playing. The slope of having played, 9.00, indicates that those who played had on average 9 more units of Testosterone than those who didnt at time 0. The interaction between time and having played indicates that those who played had a 7.39-higher slope on average than those who didnt. The variance term describing how people vary around the intercept is 182.24. The variance term describing how peoples time slopes vary is 27.22. Lets do a quick deviance test to see if including playing decreases deviance (i.e., improves model fit): anova(l1_model_cov0, l2_played) ## Data: data ## Models: ## l1_model_cov0: Testosterone ~ 1 + time0 + (1 | Code) + (0 + time0 | Code) ## l2_played: Testosterone ~ 1 + time0 + Played + Played:time0 + (1 | Code) + (0 + time0 | Code) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l1_model_cov0 5 602.67 614.12 -296.33 592.67 ## l2_played 7 599.09 615.12 -292.54 585.09 7.583 2 0.02256 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model with played does fit significantly better. Finally, lets add a fixed effect for birth control. Level Equation Level 1 \\(testosterone_{ij} = \\beta_{0j} + \\beta_{1j}time0_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}played_j + \\gamma_{02}birth\\_control_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}played_j + \\gamma_{12}birth\\_control_j + U_{1j}\\) Combined \\(testosterone_{ij} = \\gamma_{00} + \\gamma_{01}*played_j + \\gamma_{02}*birth\\_control_j + \\gamma_{10}time0_{ij} + \\gamma_{11}time0_{ij}*played_j + \\gamma_{12}time0_{ij}*birth\\_control_j + U_{0j} + U_{1j}time0_{ij} + R_{ij}\\) Here, were estimating 9 effects, the 7 previously described plus: \\(\\gamma_{02}\\): the fixed effect for birth control, effect of taking birth control on baseline testosterone levels controlling for playing; \\(\\gamma_{12}\\): interaction term between playing and birth control. l2_played_birthcontrol &lt;- lmer(Testosterone ~ 1 + time0 + Played + time0:Played + HormonCont + time0:HormonCont + (1|Code) + (0 + time0|Code), data = data, REML = FALSE) summary(l2_played_birthcontrol) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Testosterone ~ 1 + time0 + Played + time0:Played + HormonCont + ## time0:HormonCont + (1 | Code) + (0 + time0 | Code) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 600.4 621.0 -291.2 582.4 64 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.5413 -0.5573 -0.1349 0.3886 2.4161 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Code (Intercept) 159.27 12.620 ## Code.1 time0 27.99 5.291 ## Residual 67.37 8.208 ## Number of obs: 73, groups: Code, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 50.5745 6.4512 26.8271 7.840 2.07e-08 *** ## time0 4.0889 3.5168 25.6439 1.163 0.2557 ## Played 9.2374 6.3078 26.9286 1.464 0.1547 ## HormonCont -9.7086 6.1205 26.8019 -1.586 0.1244 ## time0:Played 7.4842 3.5225 25.5433 2.125 0.0435 * ## time0:HormonCont -0.3946 3.3160 25.6701 -0.119 0.9062 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) time0 Played HrmnCn tm0:Pl ## time0 -0.287 ## Played -0.652 0.187 ## HormonCont -0.591 0.169 -0.024 ## time0:Playd 0.183 -0.668 -0.280 0.007 ## tm0:HrmnCnt 0.171 -0.543 0.007 -0.289 -0.067 Matrix::bdiag(VarCorr(l2_played_birthcontrol)) ## 2 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] 159.2723 . ## [2,] . 27.99435 anova(l2_played, l2_played_birthcontrol) ## Data: data ## Models: ## l2_played: Testosterone ~ 1 + time0 + Played + Played:time0 + (1 | Code) + (0 + time0 | Code) ## l2_played_birthcontrol: Testosterone ~ 1 + time0 + Played + time0:Played + HormonCont + time0:HormonCont + (1 | Code) + (0 + time0 | Code) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l2_played 7 599.09 615.12 -292.54 585.09 ## l2_played_birthcontrol 9 600.36 620.97 -291.18 582.36 2.7264 2 0.2558 Taking birth control is associated with a drop in Testosterone by 9.71 units, controlling for all other variables, and the interaction between time and birth control was -0.39, indicating that the slope for time is 4.09 - 0.39 = 3.70 for people taking birth control. Neither effect was statistically significant. 10.3 Conclusion In this chapter, we estimated and interpreted models for longitudinal data and reviewed some of the evidence available to us in making model construction decisions: deviance testing, 95% confidence intervals, and visualizing Empirical Bayes estimates. In Chapter 11, we will review another source of evidence for model-building and interpretation: effect sizes and R-squared in multilevel models. "],["11-module-11.html", "Chapter 11 Effect Sizes in Multilevel Models 11.1 Learning Objectives 11.2 Data Demonstration 11.3 Conclusion 11.4 Additional Reading", " Chapter 11 Effect Sizes in Multilevel Models 11.1 Learning Objectives In this chapter we will discuss effect sizes in multilevel models, with a particular focus on R-squared. Note that this is an adaptation of a published work, Shaw, Rights, Sterba, and Flake (2022). The learning objectives for this chapter are: Define effect sizes; Understand the components of Rights &amp; Sterbas (2019) framework for R-squared in MLMs; Implement and interpret R-squared results for single models, via automatic and manual entry; Select models to compare for a given research question and interpret comparison output. All materials for this chapter are available for download here. 11.2 Data Demonstration 11.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(r2mlm) # for R-squared values library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for ICC We will be using the teachsat dataset included in the r2mlm package. This is simulated data about teacher job satisfaction. Teachers are clustered within schools. The level-1 variables are school-centered salary and control over curriculum, and the level-2 variables are school average salary, school average control over curriculum, and student-teacher ratio. data(teachsat) 11.2.2 Defining Effect Sizes Effect sizes are important for contextualizing the magnitude of results from your model. Effect sizes go beyond statistical significance to ask what is the practical significance of this result? Do we care about the effect of this predictor? Effect sizes can be standardized or unstandardized (expressed in the units of the dependent variable). Regression coefficients are one example of an unstandardized effect size that weve already seen in earlier chapters: for a one-unit increase in SES, math scores increase/decrease by some amount. If that coefficient is very small, like 0.01 points, even if its statistically significant were probably not that interested in a 0.01% increase in test scores. Unstandardized effect sizes like this are useful when the units are interpretable, like with math test scores. Standardized effect sizes measure magnitude without units, and as such can be particularly useful when original metrics are not particularly interpretable (e.g., log reaction times, scores on Likert scales that dont have inherent meaning). 11.2.3 R-squared in Multilevel Models One example of a standardized effect size is R-squared, or proportion of variance in the outcome explained by a model. In single-level regression, it is calculated as the outcome variance explained by the model divided by the total outcome variance: \\[R^2 = \\frac{explained\\ variance}{total\\ variance}\\] This yields an intuitive variance explained measure ranging from 0 to 1, with 0 indicating 0% explained and 1 indicating 100% explained. In multilevel regression, a single R-squared term cannot accurately capture variance explained because there are multiple sources of variance and kinds of predictors explaining that variance. As we have discussed, we partition total variance into within and between variance, so we have three possible denominators: total outcome variance, outcome variance within a cluster, and outcome variance between clusters. Further, we have multiple sources of explained variance, so we have four possible unique numerators: fixed effects at level-1, fixed effects at level-2, a random intercept effect, and random slope effects. With all these sources of variance and options for explaining variance, a single R-squared value cannot capture variance explained for MLMs. Rights &amp; Sterba (2019) first detailed a comprehensive framework that accounts for all these sources of variance and variance explained and can be referenced for a more detailed explanation of the framework. Here, we provide an overview of the 12 R-squared terms in the framework. 11.2.3.1 Within Variance Explained At the within level of the model, there are three possible sources of variance: the level-1 predictors via the fixed effects (shorthand: f1), the level-1 predictors via the random effects (shorthand: v), and the level-1 residuals (shorthand: resid). Our denominator for within R-squareds contains these three terms, because the sum of all three represents the total within variance: \\[R^2_{within} = \\frac{explained\\ variance}{var_{f1} + var_{v} + var_{resid}}\\] You can then calculate two distinct effect sizes from this. The first is within variance explained by level 1 predictors via fixed effects: \\[R^{2(f1)}_{within} = \\frac{var_{f1}}{var_{f1} + var_{v} + var_{resid}}\\] The second is within variance explained by random slopes: \\[R^{2(v)}_{within} = \\frac{var_{v}}{var_{f1} + var_{v} + var_{resid}}\\] You can also calculate a higher-level term for variance explained by BOTH level-1 fixed effects and random slopes: \\[R^{2(f1v)}_{within} = R^{2(f1)}_{within} + R^{2(v)}_{within} = \\frac{var_{f1} + var_{v}}{var_{f1} + var_{v} + var_{resid}}\\] Were not calculating an \\(R^{2(resid)}_{within}\\) with \\(var_{resid}\\) in the numerator because the residuals are all variance left after accounting for all predictors, so it doesnt make sense to have variance explained by residual variance. Regarding notation: a given R-squared is described by two elements: a subscript and a superscript. The subscripts indicate at what level variance is being explained: within for within-cluster, between for between-cluster, and total for total. The superscripts indicate what potential sources of variance are contributing to variance explained: f1 for level 1 predictors via fixed effects, f2 for level-2 predictors via fixed effects, and so on. 11.2.3.2 Between Variance Explained Between variance is composed of the contribution of level 2 predictors via fixed effects (shorthand: f2) and cluster-specific means via intercept variation (shorthand: m): \\[R^2_{between} = \\frac{explained\\ variance}{var_{f2} + var_{m}}\\] We have two R-squared terms here. The first is for between-variance explained by level-2 fixed effects: \\[R^{2(f2)}_{between} = \\frac{var_{f2}}{var_{f2} + var_{m}}\\] The second is for between-variance explained by random intercept variation: \\[R^{2(m)}_{between} = \\frac{var_{m}}{var_{f2} + var_{m}}\\] We are not calculating a higher-level term \\[R^{2(f2m)}_{between} = \\frac{var_{f2} + var_{m}}{var_{f2} + var_{m}}\\] because all variance between clusters is captured by fixed-effects at level-2 and random intercept variation, so this term would always be equal to 1. 11.2.3.3 Total Variance Explained Total variance then is the combination of within and between variance explained, and thus total R-squared measures take the following form: \\[R^2_{total} = \\frac{explained\\ variance}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] There are four possible unique R-squared terms here. The first is total variance explained by fixed effects at level-1: \\[R^{2(f1)}_{total} = \\frac{var_{f1}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] The second is total variance explained by fixed effects at level-2: \\[R^{2(f2)}_{total} = \\frac{var_{f2}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] The third is total variance explained by random slope variation: \\[R^{2(v)}_{total} = \\frac{var_{v}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] And the fourth is total variance explained by random intercept variation: \\[R^{2(m)}_{total} = \\frac{var_{m}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] There are also three higher-level terms. The first is total variance explained by fixed effects at both level-1 and level-2: \\[R^{2(f)}_{total} = R^{2(f1)}_{total} + R^{2(f2)}_{total} = \\frac{var_{f1} + var_{f2}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] The second is total variance explained by fixed effects at both level-1 and level-2 and random slope variation: \\[R^{2(fv)}_{total} = R^{2(f1)}_{total} + R^{2(f2)}_{total} + R^{2(v)}_{total} = \\frac{var_{f1} + var_{f2} + var_v}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] The third and final higher-level term is total variance explained by fixed effects at level-1 and level-2, random slope variation, and random intercept variation: \\(R^{2(fvm)}_{total} = R^{2(f1)}_{total} + R^{2(f2)}_{total} + R^{2(v)}_{total} + R^{2(m)}_{total}= \\frac{var_{f1} + var_{f2} + var_v + var_m}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\) You could also calculate higher-level terms \\(R^{2(f1v)}_{total}\\), \\(R^{2(f2v)}_{total}\\), \\(R^{2(f1m)}_{total}\\), \\(R^{2(f2m)}_{total}\\), \\(R^{2(f1vm)}_{total}\\), and \\(R^{2(f2vm)}_{total}\\) if you wanted by summing the appropriate individual terms, but those are not automatically calculated for you in the r2mlm function well see in a moment because theyre not as widely substantively interesting as the other combinations. In all, we have 12 R-squared measures: 3 within measures, 2 between measures, and 7 total measures. Here is a table for your quick reference: Measure Definition/Interpretation \\[R^{2(f1)}_{total} = \\frac{var_{f1}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by level-1 predictors via fixed slopes \\[R^{2(f2)}_{total} = \\frac{var_{f2}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by level-2 predictors via fixed slopes \\[R^{2(v)}_{total} = \\frac{var_{v}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by level-1 predictors via random slope variation/covariation \\[R^{2(m)}_{total} = \\frac{var_{m}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by cluster-specific outcome means via random intercept variation \\[R^{2(f)}_{total} = \\frac{var_{f1} + var_{f2}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by all predictors via fixed slopes \\[R^{2(fv)}_{total} = \\frac{var_{f1} + var_{f2} + var_v}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by predictors via fixed slopes and random slope variation/covariation \\[R^{2(fvm)}_{total} = \\frac{var_{f1} + var_{f2} + var_v + var_m}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by predictors via fixed slopes and random slope variation/covariation and by cluster-specific outcome means via random intercept variation \\[R^{2(f1)}_{within} = \\frac{var_{f1}}{var_{f1} + var_{v} + var_{resid}}\\] Proportion of within-cluster outcome variance explained by level-1 predictors via fixed slopes \\[R^{2(v)}_{within} = \\frac{var_{v}}{var_{f1} + var_{v} + var_{resid}}\\] Proportion of within-cluster outcome variance explained by level-1 predictors via random slope variation/covariation \\[R^{2(f1v)}_{within} = \\frac{var_{f1} + var_{v}}{var_{f1} + var_{v} + var_{resid}}\\] Proportion of within-cluster outcome variance explained by level-1 predictors via fixed slopes and random slope variation/covariation \\[R^{2(f2)}_{between} = \\frac{var_{f2}}{var_{f2} + var_{m}}\\] Proportion of between-cluster outcome variance explained by level-2 predictors via fixed slopes \\[R^{2(m)}_{between} = \\frac{var_{m}}{var_{f2} + var_{m}}\\] Proportion of between-cluster outcome variance explained by cluster-specific outcome means via random intercept variation Lets look at an example with our teacher job satisfaction data to develop our intuition for using this framework and interpreting its results. 11.2.4 Single Model, Automatic Entry Lets begin with a null model with teacher job satisfaction (satisfaction) as the outcome with school (schoolID) as the clustering variable and REML as the estimator. Level Equation Level 1 \\(satisfaction_{ij} = \\beta_{0j} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) Combined \\(satisfaction_{ij} = \\gamma_{00} + U_{0j} + R_{ij}\\) Were estimating 3 parameters here: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean teacher job satisfaction across all schools; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools average job satisfaction levels around the intercept; \\(\\sigma^2\\): a random effect capturing the variance of teachers around their school average job satisfaction. null_model &lt;- lmer(satisfaction ~ 1 + (1|schoolID), data = teachsat, REML = TRUE) summary(null_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: satisfaction ~ 1 + (1 | schoolID) ## Data: teachsat ## ## REML criterion at convergence: 30098.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8269 -0.6385 0.0012 0.6435 3.2874 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schoolID (Intercept) 0.699 0.836 ## Residual 1.516 1.231 ## Number of obs: 9000, groups: schoolID, 300 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 5.99677 0.04998 299.00000 120 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The fixed effect for our intercept is 6.00, indicating the average teacher job satisfaction across all schools. The random effect describing how schools vary around that average is 0.70, and the random effect describing how teachers vary around their school averages is 1.52. Lets use the automatic r2mlm() function for the first time to calculate how much variance in teacher job satisfaction is explained by the null model. r2mlm(null_model) ## $Decompositions ## total within between ## fixed, within 0 0 NA ## fixed, between 0 NA 0 ## slope variation 0 0 NA ## mean variation 0.315546785367943 NA 1 ## sigma2 0.684453214632058 1 NA ## ## $R2s ## total within between ## f1 0 0 NA ## f2 0 NA 0 ## v 0 0 NA ## m 0.315546785367943 NA 1 ## f 0 NA NA ## fv 0 0 NA ## fvm 0.315546785367943 NA NA There are three parts to our output: Decompositions, R2s, and a graph. The decompositions output gives us all of our unique R-squared estimates. The R2s output gives us the unique output plus the higher-level combination output: (1) variance explained by all fixed effects at both level-1 and level-2 (represented by f), (2) variance explained by all fixed effects and random slope variances (fv), and (3) variance explained by all fixed effects, random slope variances, and the random intercept variance (fvm). The graph visualizes the unique R-squared estimates from the Decompositions output and includes a legend. Looking at our output, we can see that the variance explained by our random intercept, m, is 0.316, or 31.6%. What is this number? Its the ICC, the variance attributed to school membership! We can use our familiar ICC function to see that it matches our r2mlm output: performance::icc(null_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.316 ## Conditional ICC: 0.316 Lets now use r2mlm to fit a larger model predicting job satisfaction from salary (with a random slope), control over curriculum, and student-teacher ratio: Level Equation Level 1 \\(Y_{satisfaction} = \\beta_{0j} + \\beta_{1j}*salary\\_c_{ij} + \\beta_{2j}*control\\_c_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}*s\\_t\\_ratio_{j} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) \\(\\beta_{2j} = \\gamma_{20} + U_{2j}\\) Combined \\(Y_{satisfaction} = \\gamma_{00} + \\gamma_{01}*s\\_t\\_ratio_{j} + \\gamma_{10}*salary\\_c_{ij} + \\gamma_{20}*control\\_c_{ij} + U_{0j} + U_{2j}*control\\_c_{ij} + R_{ij}\\) We will be estimating 8 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean job satisfaction levels controlling for salary, curriculum-control, and student-teacher ratio; \\(\\gamma_{01}\\): the fixed effect for student-teacher ratio, effect of school-wide class size on job satisfaction controlling for salary and curriculum; \\(\\gamma_{10}\\): the fixed effect for salary_c, effect of salary on job satisfaction within a school, controlling for curriculum and student-teacher ratio; \\(\\gamma_{20}\\): the fixed effect for control_c, effect of control over curriculum on job satisfaction within a school, controlling for salary and student-teacher ratio; \\(\\tau_0^2\\): a random effect for the intercept capturing the variance of schools average job satisfaction around the intercept controlling for salary, curriculum, and student-teacher ratio; \\(\\tau_2^2\\): a random effect for the slope of curriculum control capturing the variance of schools curriculum slopes around the grand mean slope controlling for salary and student-teacher ratio; \\(\\sigma^2\\): a random effect capturing the variance of teachers around their school average job satisfaction, controlling for salary, curriculum, and student-teacher ratio; \\(\\tau_{02}\\): the covariance between the random intercept and random slope. Do schools with higher job satisfaction intercepts have higher/lower effects of curriculum? full_model &lt;- lmer(satisfaction ~ 1 + control_c + salary_c + s_t_ratio + (1 + control_c | schoolID), data = teachsat, REML = TRUE) summary(full_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: satisfaction ~ 1 + control_c + salary_c + s_t_ratio + (1 + control_c | schoolID) ## Data: teachsat ## ## REML criterion at convergence: 24507.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.6115 -0.6275 0.0108 0.6414 3.7958 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schoolID (Intercept) 0.57478 0.7581 ## control_c 0.02826 0.1681 0.07 ## Residual 0.76561 0.8750 ## Number of obs: 9000, groups: schoolID, 300 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 7.186e+00 1.442e-01 2.982e+02 49.824 &lt; 2e-16 *** ## control_c 3.113e-01 1.136e-02 2.974e+02 27.398 &lt; 2e-16 *** ## salary_c 7.413e-02 1.078e-03 8.535e+03 68.752 &lt; 2e-16 *** ## s_t_ratio -3.718e-02 4.285e-03 2.980e+02 -8.676 2.71e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) cntrl_ slry_c ## control_c 0.017 ## salary_c 0.000 -0.004 ## s_t_ratio -0.951 0.000 0.000 Matrix::bdiag(VarCorr(full_model)) ## 2 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## (Intercept) control_c ## (Intercept) 0.574775228 0.008596084 ## control_c 0.008596084 0.028261325 The average teaching satisfaction across all schools at average levels of curriculum control and salary and a student-teacher ratio of 0 is 7.19. (Note: this is an example of an uninterpretable zero point because there is no meaning behind a student-teacher ratio of zero. However, for demonstration purposes were going to keep going and ignore that issue.) A one-unit increase in control over the curriculum within a school is associated with a 0.31-unit increase in teacher job satisfaction (on the 1-10 scale), controlling for salary and student-teacher ratio. A one-unit increase in salary within school is associated with a 0.07-unit increase in job satisfaction, controlling for curriculum control and student-teacher ratio. A one-unit increase in student-teacher ratio (i.e., one more student per class; all classes in one school have the same number of students) is associated with a 0.03-unit decrease in job satisfaction, at school average salary and control over curriculum. Looking at random effects, the term describing how the intercepts vary across schools is 0.57. The term describing how schools curriculum-control slopes vary around the grand mean is 0.03. The term describing how teachers intercepts vary around the grand mean intercept is 0.77. The covariance between the random intercept and random slope of curriculum is 0.01, so there is a negligible relationship between intercept and slope values. Recall that interpreting these coefficients in this way  a one-unit change is associated with an X-unit change in the outcome  is also an embodiment of effect sizes. Now lets also calculate and interpret R-squared for this model. r2mlm(full_model) ## $Decompositions ## total within between ## fixed, within 0.296431806719555 0.440263091958242 NA ## fixed, between 0.0676695868874132 NA 0.207134501406624 ## slope variation 0.0318477856338068 0.0473006076180897 NA ## mean variation 0.259024355588986 NA 0.792865498593376 ## sigma2 0.34502646517024 0.512436300423669 NA ## ## $R2s ## total within between ## f1 0.296431806719555 0.440263091958242 NA ## f2 0.0676695868874132 NA 0.207134501406624 ## v 0.0318477856338068 0.0473006076180897 NA ## m 0.259024355588986 NA 0.792865498593376 ## f 0.364101393606968 NA NA ## fv 0.395949179240775 0.487563699576332 NA ## fvm 0.65497353482976 NA NA With our larger model, our R-squared output is filling out. Lets look at our Decompositions output, the top of the output printed to the console. The first column represents total variance explained. Reading from top to bottom, we can see that for this model: Fixed effects at level-1 explain 29.6% of total variance in teacher job satisfaction Fixed effects at level-2 explain 6.8% of total variance in teacher job satisfaction Random slope variation explains 3.2% of total variance in teacher job satisfaction Random intercept variation explains 25.9% of total variance in teacher job satisfaction Remaining residual variance is 34.5% of total variance in teacher job satisfaction The second column represents within variance explained. Reading from top to bottom, we can see that for this model: Fixed effects at level-1 explain 44% of within variance in teacher job satisfaction Random slope variation explains 4.7% of within variance in teacher job satisfaction Remaining residual variance is 51.2 of within variance in teacher job satisfaction The third column represents between variance explained. Reading from top to bottom, we can see that for this model: Fixed effects at level-2 explain 20.7% of between variance in teacher job satisfaction Random intercept variation explains 79.3% of between variance in teacher job satisfaction The second output printed to console, R2s, repeats this information using the f1, f2, f, v, m notation discussed above and includes the higher-level R-squared combinations. In the first column: All fixed effects at level-1 and level-2 (f) explain 36.4% of total variance in teacher job satisfaction All fixed effects plus random slope variation (fv) accounts for 40.0% of total variance in teacher job satisfaction All fixed effects and random slope variation plus random intercept variation (fvm) account for 65.5% of total variance in teacher job satisfaction In the second column: Fixed effects at level-2 and random slope variation (f1v) account for 48.8% of within variance in teacher job satisfaction A graph is printed to visualize this information and facilitate understanding and interpreting it. 11.2.5 Single Model, Manual Entry It is convenient but not always possible to use automatic entry where you feed r2mlm your model name and it extracts the necessary information and calculates the R-squareds. For example, perhaps you estimated your model in a different software like MPlus or SPSS and dont have a model object to give the function. Or, you might have a complex model that doesnt work with the automatic entry function (for example, containing higher-order terms created with the I() function). In cases where using the automatic function isnt possible, there is r2mlm_manual(). It takes the following arguments: Parameter Definition For Our Example data your dataset teachsat within_covs list of numbers or variable names corresponding to the column numbers/names in your dataset for level-1 predictors our level-1 predictors are control_c and salary_c, which are in the fourth and fifth columns of our dataset, so c(4, 5) between_covs list of numbers or variable names corresponding to the column numbers/names in your dataset for level-2 predictors our level-2 predictor is student-teacher ratio, which is the eighth column in our dataset, so c(8) random_covs list of numbers or variable names corresponding to the column numbers/names in your dataset for level-1 predictors with random effects control has a random effect, so c(4) gamma_w list of fixed slope estimates for level-1 predictors in the order listed in within_covs our fixed slope estimates for the level-1 predictors are c(0.311, 0.074) gamma_b list of intercept estimate (if applicable) followed by fixed slope estimates for level-2 predictors in the order listed in between_covs our intercept and level-2 fixed slope estimates are c(7.186, -0.037) Tau random effect covariance matrix. The first row/column denotes the intercept variances and covariances; set to 0 if intercept is fixed. Subsequent rows/columns denote random slope variances and covariances in the order listed in random_covs our Tau matrix is matrix(c(0.575, 0.009, 0.009, 0.028), 2, 2) sigma2 level-1 residual variance the level-1 residual variance is 0.766 has_intercept true/false indicating whether your model estimates an intercept; default value of true our model does have an intercept, so TRUE clustermeancentered true/false indicating whether your level-1 predictors are centered-within-cluster; default value of true our level-1 predictors are centered within cluster, so TRUE bargraph indicate whether you want a bar graph wed like to see the graph, so TRUE If we enter these values into r2mlm_manual(), well get the same results as from our automatic function: r2mlm_manual(data = teachsat, within_covs = c(4, 5), between_covs = c(8), random_covs = c(4), gamma_w = c(0.311, 0.074), gamma_b = c(7.186, -0.037), Tau = matrix(c(0.575, 0.009, 0.009, 0.028), 2, 2), sigma2 = 0.766, has_intercept = TRUE, clustermeancentered = TRUE) ## $Decompositions ## total within between ## fixed, within 0.296022422439019 0.439625147440015 NA ## fixed, between 0.0671264807082975 NA 0.205500898247648 ## slope variation 0.0316015152901635 0.0469316503266841 NA ## mean variation 0.259521632661036 NA 0.794499101752352 ## sigma2 0.345727948901484 0.513443202233301 NA ## ## $R2s ## total within between ## f1 0.296022422439019 0.439625147440015 NA ## f2 0.0671264807082975 NA 0.205500898247648 ## v 0.0316015152901635 0.0469316503266841 NA ## m 0.259521632661036 NA 0.794499101752352 ## f 0.363148903147317 NA NA ## fv 0.39475041843748 0.486556797766699 NA ## fvm 0.654272051098516 NA NA 11.2.6 Model Comparison In some earlier chapters, we calculated proportion of variance reduced to get a sense of the impact of adding or removing a predictor using the equation \\(\\frac{variance\\ model\\ A - variance\\ model\\ B}{variance\\ model\\ A}\\). The issue with calculating variance reduced this way is it can yield impossible negative values. Model B might have more variance than model A at a given level. This would suggest that variance increases when adding a predictor to a model, when really the variance has been re-allocated elsewhere. We can more accurately quantify differences between models using r2mlm. To do so, we run two models and give them to the r2mlm_comp() function. Lets compare our full model with a model without the level-2 fixed effect of student-teacher ratio. First, create a model object for this reduced-size model: reduced_model &lt;- lmer(satisfaction ~ 1 + control_c + salary_c + (1 + control_c | schoolID), data = teachsat, REML = TRUE) Then, we run r2mlm_comp(): r2mlm_comp(full_model, reduced_model) ## $`Model A R2s` ## total within between ## f1 0.296431806719555 0.440263091958242 NA ## f2 0.0676695868874132 NA 0.207134501406624 ## v 0.0318477856338068 0.0473006076180897 NA ## m 0.259024355588986 NA 0.792865498593376 ## f 0.364101393606968 NA NA ## fv 0.395949179240775 0.487563699576332 NA ## fvm 0.65497353482976 NA NA ## ## $`Model B R2s` ## total within between ## f1 0.29656715948444 0.440275210292464 NA ## f2 0 NA 0 ## v 0.0318597529079789 0.0472980873398502 NA ## m 0.326405047226169 NA 1 ## f 0.29656715948444 NA NA ## fv 0.328426912392419 0.487573297632314 NA ## fvm 0.654831959618589 NA NA ## ## $`R2 differences, Model B - Model A` ## total within between ## f1 1.353528e-04 1.211833e-05 NA ## f2 -6.766959e-02 NA -0.2071345 ## v 1.196727e-05 -2.520278e-06 NA ## m 6.738069e-02 NA 0.2071345 ## f -6.753423e-02 NA NA ## fv -6.752227e-02 9.598056e-06 NA ## fvm -1.415752e-04 NA NA This output contains three groups of R2s: The R2s for Model A The R2s for Model B The differences between Model A and Model B  these are what we reference to see variance explained differences between models Lets look at the R2 differences, Model B - Model A output, the third group of R2s printed to the console. The first column is for the differences in total variance explained. There are minuscule differences between the models in total variance explained by level-1 fixed effects (f1), and random slope variation (v). There is a -6.8% difference in total variance explained by level-2 fixed effects (f2); because these differences are calculated as Model B - Model A and the result is negative, Model B explains 6.8% less total variance with level-2 fixed effects, which makes sense because we got rid of the level-2 fixed effect student-teacher ratio. That 6.8% is instead allocated to random intercept variation (m). Note that this is one example where our old method \\(\\frac{variance\\ model\\ A - variance\\ model\\ B}{variance\\ model\\ A}\\) might make it look like variance increases in model B, when really it is being redistributed. The second column is for within variance explained, and there are functionally no differences between models, which makes sense given that we didnt touch our level-1 predictors that explain within-school variance, school_c and control_c. The third column represents differences in explained job satisfaction variance between schools. Model B explains 20.7% less due to level-2 fixed effects (f2), which again is because we removed that predictor. That variance is re-allocated to intercept variation (m). For visualizing these results, five graphs are also printed: A full decomposition for Model A A full decomposition for Model B A side-by-side of total variance decompositions for Model A and Model B A side-by-side of within variance decompositions for Model A and Model B A side-by-side of between variance decompositions for Model A and Model B In this case, Model A and Model B are nested. That is, all terms in Model B are in Model A, Model B is just missing one. If we were comparing two unnessted models (say, one with just salary_c as a predictor and one with just control_c as a predictor, neither model is nested within the other), we would have to provide a data argument to the function: r2mlm_comp(modelA, modelB, data). Like with the single-model r2mlm() function, there is a manual function for model comparison as well: r2mlm_comp_manual. 11.3 Conclusion In this chapter, we reviewed the definition of effect size, detailed a comprehensive framework for R-squared in MLMs, and practiced calculating and interpreting R-squareds using the r2mlm function. In the next (final!) chapter, we will look at the assumptions that underpin all multilevel modelling. 11.4 Additional Reading Rights, J. D., &amp; Sterba, S. K. (2019). Quantifying explained variance in multilevel models: An integrative framework for defining R-squared measures. Psychological Methods, 24(3), 309338. https://doi.org/10.1037/met0000184 Rights, J. D., &amp; Sterba, S. K. (2020). New recommendations on the use of r-squared differences in multilevel model comparisons. Multivariate Behavioral Research, 55(4), 568599. https://doi.org/10.1080/00273171.2019.1660605 Shaw, M., Rights, J. D., Sterba, S. K., &amp; Flake, J. K. (in-press). r2mlm: An R Package Calculating R-Squared for Multilevel Models. Behavior Research Methods. "],["12-module-12.html", "Chapter 12 Assumptions 12.1 Learning Objectives 12.2 Data Demonstration 12.3 Conclusion", " Chapter 12 Assumptions 12.1 Learning Objectives In this chapter we will review assumptions of MLMs and some ways to check them in R. The learning objectives for this chapter are: Understand the assumptions underpinning MLMs; Develop and interpret R code and output for testing assumptions. All materials for this chapter are available for download here. 12.2 Data Demonstration 12.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(lme4) # for multilevel models library(lmerTest) # for p-values library(dplyr) # for data manipulation library(ggplot2) # for graphing The data for chapter are a subsample from the 1982 High School Beyond data collected by the National Center for Educational Statistics, used as example data throughout Raudenbush, S. W., &amp; Bryk, A. S. (2002). Hierarchical Linear Models: Applications and Data Analysis Methods: SAGE Publications. These data are related to student math achievement. data &lt;- read.csv(&#39;rb2002.csv&#39;) 12.2.2 Assumptions of MLMs In brief, the assumptions underlying MLMs are as follows: The model is correctly specified (i.e., all the predictors associated with the outcome and relevant random effects are included); The functional form is correct (e.g., the relationship between the predictors and outcome is linear if using a linear model); Level-1 residuals are independent and normally distributed; Level-2 residuals are independent and multivariate normally distributed; Residuals at level-1 and level-2 are unrelated; Predictors at one level are not related to errors at another level. # 1. Model is correctly specified # 2. The functional form is correct -- Q1-3 # 3. At L2, residuals are independent and normally distributed with a mean of 0 and some variance -- Q5 # 4. L1 predictors are independent of L1 residuals for all predictors -- Q6 # 5. L2 residuals are multivariate normal and are independent from one another and across L2 units -- # 6. L2 predictors are unrelated to L2 residuals -- Q12-13 # 7. Residuals at L1 and L2 are unrelated -- Q14 # 8. Predictors at one level are not related to errors at another level -- Q15 In the data demonstration that follows, we will be showing some techniques for checking these assumptions. However, we note that there are many other ways to conduct similar checks; assumption checks are data exploration at the end of the day. If you know and prefer other methods of exploring the same facets of your data, go for it! 12.2.3 Assumption 1: Model Specification Specifying your model means determining which predictors should be included and what parameters to estimate. A correctly-specified model is one that includes everything associated with your outcome and nothing not-associated. This is impossible to know for certain and most researchers are constrained by the resources they have to get data and ultimately by the data they have. A general approach for considering this is trying to include all possible relevant IVs when predicting a DV. When building your MLM, build a maximal model with all possibe random effects, and trim down as you see effects are zero. Comparing models with and without effects and reviewing the literature for important variables to consider are decent strategies here. Get experts to read your results and their ideas on if anything is missing! 12.2.4 Assumption 2: Functional Form is Correct The functional form of your model refers to the function connecting your predictors and outcome. In linear models like MLMs, that form is (you guessed it!) linear. However, you can use MLMs for non-linear data by specifying other functions, which are not covered in the current set of modules. In our example data, lets look at the relationship between an outcome of math achievement (mathach) and a predictor of socioeconomic status (ses). We can get a sense of whether that relationship is linear with a scatterplot: data %&gt;% ggplot(mapping = aes(x = ses, y = mathach)) + geom_point() ## Warning: Removed 1 rows containing missing values (geom_point). The scatterplot is hard to read because we have over 11,000 observations in our data. One option to improve readability is reducing the opacity of each point to get a clearer picture of how many points there are: data %&gt;% ggplot(mapping = aes(x = ses, y = mathach)) + geom_point(alpha = .2) ## Warning: Removed 1 rows containing missing values (geom_point). It looks loosely linear. Before we continue, for teaching purposes well take a subset of 30 schools to make plotting easier for the rest of this data demo. For your data, you should consider all observations; this is just to make examples clearer. ID ADD EMPHASIS TO THE PART ABOUT THEIR DATA, ABOVE data &lt;- data %&gt;% filter(SCHOOL %in% head(unique(SCHOOL), n = 30)) Lets take a look at the relationship between SES and math achievement for each of the 30 schools in our subsetted data. data %&gt;% ggplot() + geom_point(mapping = aes(x = ses, y = mathach)) + facet_wrap(~ SCHOOL) Looks like we have vaguely linear relationships; nothing looks quadratic. This is a subjective determination, be open to other, non-linear functions if a straight line doesnt appear to do the job. Finally, lets graph all points on one plane again but with regression lines overlaid for each school. CURRENT HTML HAS NO REGRESSION LINES ON THE SCHOOLS data %&gt;% group_by(SCHOOL) %&gt;% ggplot(mapping = aes(x = ses, y = mathach, colour = factor(SCHOOL))) + geom_point(show.legend = FALSE) + geom_smooth(method = lm, se = FALSE, show.legend = FALSE, fullrange = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; This isnt an explicit test of functional form, but it looks like there is variance in intercepts and slopes across schools that merits multilevel modelling (or cluster-robust standard errors, if you dont have any multilevel research questions). 12.2.5 An Aside: Extracting Residuals In linear regression, we have the assumption of homoscedasticity. That is, the variance of residuals is roughly the same at any value of of your predictor(s). In multilevel models, we have two classes of residuals: level-1 residuals and level-2 residuals. The remaining assumptions all deal with these residuals: that they are normally distributed, that they dont relate to each other, that they dont relate to predictors at the same level or at the other level. Were going to look at a model with math achievement predicted by SES centered within cluster (school) and mean SES predicting the intercept and slope of SES centered within cluster. Well estimate a random effect for the slope of SES centered within cluster. Were not estimating the random effect covariance. Level Equation Level 1 \\(mathach_{ij} = \\beta_{0j} + \\beta_{1j}cwc\\_ses + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{10}ses\\_mean + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}ses\\_mean + U_{1j}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}ses\\_mean + \\gamma_{10}cwc\\_ses + \\gamma_{11}cwc\\_ses*ses\\_mean + U_{0j} + U_{1j}cwc\\_ses + R_{ij}\\) model &lt;- lmer(mathach ~ CWCses*ses_mean + (1|SCHOOL) + (0 + CWCses|SCHOOL), data = data, REML = TRUE) summary(model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: mathach ~ CWCses * ses_mean + (1 | SCHOOL) + (0 + CWCses | SCHOOL) ## Data: data ## ## REML criterion at convergence: 8555.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.87105 -0.70607 -0.03647 0.76336 2.77351 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SCHOOL (Intercept) 1.990 1.411 ## SCHOOL.1 CWCses 1.379 1.174 ## Residual 35.315 5.943 ## Number of obs: 1329, groups: SCHOOL, 30 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.8243 0.3078 28.9279 41.661 &lt; 2e-16 *** ## CWCses 1.9581 0.3412 27.3088 5.739 4.05e-06 *** ## ses_mean 6.5269 0.6499 28.8676 10.044 6.28e-11 *** ## CWCses:ses_mean 1.5239 0.7253 27.7253 2.101 0.0448 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) CWCses ses_mn ## CWCses 0.000 ## ses_mean -0.026 0.000 ## CWCss:ss_mn 0.000 0.030 0.000 Lets extract the level-1 residuals and add them to our data to work with moving forward: data$l1resid &lt;- residuals(model) head(data$l1resid) ## [1] -1.9478535 10.1937749 10.7268788 -0.5893637 7.6105198 -6.0281684 Well extract the level-2 residuals shortly. 12.2.6 Assumption 3: Level-1 Residuals are Independent and Normally Distributed 12.2.6.1 Independent Our level-1 residuals should be independent of our level-1 predictors. For our example, if our level-1 residuals (\\(\\sigma^2\\)) correlate to SES centered within cluster, we have heteroscedasticity, which we dont want. We can check this visually by creating a scatterplot of the predictor and the residuals: data %&gt;% ggplot(mapping = aes(x = CWCses, y = l1resid)) + geom_point() + labs(x = &quot;CWCses&quot;, y = &quot;residuals&quot;) Those dont look correlated; they look like a blob. We can check this statistically by calculating the correlation. cor.test(data$l1resid, data$CWCses) ## ## Pearson&#39;s product-moment correlation ## ## data: data$l1resid and data$CWCses ## t = -3.0409e-15, df = 1327, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.05377213 0.05377213 ## sample estimates: ## cor ## -8.347562e-17 That correlation is essentially zero, meaning that our level-1 residuals and level-1 predictor are indeed uncorrelated! 12.2.6.2 Normally Distributed We can check whether our level-1 residuals are normally distributed with a histogram: data %&gt;% ggplot(mapping = aes(x = l1resid)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Those look normal-ish  theres maybe something odd going on in that the distribution is bimodal, but that can be influenced by the number of bins. For example, with fewer bins: data %&gt;% ggplot(mapping = aes(x = l1resid)) + geom_histogram(bins = 15) Lets clear this up with a QQ plot: data %&gt;% ggplot(mapping = aes(sample = l1resid)) + stat_qq() Those look fine. SAY WHY THEY LOOK FINE AND HOW TO TELL IF THEY ARE FINE 12.2.7 Assumption 4: Level-2 Residuals are Independent and Multivariate Normal To check our level-2 residual assumptions, we need to extract the level-2 residuals and add them to our data. This requires a little more work than extracting the level-1 residuals. We could easily add level-1 residuals to our dataset because every row is a person and had a residual. Level-2 residuals are at the school level, so the vector of residuals cannot be appended as easily. Lets create a level-2 dataset to work with instead. l2_data &lt;- data %&gt;% group_by(SCHOOL) %&gt;% # group data by clustering variable, school mutate( mathach_mean = mean(mathach) # create mean math achievement per school ) %&gt;% select(SCHOOL, ses_mean, mathach_mean) %&gt;% unique() # select unique rows (rather than having school, ses_mean, and mathach_mean repeating over and over again) Then lets add the level-2 residuals to this dataset. We get two columns of residuals with the ranef(model)$SCHOOL command; the first column is intercept residuals \\(U_{0j}\\)s, the second column the slope residuals \\(U_{1j}\\)s. Matrices are indexed as [row, column], so we use the [, 1] code to get all rows from column 1 and [, 2] to get all rows from column 2. l2_data$intercept_resid = ranef(model)$SCHOOL[, 1] l2_data$slope_resid = ranef(model)$SCHOOL[, 2] 12.2.7.1 Independent Like with level-1 residuals, we want our level-2 residuals to be independent from level-2 predictors. We also want both our level-2 residuals  intercept and slope residuals  to be independent of each other (which was not an issue at level-1, given that we only had one category of residual, \\(\\sigma^2\\)). We can check whether our level-2 residuals are independent from the level-2 predictors and from one another as we did before: with a scatterplot and/or by calculating the correlation among them. First, lets look at the relationship between the intercept residuals and the level-2 variable of mean SES. l2_data %&gt;% ggplot(mapping = aes(x = intercept_resid, y = ses_mean)) + geom_point() And the correlation: cor.test(l2_data$ses_mean, l2_data$intercept_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: l2_data$ses_mean and l2_data$intercept_resid ## t = -7.7435e-14, df = 28, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3602692 0.3602692 ## sample estimates: ## cor ## -1.463377e-14 Those look uncorrelated to me! Next, lets check that the slope residuals are independent from the predictor of mean SES: l2_data %&gt;% ggplot(mapping = aes(x = slope_resid, y =ses_mean)) + geom_point() cor.test(l2_data$ses_mean, l2_data$slope_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: l2_data$ses_mean and l2_data$slope_resid ## t = 3.6749e-15, df = 28, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3602692 0.3602692 ## sample estimates: ## cor ## 6.944986e-16 Those also look uncorrelated! Finally, lets check whether the level-2 residuals are independent of each other: l2_data %&gt;% ggplot(mapping = aes(x = slope_resid, y = intercept_resid)) + geom_point() cor.test(l2_data$intercept_resid, l2_data$slope_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: l2_data$intercept_resid and l2_data$slope_resid ## t = -3.9046, df = 28, p-value = 0.0005424 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.7859072 -0.2970177 ## sample estimates: ## cor ## -0.5937517 JESS THIS SEEMS WEIRD, is it just that we should be modelling the covariance between intercepts and slopes? YEAH I WOULD STATE THIS AS, OUR MODEL ASSUMES THEY ARE INDEP, BUT WE CAN RELAX THIS BY ADDING THE COVARIANCE. WHICH WE KNOW IS THERE BECAUSE THE CORR IS ONE, IT WOULD BE BETTER TO FIX THE CORR TO 1 THAN TO CONSTRAIN IT TO ZERO, DO YOU KNOW IF YOU CAN DO THIS IN LME4? 12.2.7.2 Multivariate Normal As we did with our level-1 residuals, we can check the normality of our level-2 residuals with histograms and QQ plots. First, intercept residuals: l2_data %&gt;% ggplot(mapping = aes(x = intercept_resid)) + geom_histogram(binwidth = .75) l2_data %&gt;% ggplot(mapping = aes(sample = intercept_resid)) + stat_qq() Those look roughly normal, but a bit wonky. WE NEED TO ADD SOMETHING HERE ABOUT WHAT THE RAMIFICATIONS ARE OF NON NORMAL RESIDUALS. Next, our slope residuals: l2_data %&gt;% ggplot(mapping = aes(x = slope_resid)) + geom_histogram(binwidth = .50) l2_data %&gt;% ggplot(mapping = aes(sample = slope_resid)) + stat_qq() JESS CAN YOU WRITE ABOUT MULTIVARIATE NORMALITY HERE pls EVENTUALLY BUT NOT KNOW BECAUSE IM NEARLY 4 HOURS INTO THIS AND AM OUT OF TIME, EEEP. 12.2.8 Assumption 5: Residuals at Level-1 and Level-2 are Independent Our residuals at level-1 and level-2 should not be related to each other. We can check that with scatterplots and calculating correlations. To do so, we first need to add our level-2 residuals to our level-1 dataset. As mentioned, each school only has one level-2 intercept residual and one level-2 slope residual, but as many level-1 residuals as there are students in that school To add our level-2 residuals to the dataset, then, we need to replicate them X times, where X is the number of students in a school. For example, if a school has 10 students, we need to repeat the level-2 residuals 10 times. (This is how level-2 predictors are encoded, too: for a school with 10 students, the mean values of SES for that school repeats 10 times in the dataset.) To do that, lets use the replication function rep with the times argument to repeat each value X times. We can find the number of students per school as follows: n_per_school &lt;- data %&gt;% group_by(SCHOOL) %&gt;% # group by school select(SCHOOL) %&gt;% # we just want to count schools count() %&gt;% ungroup() %&gt;% select(n) %&gt;% unlist() Then lets create vectors of level-1 and level-2 residuals that repeat X times: data$intercept_resid &lt;- rep(l2_data$intercept_resid, times = n_per_school) data$slope_resid &lt;- rep(l2_data$slope_resid, times = n_per_school) Now that we have a dataset with all of our residuals, lets assess whether the level-1 and level-2 residuals are correlated with our trusty scatterplots and correlations. First, level-2 intercept residuals and level-1 residuals. data %&gt;% ggplot(mapping = aes(x = l1resid, y = intercept_resid)) + geom_point() cor.test(data$l1resid, data$intercept_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: data$l1resid and data$intercept_resid ## t = 2.7251, df = 1327, p-value = 0.006512 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.02091225 0.12785972 ## sample estimates: ## cor ## 0.07460049 This looks odd at first, but the striation is because for any one level-2 residual there are multiple level-1 residuals. This looks decent; for each level-2 residual, there seems to be a decent spread of level-1 residuals. We would be concerned if level-1 residuals were clustering so that, say, for lower level-2 residuals there were also lower level-1 residuals or vice versa. Our correlation is also quite small, emphasizing that things seem fine. Next, level-1 residuals with level-2 slope residuals: data %&gt;% ggplot(mapping = aes(x = l1resid, y = slope_resid)) + geom_point() cor.test(data$l1resid, data$slope_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: data$l1resid and data$slope_resid ## t = -1.666, df = 1327, p-value = 0.09595 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.099214249 0.008106189 ## sample estimates: ## cor ## -0.04568585 This similarly looks fine. 12.2.9 Assumption 6: Level-1 Residuals Independent of Level-2 Predictors, Level-2 Residuals Independent of Level-1 Predictors Weve tested that our residuals are independent of predictors at their same level  level-1 residuals with ses_cwc and level-2 residuals with ses_mean. We also want residuals to be independent of predictors at the other level  level-1 residuals independent of ses_mean and level-2 residuals independent of ses_cwc. How do we test this? Scatterplots and correlations, of course. Lets check level-1 residuals first: data %&gt;% ggplot(mapping = aes(x = l1resid, y = ses_mean)) + geom_point() cor.test(data$l1resid, data$ses_mean) ## ## Pearson&#39;s product-moment correlation ## ## data: data$l1resid and data$ses_mean ## t = -4.2298e-14, df = 1327, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.05377213 0.05377213 ## sample estimates: ## cor ## -1.161143e-15 The scatterplot looks like there might be a pattern where lower values on ses_mean are associated with higher residuals, but the correlation belies this. Next level-2 intercept residuals: data %&gt;% ggplot(mapping = aes(x = intercept_resid, y = CWCses)) + geom_point() cor.test(data$intercept_resid, data$CWCses) ## ## Pearson&#39;s product-moment correlation ## ## data: data$intercept_resid and data$CWCses ## t = -2.7815e-15, df = 1327, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.05377213 0.05377213 ## sample estimates: ## cor ## -7.635609e-17 Looks good! And finally level-2 slope residuals: data %&gt;% ggplot(mapping = aes(x = slope_resid, y = CWCses)) + geom_point() cor.test(data$slope_resid, data$CWCses) ## ## Pearson&#39;s product-moment correlation ## ## data: data$slope_resid and data$CWCses ## t = -1.9141e-15, df = 1327, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.05377213 0.05377213 ## sample estimates: ## cor ## -5.254428e-17 Looks great! 12.3 Conclusion We could have a longer conclusion here, or a new chapter? SOMETHING ABOUT THIS BEING SUBJECTIVE? I THINK WHAT NEEDS TO HAPPEN HERE IS THAT YOU REVIEW THE CHAPTER IN THE TEXTBOOK FROM CLASS AND WE SEE IF WE CAN BEEF THIS UP. THIS IS ALWAYS THE ONE IVE BEEN THE MOST THIN ON, AND ID LIKE TO TAKE THIS AS AN OPPORTUNIY TO MAKE SURE IT IS A BIT MORE THOROUGH. JUST READING A TEXTBOOK CHAPTER ON THIS AND ADDING SOME MORE DISCUSSION AND/OR EXAMPLES WOULD DO THE TRICK. I PROBABLY PULLED THIS FROM THE BOOK, BUT I DONT QUITE REMEMBER. CAN YOU TAKE A LOOK AND ENSURE NOTHING MAJOR IS MISSING? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
