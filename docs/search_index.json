[["index.html", "Introduction to Multilevel Modelling Welcome About the Authors Funding", " Introduction to Multilevel Modelling Mairead Shaw and Jessica Kay Flake Welcome Bonjour/hi, and welcome to learn-mlms.com. This website will teach you the fundamentals about multilevel modelling, from why and when you would use them and how to do so for various research questions and data structures. This website is free to use and is licensed under the Creative Commons Attribution 4.0 International, so instructors can feel free to borrow and alter materials to structure their own courses. If you notice any errors or would otherwise like to contribute, feel free to file an issue on GitHub. About the Authors Mairead Shaw is a graduate student in the Quantitative Psychology and Modelling area at McGill University. Her research interests center around effect sizes in multilevel models and measurement considerations for multi-group and replication research. Dr. Jessica Flake is an Assistant Professor of Quantitative Psychology and Modelling at McGill University. She received an MA in quantitative psychology from James Madison University and a a PhD in Measurement, Evaluation, and Assessment from the University of Connecticut. Her work focuses on technical and applied aspects of psychological measurement including scale development, psychometric modelling, and scale use and replicability. Funding These materials were made possible by funding from the APS Fund for Teaching and Public Understanding of Psychological Science. You can read more about the fund here. "],["01-module-1.html", "Chapter 1 Introduction 1.1 Overview 1.2 Goals 1.3 Prerequisites 1.4 Materials", " Chapter 1 Introduction 1.1 Overview These materials focus on conceptual foundations of multilevel models (MLMs), specifiying them, and interpreting the results. Topics include multilevel data and approaches to dependence, specifying and interpreting fixed and random effects, model estimation, centering, repeated measures and longitudinal models, assumptions testing, and effect sizes in MLMs. 1.2 Goals These materials are intended for students and instructors. By the end of this course, students will be able to: Estimate variance components and interpret the intraclass correlation coefficient; Decide if and when a multilevel model is needed; Specify and build multilevel models with covariates at level 1 and 2 with both cross-sectional and repeated measures designs; Interpret regression coefficients and variance components from multilevel models; Assess the assumptions of multilevel models; Calculate effect sizes for multilevel models. 1.3 Prerequisites Readers should be comfortable with multiple linear regression, including building regression models, interpreting regression output, and testing for and interpreting regression coefficients including interactions. The first module reviews multiple regression and can be used to gauge your preparedness for continuing. For those wishing to brush up their regression skills before working through these materials, we recommend UCLA’s Statistical Methods and Data Analytics resources and online seminars: https://stats.oarc.ucla.edu/other/mult-pkg/seminars/ The worked examples will be conducted using lme4 in R. The lme4 documentation provides details of the workings of lme4, for interested readers. 1.4 Materials All materials are available for download in the appendix. The following are available for download: Data: the data used in each chapter R Script: an R script of the code used in each chapter Worksheet: a worksheet with questions that follows a similar structure to each chapter, but without answers provided We recommend that people self-studying download the data and R script and following along with the code and output interpretations in each chapter. Instructors can benefit from downloading the data, code, and worksheets for use in a lab portion in their classes. "],["02-multiple-regression.html", "Chapter 2 Multiple Regression Review 2.1 Learning Objectives 2.2 Data Demonstration 2.3 Conclusion", " Chapter 2 Multiple Regression Review 2.1 Learning Objectives In this module, we will review simple and multiple linear regression to establish a strong foundation before moving onto multilevel models. Note that this is intended more as review than a comprehensive guide to regression; for the latter, we recommend https://stats.oarc.ucla.edu/other/mult-pkg/seminars/. All materials for this chapter are available for download here. The learning objectives for this chapter are: Understand using file paths for project management/loading data; Review using simple and multiple linear regression to analyze data. 2.2 Data Demonstration In this data demo, we will first review setting up an R session, then simple and multiple linear regression. The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. These data have a multilevel structure, which we will work with in chapter 3, but for this chapter we will ignore the clustering structure and conduct regular regression. The following variables are in this data set: Variable Level Description Values Measurement schcode School School identifier (419 schools) Integer Ordinal Rid Individual A within-group level identifier representing a sequential identifier for each student within 419 schools. 1 to 37 Ordinal id Individual Student identifier (6,871 students) Integer Ordinal female Individual Student sex 0 = Male, 1 = Female Scale ses Individual Z-score measuring student socioeconomic status composition within the schools -2.41 to 1.87 Scale femses Individual Grand-mean-centered variable measuring student socioeconomic status by gender (female) -2.41 to 1.85 Scale math Individual Student math achievement test score 27.42 to 99.98 Scale ses_mean School Grand-mean-centered variable measuring student socioeconomic status -1.30 to 1.44 Scale pro4yrc School Aggregate proportion of students who intend to study at 4-year universities 0.00 to 1.00 Scale public School Dichotomous variable identifying school type 0 = Other, 1 = Public School Scale 2.2.1 Creating R Projects Before we get into analyzing the data, let’s start by creating a new project file for this module. R project files help you keep all of the files associated with your project – data, R scripts, and output (including figures) – in one location so you can easily navigate everything related to your project. To create a project, open R, click “File” and “New Project…”. If you have already created a folder for this chapter, you can add an R Project to that folder by clicking “Existing Directory”; the R project file will take on the name of that folder. If you do not already have a folder, click “New Directory,” choose where you want to put your new folder and what you want to call it. The R Project file will again take on the name of your new folder. 2.2.2 Loading Data and Dependencies Next, let’s load in the data and packages we’ll be using for this demo. We’ll be using the following packages: library(ggplot2) # for data visualization library(magrittr) # for pipe, %&gt;% You must install a given package before you can use it. For example: install.package(\"ggplot2\"). Once you have installed a package, you can load it into any future sessions with library(package_name). Next, let’s read in the data. If you have your code and data in the same directory, you can read the data in as follows: data &lt;- read.csv(&#39;heck2011.csv&#39;) This is called a relative file path, because you’re telling your computer where to find the data relative to your current folder (a folder can also be called a “directory”). You could also use an absolute file path that fully states where your files are located, like: read.csv(&#39;/Users/maireadshaw/open_mlm_materials/heck2011.csv&#39;) Let’s calculate some descriptive statistics and compare them to the above table to make sure we read our data in correctly. summary(data) ## schcode Rid id female ses ## Min. : 1.0 Min. : 1.000 Min. : 1 Min. :0.0000 Min. :-2.4140 ## 1st Qu.:103.5 1st Qu.: 5.000 1st Qu.:1718 1st Qu.:0.0000 1st Qu.:-0.5180 ## Median :209.0 Median : 9.000 Median :3436 Median :1.0000 Median : 0.0150 ## Mean :209.4 Mean : 9.196 Mean :3436 Mean :0.5025 Mean : 0.0319 ## 3rd Qu.:314.5 3rd Qu.:13.000 3rd Qu.:5154 3rd Qu.:1.0000 3rd Qu.: 0.6050 ## Max. :419.0 Max. :37.000 Max. :6871 Max. :1.0000 Max. : 1.8730 ## femses math ses_mean pro4yrc public ## Min. :-2.4140000 Min. :27.42 Min. :-1.29673 Min. :0.0000 Min. :0.0000 ## 1st Qu.:-0.0175000 1st Qu.:52.41 1st Qu.:-0.30525 1st Qu.:1.0000 1st Qu.:0.0000 ## Median : 0.0000000 Median :60.67 Median :-0.01050 Median :1.0000 Median :1.0000 ## Mean : 0.0000984 Mean :57.73 Mean : 0.03179 Mean :0.8689 Mean :0.7306 ## 3rd Qu.: 0.0000000 3rd Qu.:62.38 3rd Qu.: 0.29560 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. : 1.8540000 Max. :99.98 Max. : 1.44315 Max. :1.0000 Max. :1.0000 That looks good, so let’s proceed to conducting regressions. 2.2.3 Simple Linear Regression Let’s run a simple linear regression predicting math achievement (math) from socioeconomic status (ses). The syntax for the lm() (linear modelling) command in R is lm(DV ~ IV1 + IV2 + ... + IVn, data = dataframe). model1 &lt;- lm(math ~ ses, data = data) summary(model1) ## ## Call: ## lm(formula = math ~ ses, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.459 -4.678 1.144 5.355 47.560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 57.59817 0.09819 586.61 &lt;2e-16 *** ## ses 4.25468 0.12566 33.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.132 on 6869 degrees of freedom ## Multiple R-squared: 0.143, Adjusted R-squared: 0.1429 ## F-statistic: 1146 on 1 and 6869 DF, p-value: &lt; 2.2e-16 The intercept from this regression is 57.60, indicating that students at the mean level of SES within a school (i.e., when SES = 0, given that SES is z-scored) have an average math achievement score of 57.6 out of 100. This score is significantly different from 0, per the p-value. Per the coefficient for SES, a one-unit increase in SES is associated with a 4.25-point increase in student math achievement on average, also significant. The adjusted R-squared value is 14.3%, indicating that 14.3% of the variance in math achievement is explained by socioeconomic status. We can visualize this relationship by graphing a scatter plot. ggplot(data = data, mapping = aes(x = ses, y = math)) + geom_point() Our graph reflects the positive relationship between SES and math achievement (and also shows a lot of math scores collecting around the 60 mark). 2.2.4 Multiple Regression Next, let’s add the available sex variable female (0 = male, 1 = female) as a predictor in our regression and interpret the coefficients and R-squared value. model2 &lt;- lm(math ~ ses + female, data = data) summary(model2) ## ## Call: ## lm(formula = math ~ ses + female, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -30.923 -4.606 1.176 5.317 48.054 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 58.1330 0.1390 418.093 &lt; 2e-16 *** ## ses 4.2269 0.1255 33.679 &lt; 2e-16 *** ## female -1.0626 0.1960 -5.422 6.09e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.115 on 6868 degrees of freedom ## Multiple R-squared: 0.1467, Adjusted R-squared: 0.1464 ## F-statistic: 590.3 on 2 and 6868 DF, p-value: &lt; 2.2e-16 The intercept of 58.13 reflects the average math achievement score (out of 100) for male students (female = 0) at their class average SES (ses = 0). For a one-unit increase in SES, math achievement increases by 4.23 points, controlling for sex. Female students had a math achievement score lower by 1.06 points on average, controlling for SES. SES and sex together explain 14.6% of the variance in math achievement. 2.2.5 Interaction Terms In the previous model, we assumed that the relationship between SES and math achievement was constant for both sexes (homogeneity of regression slopes, i.e., an ANCOVA model). As a final exercise, let’s add an interaction term to our regression between sex and SES. model3 &lt;- lm(math ~ ses + female + ses:female, data = data) summary(model3) ## ## Call: ## lm(formula = math ~ ses + female + ses:female, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -30.975 -4.596 1.172 5.288 48.265 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 58.1441 0.1393 417.506 &lt; 2e-16 *** ## ses 4.0533 0.1775 22.840 &lt; 2e-16 *** ## female -1.0737 0.1961 -5.475 4.54e-08 *** ## ses:female 0.3472 0.2510 1.383 0.167 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.115 on 6867 degrees of freedom ## Multiple R-squared: 0.1469, Adjusted R-squared: 0.1465 ## F-statistic: 394.2 on 3 and 6867 DF, p-value: &lt; 2.2e-16 # Could also succinctly code it as follows: # lm(math ~ ses*female, data = data) An interaction captures that the relationship between two variables may differ based on the level of another variable (i.e., different slopes for different folks). An interaction term, A:B, has two possible interpretations: The effect of A on the effect of B on your outcome Y. The effect of B on the effect of A on your outcome Y. The ses:female interaction term, .34, represents the effect of being female on the relationship between SES and math achievement. Alternatively, it could represent the effect of SES on the relationship between being female and math achievement. In this case, the latter is a more intuitive interpretation: female students from higher socioeconomic statuses are slightly insulated from the negative relationship between female and math achievement in this sample. As SES increases by one point, the relationship between being female and math achievement becomes less negative, from -1.07 to -.73 (-1.07 + .34). However, this interaction term is not statistically significantly different from zero per the p-value. We can see this graphically using the sjPlot package: sjPlot::plot_model(model3, type = &quot;pred&quot;, terms = c(&quot;ses&quot;, &quot;female&quot;)) As we can see, math scores for males (female = 0, the red line) are higher than those for females (female = 1, the blue line) at all levels of SES. However, the difference between males and females shrinks with increasing SES, as indicated by math scores at higher SES levels being closer than those at lower levels of SES. The other coefficients have the same interpretations as before. The R-squared indicates that SES and sex account for 14.7% of the variance in math achievement. 2.3 Conclusion If you feel comfortable with the material presented in this data demonstration, then you have a sufficiently strong baseline to move forward with the materials. In this chapter, we ignored that students were clustered into schools; in the next chapter, we’ll examine that clustering, consider its implications for our analyses, and introduce one non-multilevel-model method for handling clustered data. "],["03-module-3.html", "Chapter 3 Approaches to Multilevel Data 3.1 Learning Objectives 3.2 Data Demonstration 3.3 Conclusion 3.4 Further Reading", " Chapter 3 Approaches to Multilevel Data 3.1 Learning Objectives In this chapter, we will discuss implications of clustered data and review non-multilevel-modelling options for handling that clustering. The learning objectives for this chapter are: Understand the implications of treating clustered data as unclustered; Use cluster-robust standard errors to account for clustering; Compare results between regular and cluster-robust regression. All materials for this chapter are available for download here. 3.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the dataset. 3.2.1 Load Data and Dependencies First, let’s load in the data and packages we’ll be using for this data demo. We will use the following packages: library(dplyr) # for data processing library(lmtest) # for cluster-robust standard errors library(sandwich) # for cluster-robust standard errors We’ll store the data in an object called data. If you want to read the data in with the relative filepath (i.e., just referencing “heck2011.csv”), make sure the file is in the same folder as your R script. If the file is in a different folder, tell your computer exactly where to find it with an absolute file path. data &lt;- read.csv(&#39;heck2011.csv&#39;) 3.2.2 Dealing with Dependence Our dataset is clustered, students within schools, but in Chapter 2 we treated it as if it were not clustered, i.e., as if each student was randomly selected from a population of students, regardless of school. When we treat clustered data as unclustered, we bias the significance testing for our models such that we are more likely to make a Type I error. A t value is based on dividing a regression coefficient by the standard error: \\(t = \\frac{b}{SE}\\). The standard error is a quotient of the standard deviation and the square root of the degrees of freedom, the sample size n: \\(SE = \\frac{\\sigma}{\\sqrt n}\\). When we assume that data are unclustered, we act like we have a larger sample size than we do (e.g., 100 independent observations rather than 10 classes of 10 students), which reduces the standard error and inflates the t-value, making it more likely that our coefficients will be significant. We have multiple options for dealing with dependence. Multilevel models are one option, accounting for the clustered data structure by quantifying how the clusters vary across the entire sample. For example, looking at student math achievement, we can move beyond having a single intercept to having a mean intercept across all schools and a term representing how schools’ mean math achievements vary around the mean intercept. Multilevel models are a powerful tool! But in providing more information to the researcher (e.g., how schools vary around the grand mean intercept), they also require more input from the researcher: do we expect our intercepts to vary? Do we expect our slopes to vary? If so, which slopes? Sometimes, we don’t need an MLM (and the assumptions that come along with it) because we don’t want to ask questions at multiple levels, like “how does student SES and teacher years of experience affect student math achievement?” We just want to know “how does student SES affect student math achievement?” In this case, we might think of the clustering in our data as a nuisance, something to be handled so that our standard errors aren’t biased, but not theoretically investigated. In such a case where we want to run a single-level regression that controls bias in the standard errors, we can use cluster-robust standard errors. 3.2.3 Cluster-Robust Standard Errors Cluster-robust standard errors account for clustering but retain the interpretation of regular regression models. That is, the coefficients do not delineate within and between effects, but provide average effects pooled across the whole dataset and unbiased standard errors that take into account the clustering. For more information on clustered standard errors, see this helpful overview: https://mldscenter.maryland.gov/egov/Publications/ResearchSeries/Clustered%20Data,%20Are%20Multilevel%20Models%20Really%20Necessary.pdf. Let’s look at cluster-robust standard errors in action! In chapter 2, we conducted a linear regression predicting math achievement from socioeconomic status and sex. Let’s run that same model. model &lt;- lm(math ~ ses + female, data = data) summary(model) ## ## Call: ## lm(formula = math ~ ses + female, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -30.923 -4.606 1.176 5.317 48.054 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 58.1330 0.1390 418.093 &lt; 2e-16 *** ## ses 4.2269 0.1255 33.679 &lt; 2e-16 *** ## female -1.0626 0.1960 -5.422 6.09e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.115 on 6868 degrees of freedom ## Multiple R-squared: 0.1467, Adjusted R-squared: 0.1464 ## F-statistic: 590.3 on 2 and 6868 DF, p-value: &lt; 2.2e-16 Now, let’s run the same model with cluster-robust standard errors and compare the coefficients and their significance between the regular and cluster-robust models. model_crse &lt;- coeftest(model, vcov = vcovCL, cluster = ~ schcode) model_crse ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 58.13305 0.16528 351.7188 &lt; 2.2e-16 *** ## ses 4.22690 0.14951 28.2714 &lt; 2.2e-16 *** ## female -1.06257 0.21089 -5.0386 4.81e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As expected, the coefficients are the same between the two models, but the significance levels differ. In this case, the differences are trivial, and all coefficients retain their significance. But in your case, correcting for clustering might make the difference between significant and non-significant results. 3.3 Conclusion In this chapter, we discussed why we need to account for clustering in our analyses. We then demonstrated an MLM alternative, cluster-robust standard errors, that can be used to account for clustering if you’re asking questions at one level and want to run a single-level regression, but adjust the standard errors. In chapter 4, we will look at our first multilevel models for handling clustered data structures and asking multilevel questions. 3.4 Further Reading McCoach, D. B. (2010). Dealing With Dependence (Part II): A Gentle Introduction to Hierarchical Linear Modeling. Gifted Child Quarterly, 54(3), 252–256. https://doi.org/10.1177/0016986210373475 McCoach, D. B., &amp; Adelson, J. L. (2010). Dealing with dependence (Part 1): Understanding the effects of clustered data. Gifted Child Quarterly, 54(2), 152–155. https://doi.org/10.1177/0016986210363076 McNeish, D., Stapleton, L. M., &amp; Silverman, R. D. (2017). On the unnecessary ubiquity of hierarchical linear modeling. Psychological Methods, 22(1), 114–140. https://doi.org/10.1037/met0000078 "],["04-module-4.html", "Chapter 4 Our First Multilevel Models 4.1 Learning Objectives 4.2 Data Demonstration 4.3 Conclusion", " Chapter 4 Our First Multilevel Models 4.1 Learning Objectives In this chapter, we will run our first multilevel model to account for the clustered nature of our data and begin visualizing and understanding the variance in our data. The learning objectives for this chapter are: Visualize clustering in data structures; Recognize when to use a multilevel model over cluster-robust standard errors; Explain the difference between fixed and random effects in MLMs; Code and interpret the null model; Determine how variance is distributed in a dataset. All materials for this chapter are available for download here. 4.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the dataset. 4.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for visualizations library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for intraclass correlation And the same dataset of students’ math achievement from chapters 2 and 3: data &lt;- read.csv(&#39;heck2011.csv&#39;) 4.2.2 Why Multilevel Models? In chapter 3 we talked about cluster-robust standard errors, which handle clustering as a nuisance, but doesn’t investigate it as interesting or enable multilevel research questions. But if you’re interested in the clustered structure of the data, and you want to know how clusters differ or ask questions at multiple levels, you’ll need a multilevel model. To get a sense of how the outcome is clustered, let’s start by making some graphs. Our data set contains students clustered into 419 schools. For demonstration, we will take a subset of 10 schools. data_sub &lt;- data %&gt;% filter(schcode &lt;= 10) In chapter 2, we created a scatterplot with math achievement (math) on the y-axis and socioeconomic status (ses) on the x-axis. Let’s re-create that graph and overlay a line of best fit, first ignoring the clustering. data_sub %&gt;% ggplot(mapping = aes(x = ses, y = math)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Now, let’s create the same scatterplot, this time colouring the points by school (schcode). data_sub %&gt;% ggplot(mapping = aes(x = ses, y = math, colour = factor(schcode))) + geom_point() + geom_smooth(mapping = aes(group = schcode), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + labs(colour = &quot;schcode&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; When we run a regular regression without accounting for the clustered structure of the data, we treat all schools as though they have the same intercept and slope. What do you notice about the intercepts and slopes for different schools? The intercepts and slopes vary widely! For example, school 4 has an intercept around 62, compared to the intercept of school 3 around 49. School 8 has a relatively large positive slope, while school 6 has a slightly negative slope. With multilevel models, we can quantify this variance and ask questions about it. 4.2.3 Fixed vs Random Effects Multilevel models have two main ingredients: fixed and random effects. For our purposes of executing and interpreting MLMs, a fixed effect is an average effect across all clusters and a random effect describes how an effect for a given cluster differs from the average. We are usually interested less in individual random effects, and more in the random effects variances that describe how effects vary across all clusters. For example, we might have a fixed effect for the intercept that describes average math achievement across all schools. Then we have a random effect variance that describes how intercepts for math achievement vary across schools. Together, the fixed effect and random effect variance describe math achievement scores across schools. 4.2.4 The Null Model In the simplest MLM we can run, we let intercepts vary between clusters by estimating random effects for the intercepts in addition to a fixed effect. The random effect allows the intercepts to deviate from the fixed effect, the grand mean of the intercepts. As a result, this model is called the “random intercept only model,” also known as the “null model.” For our example, math achievement is our outcome variable, and the equations for the null model look like this: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) Combined \\(math_{ij} = \\gamma_{00} + U_{0j} + R_{ij}\\) Let’s run the model. null_model &lt;- lmer(math ~ 1 + (1|schcode), data = data) summary(null_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48877.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6336 -0.5732 0.1921 0.6115 5.2989 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 10.64 3.262 ## Residual 66.55 8.158 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6742 0.1883 416.0655 306.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In lme4, the syntax for a two-level model is lmer(DV ~ 1 + IV1 + IV2 + ... + IVp + (random_effect1 + random_effect2 + ... + random_effect3 | grouping_variable), data = dataset). Our dependent variable is math achievement (math), and we have a fixed and random effect of the intercept (represented by the 1s). Our grouping variable is school (schcode). The key output to interpret is: Number of parameters Estimates of fixed effects Estimates of random effects variances As indicated in our combined equation, we are estimating three parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept; \\(\\tau_0^2\\): a variance for the random effects of the intercept, capturing the variance of schools around the intercept. Each \\(U_{0j}\\) is the residual of a school around the intercept; that is, it describes how the school’s mean math achievement at the intercept deviates from the intercept for the entire sample. Every school has a \\(U_{0j}\\), and the variance of all of the \\(U_{0j}\\)s is \\(\\tau_0^2\\); \\(\\sigma^2\\): a random effect variance capturing the variance of students around their school mean math achievement. Each student has a residual, \\(R_{ij}\\), and the variance of all the \\(R_{ij}\\)s is \\(\\sigma^2\\). We can double-check we’ve counted parameters correctly using the logLik function from the stats package. logLik(null_model) ## &#39;log Lik.&#39; -24438.63 (df=3) The degrees of freedom listed, (df = 3), correspond to the number of parameters we’re estimating. We’ll discuss log likelihoods a bit more in chapter 7, so ignore the other number for now. The fixed effect for the intercept is 57.67, representing the average math achievement score across all schools. The variance of schools around the intercept is 10.64, and of students around their school’s mean is 66.55. 4.2.5 Understanding Variance 4.2.5.1 Intraclass Correlation Coefficient (ICC) The intraclass correlation coefficient quantifies the extent of clustering in a dataset. It ranges from 0 to 1 and is the quotient of the variance between clusters to the total variance: \\(ICC = \\frac{\\tau_0^2}{\\tau_0^2 + \\sigma^2}\\). The more extensive the impact of clustering, the more variance between clusters, the larger the ICC. The ICCs can also be interpreted as (1) the proportion of variance in the outcome that is attributable to clusters or (2) the expected correlation between the outcome from randomly selected units from a randomly selected cluster. Per our model output, the total variance is \\(\\tau_0^2 + \\sigma^2 = 10.64 + 66.55 = 77.19\\), and the variance between schools is \\(\\tau_0^2\\), 10.64. The ICC is then \\(\\frac{10.64}{77.19} = 0.138\\); 13.8% of the total variance in math achievement can be attributed to school membership. We can use the performance package to calculate this automatically: performance::icc(null_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.138 ## Unadjusted ICC: 0.138 Don’t worry about the adjusted vs conditional ICC here. In short, the adjusted ICC accounts only for the random effect variances, while the conditional ICC accounts for the fixed effect variances, too. You can read more about it here. 4.2.5.2 Plausible Values Range Another way to understand the variance in our data is by calculating a 95% plausible values range for a given effect. For example, the intercept: given the fixed effect for the intercept (\\(\\gamma_{00}\\)) and the variance of residuals around that fixed effect (\\(\\tau_0^2\\)), we can describe how much the schools vary in mean math achievement by calculating a 95% plausible values range. \\(95\\%\\ plausible\\ values\\ range = \\gamma_{00} ± 1.96\\sqrt{\\tau_0^2}\\). Tau0 &lt;- VarCorr(null_model)$schcode[1] lower_bound &lt;- null_model@beta - 1.96*sqrt(Tau0) upper_bound &lt;- null_model@beta + 1.96*sqrt(Tau0) lower_bound ## [1] 51.28024 upper_bound ## [1] 64.06822 This range gives us a sense of the variance in school intercepts: 95% of intercepts will fall between 51 and 64. 13 points of variance is a fair amount for a scale from 0 to 100! It seems good that we’re accounting for that variance with our multilevel model, rather than treating all schools like they have the same intercept. 4.2.5.3 Empirical Bayes Estimates As noted, every school has its own intercept residual, \\(U_{0j}\\). We’re not usually interested in individual residuals; rather, we’re interested in the variance of those residuals to understand the clustering in our data. But we can visualize at the individual residuals as a third way of understanding that variation. We can extract and plot the residuals as Empirical Bayes estimates, which are weighted. The random effects for the intercept (the \\(U_{0j}\\)s) are latent variables rather than statistical parameters, but we can estimate them to visualize how much they vary (and thus how much schools vary around the grand mean intercept). We can estimate a weighted intercept for a given group with the following equation: \\(\\hat\\beta_{0j}^{EB} = \\lambda_j\\hat\\beta_{0j} + (1 - \\lambda_j)\\hat\\gamma_{00}\\) To calculate the weighted intercept, we use the following information: Group mean information (\\(\\hat{\\beta}\\)); Population mean information (\\(\\hat{\\gamma_{00}}\\)); Weight \\(\\lambda_j = \\frac{\\tau_0^2}{\\tau_0^2 + \\frac{\\sigma^2}{n_j}}\\). The larger a cluster, the closer the denominator is to the numerator, the larger the weight. The Empirical Bayes estimate of a residuals, \\(U_{0j}\\), is then the difference between the fixed effect \\(\\gamma_{0j}\\) and the EB estimate \\(\\hat\\beta_{0j}^{EB}\\). To develop an intuition about EB estimates, let’s manually calculate the residual for the intercept for school 1, or \\(U_{01}\\). First, we need the group mean math achievement, \\(\\hat{\\beta_{01}}\\): data %&gt;% filter(schcode == 1) %&gt;% # select only school code 1 summarize( mean(math) ) ## mean(math) ## 1 58.99677 Next, we need the estimated population mean, \\(\\hat{\\gamma_{00}}\\). We have that from our earlier MLM: the intercept of 57.6742. Finally, we need the weight: \\(\\lambda_j = \\frac{\\tau_0^2}{\\tau_0^2 + \\frac{\\sigma^2}{n_j}}\\). We also have \\(\\tau_0^2 = 10.64\\) and \\(\\sigma^2 = 66.55\\) from our earlier MLM. We need the group sample size, \\(n_1\\): data %&gt;% filter(schcode == 1) %&gt;% count() ## n ## 1 12 So \\(\\lambda_j = \\frac{\\tau_0^2}{\\tau_0^2 + \\frac{\\sigma^2}{n_j}} = \\frac{10.64}{10.64 + \\frac{66.55}{12}} = 0.657\\). Combining this information into our Empirical Bayes formula, \\(\\hat\\beta_{01}^{EB} = 58.545\\). The residual between the intercept from our MLM — 57.6742 — and our Empirical Bayes estimate — 58.545 — is 0.87. We could repeat this manual calculation process and get an Empirical Bayes residual for every school, and then plot those residuals to visualize their distribution. Luckily, we don’t need to do this manual process 419 times; we can extract the Empirical Bayes estimates of the residuals using code: empirical_bayes_data &lt;- as_tibble(ranef(null_model)) We can double-check our manual calculation: head(empirical_bayes_data, 1) ## # A tibble: 1 × 5 ## grpvar term grp condval condsd ## &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 schcode (Intercept) 1 0.869 1.91 Looks like the residual for school 1 (grp 1) is 0.87, just like we manually calculated! Then, we can plot the residuals to visualize their distribution. ggplot(data = empirical_bayes_data, mapping = aes(x = condval)) + # &quot;condval&quot; is the name of the EB estimates returned by the ranef function above geom_histogram() + labs(x = &quot;EB estimate of U0j&quot;) As we would expect, the residuals have a mean of 0 because the process of estimating the model is a process of minimizing the residuals. It looks like they mostly range from -5 to 5 in a normal distribution. Again, looks like our intercepts vary fairly widely between schools, so it’s a good thing we’re modelling that variation. 4.3 Conclusion In this chapter, we discussed why and when one should use multilevel models, reviewed different ways to visualize and understand the variance in your data at different levels, and estimated our first multilevel model: the random-intercept-only model (also called the null model). In the next chapter, we’ll start adding more fixed effects. "],["05-module-5.html", "Chapter 5 Adding Fixed Predictors to MLMs 5.1 Learning Objectives 5.2 Data Demonstration 5.3 Conclusion", " Chapter 5 Adding Fixed Predictors to MLMs 5.1 Learning Objectives In this chapter, we will introduce fixed predictors at both level-1 and level-2. The learning objectives for this chapter are: Code and interpret fixed effects in multilevel models; Explain the difference between conditional and unconditional effects; Evaluate the utility of predictors in a model by considering the information from regression coefficients and variance reduced. All materials for this chapter are available for download here. 5.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the data. 5.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for visualizations library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for intraclass correlation And the same dataset of students’ math achievement: data &lt;- read.csv(&#39;heck2011.csv&#39;) 5.2.2 MLM with Level-1 Predictor As a reminder, in Chapter 4 we estimated the random-intercept-only model, also called the null model: null_model &lt;- lmer(math ~ 1 + (1|schcode), data = data) summary(null_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48877.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6336 -0.5732 0.1921 0.6115 5.2989 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 10.64 3.262 ## Residual 66.55 8.158 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6742 0.1883 416.0655 306.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now that we’ve explored the null model and variance decomposition it gives us access to, let’s practice adding a level-1 predictor to our model. Level-1 predictors vary at level-1, which in our example is the student level, meaning that students have different values for a variable. In our data, socioeconomic status (ses) and sex (female) vary across students, at level-1. Let’s add a fixed effect for ses as a predictor to our model. The following equations describe this model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{10}ses_{ij} + U_{0j} + R_{ij}\\) We’ll be estimating four parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses; \\(\\gamma_{10}\\): the fixed effect for the slope of ses; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of schools around the intercept, controlling for ses; \\(\\sigma^2\\): a random effect variance capturing the variance of students around their school mean math achievement, controlling for ses. Notice that the parameters are now conditional on ses. The intercept is no longer interpreted as the intercept across all schools; it’s the intercept across all schools conditional on ses being equal to 0, or at the mean ses level for the sample given that ses is z-scored in these data. Additionally, note that there is no \\(U_j\\) term associated with the coefficient for ses; that’s because we’re only adding a fixed effect for ses right now. This implies that the relationship between ses and math achievement is the same across all schools (i.e., the slope is fixed, not randomly varying). We’ll look at adding random slope effects in the next chapter. For now, let’s run our model. ses_l1 &lt;- lmer(math ~ ses + (1|schcode), data = data, REML = TRUE) summary(ses_l1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ ses + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48215.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7733 -0.5540 0.1303 0.6469 5.6908 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.469 1.863 ## Residual 62.807 7.925 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5960 0.1329 375.6989 433.36 &lt;2e-16 *** ## ses 3.8739 0.1366 3914.6382 28.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.025 Per the intercept, the average math achievement across all schools at mean ses is 57.596. A one-standard-deviation increase in ses is associated with a 3.87-point increase in math achievement. The variance term describing how schools vary around the intercept is 3.469, whereas the variance term describing how the students vary within schools, about their schools’ mean, is 62.807. These variance terms are different from our null model that had no predictors; we can quantify that difference in at least two ways. One option is to calculate how much level-1 variance was reduced by adding ses as a level-1 predictor. If we divide the difference between our null model’s level-1 variance and this new model’s (l1) level-1 variance by the null model variance, we can see what proportion of variance was reduced. null &lt;- sigma(null_model)^2 l1 &lt;- sigma(ses_l1)^2 (null - l1) / null ## [1] 0.05624991 So we reduced about 5.6% of level-1 variance by adding ses as a level-1 predictor. Another way of stating this is that we reduced the unexplained within school variance by 5.6%. Another option is to calculate the conditional ICC, or the proportion of variance explained by clustering after we account for ses. Recall from last chapter that the adjusted ICC accounts only for random effect variances, while the conditional ICC accounts for both variance of both random effects and fixed effects. With the null model, the adjusted and conditional ICC values from performance are the same because there are no predictors in the model, but with a fixed level-1 predictor in the model, we should reference the conditional ICC. performance::icc(ses_l1) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.052 ## Unadjusted ICC: 0.046 After accounting for the effect of socioeconomic status, 4.6% of the variance in math achievement is accounted for by school membership. 5.2.3 Compare Regular and Multilevel Regression In the previous chapter, we compared a regular regression to a cluster-robust standard error regression. Now, let’s compare those two with a multilevel model. The regular regression from Chapter 4: model &lt;- lm(math ~ ses, data = data) summary(model) ## ## Call: ## lm(formula = math ~ ses, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.459 -4.678 1.144 5.355 47.560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 57.59817 0.09819 586.61 &lt;2e-16 *** ## ses 4.25468 0.12566 33.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.132 on 6869 degrees of freedom ## Multiple R-squared: 0.143, Adjusted R-squared: 0.1429 ## F-statistic: 1146 on 1 and 6869 DF, p-value: &lt; 2.2e-16 The cluster-robust standard error regression from Chapter 4: model_crse &lt;- lmtest::coeftest(model, vcov = sandwich::vcovCL, cluster = ~ schcode) model_crse ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 57.59817 0.13020 442.38 &lt; 2.2e-16 *** ## ses 4.25468 0.14981 28.40 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 These two models had the same coefficients, with different significance values. This is our multilevel model: summary(ses_l1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ ses + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48215.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7733 -0.5540 0.1303 0.6469 5.6908 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.469 1.863 ## Residual 62.807 7.925 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5960 0.1329 375.6989 433.36 &lt;2e-16 *** ## ses 3.8739 0.1366 3914.6382 28.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.025 The intercepts are the same between the MLM and regular regressions, but the coefficient for ses is not. Why? The coefficient for ses represents the mean relationship between SES and math achievement across all schools, weighted by the reliability of the cluster. The weighting reflects cluster-level sample size, and thus varies from the regular regression estimates that treat all observations equally. 5.2.4 MLM with Level-2 Predictor We added ses as a level-1 predictor to explain some of the student-level variance in math achievement. Now, let’s add a predictor that varies at level-2, meaning that the value is different across level 2 units, which is the school level. Level-2 predictors are different across schools but the same for all students within a school. There are three possible level-2 predictors: ses_mean: the mean SES per school (this variable is centered, we’ll discuss centering more in Chapter 9) pro4yc: the percentage of students at a school who intend to study at a 4-year college/university public: whether the school is private (0) or public (1) This is where we begin to unlock the potential of MLMs, to ask questions about both individual differences (level-1 variables) and school differences (level-2 variables) at the same time while accounting for clustered data structures. Let’s consider the role of school type in our model by adding a fixed effect for public as a predictor of our intercept. The following equations describe this model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}public_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}public_{j} + \\gamma_{10}ses_{ij} + U_{0j} + R_{ij}\\) A few things to note here: first, public_j only has a j subscript because only different schools (j’s) have different values of public. All students (i’s) within a school have the same value. Second, public is currently only a predictor for the intercept. In Chapter 6 we’ll look at using level-2 variables as predictors of level-1 slopes and the cross-level interactions that result. We’ll be estimating five parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses and public; \\(\\gamma_{01}\\): the fixed effect for the slope of public controlling for ses \\(\\gamma_{10}\\): the fixed effect for the slope of ses controlling for public; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of schools around the intercept, controlling for ses and public; \\(\\sigma^2\\): a random effect variance capturing the variance of students around their school mean math achievement, controlling for ses and public. Notice that the parameters are conditional on both ses and on public now. Let’s run our model. ses_l1_public_l2 &lt;- lmer(math ~ 1 + ses + public + (1|schcode), data = data, REML = TRUE) summary(ses_l1_public_l2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + public + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48216 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7718 -0.5541 0.1309 0.6477 5.6916 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.486 1.867 ## Residual 62.807 7.925 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.63143 0.25535 381.81733 225.693 &lt;2e-16 *** ## ses 3.87338 0.13673 3928.37427 28.329 &lt;2e-16 *** ## public -0.04859 0.29862 385.93649 -0.163 0.871 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ses ## ses 0.013 ## public -0.854 -0.031 Let’s look at our fixed effects, which describes the conditional mean effect of a variable on the outcome, across all schools. Per the intercept, the average math achievement across all private schools (public = 0) at mean SES (ses = 0) is 57.70. A one-standard-deviation increase in ses across all private schools is associated with a 3.87-point increase in math achievement. Public schools at mean ses have a -0.14-point decrease on average in math achievement relative to private schools. From our random effect variances, the variance term describing how schools vary around the intercept (at mean SES at private schools) is 3.48, and the variance term describing how students vary around their school means is 62.81. Let’s calculate variance reduced at level 1 and level 2 by adding school type as a predictor. # level-1 variance reduced sigma2_null &lt;- sigma(null_model)^2 sigma2_public &lt;- sigma(ses_l1_public_l2)^2 (sigma2_null - sigma2_public) / sigma2_null ## [1] 0.05624525 # level-2 variance reduced tau2_null &lt;- VarCorr(null_model)$schcode[1] tau2_public &lt;- VarCorr(ses_l1_public_l2)$schcode[1] (tau2_null - tau2_public) / tau2_null ## [1] 0.6724414 We reduced around 5.6% of variance in math achievement at level-1 and 67.2% of variance at level-2 by adding public as a level-2 predictor. It makes sense that the variance at level-2 was reduced by so much more, because we added a level-2 predictor that varies at level-2. So, does it seem like school type is related to math achievement? We have two sources of information to consider so far: the regression coefficient and the variance reduced. While the regression coefficient is relatively small, the intercept variance reduced at level-2 is quite large (67%!), so it seems like school type is a valuable predictor in our model. 5.3 Conclusion In this chapter, we added level-1 and level-2 fixed effects to our models, considered the difference between conditional and unconditional effects, and used regression coefficients and variance reduced to make a decision about retaining model parameters. In Chapter 6, we’ll work with random slopes and explain cross-level interactions. "],["06-module-6.html", "Chapter 6 Random Effects and Cross-level Interactions 6.1 Learning Objectives 6.2 Data Demonstration 6.3 Conclusion", " Chapter 6 Random Effects and Cross-level Interactions 6.1 Learning Objectives In this chapter, we will introduce cross-level interactions and random effect covariances. The learning objectives for this chapter are: Code and interpret models with random slope effects and cross-level interactions; Interpret meaning of different elements of a Tau matrix; Visualize a random effect covariance using Empirical Bayes estimates. All materials for this chapter are available for download here. 6.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the data. 6.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for visualizations library(lme4) # for multilevel models library(lmerTest) # for p-values And the same dataset of students’ math achievement: data &lt;- read.csv(&#39;heck2011.csv&#39;) 6.2.2 MLM with Random Slope Effect As a reminder, in Chapter 4 we made the following scatterplot visualizing the relationship between math achievement and socioeconomic status across different schools: data %&gt;% filter(schcode &lt;= 10) %&gt;% ggplot(mapping = aes(x = ses, y = math, colour = factor(schcode))) + geom_point() + geom_smooth(mapping = aes(group = schcode), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + labs(colour = &quot;schcode&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; As we can see, the intercept and slope values are quite different across schools. For example, school 3 has an intercept around 38 and a small positive slope, whereas school 8 has an intercept around 55 and a larger positive slope. In Chapter 5, we modelled the relationship between math achievement and SES as follows: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{1j}ses_{ij} + U_{0j} + R_{ij}\\) We did not assume all schools had the same mean math achievement: we modelled the variation in intercepts by adding a random intercept term (U_{0j}) to our model, which estimated the variances in intercepts across schools. However, we assumed that all schools had the same slope by only estimating the average effect of SES, and not a variance around that slope. That doesn’t seem accurate; look at our scatterplot and the variability in slopes! We can model this variance in slopes between schools by adding a random slope term to our model. The following equations describe this model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + U_{1j}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{1j}ses_{ij} + U_{0j} + U_{1j}ses_{ij} + R_{ij}\\) With this model, we’ll now be estimating 6 parameters — 2 fixed effects, 3 random effect variances, and a random effect covariance: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses; \\(\\gamma_{10}\\): the fixed effect for the slope of ses; \\(\\sigma^2\\): a random effect variance capturing the variance of students around their school’s mean math achievement, controlling for ses; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of schools around the intercept, controlling for ses; \\(\\tau_1^2\\): a random effect variance for the slope capturing variance of school slopes around the grand mean slope, controlling for ses; \\(\\tau_{01}\\): a random effect covariance capturing how the intercept variance and slope variance relate to each other. The random effect covariance \\(\\tau_{01}\\) quantifies the relationship between \\(\\tau_0^2\\) and \\(\\tau_1^2\\). It is interpreted like any other covariance, as the unstandardized relationship, but lme4 also outputs the standardized form, the correlation. If the covariance is positive, then a higher intercept value is associated with a higher slope. If negative, a higher intercept value is associated with a lower slope. If near-zero, there is minimal/no relationship between the intercept and slope values. In our example, that would indicate schools with higher mean levels of math achievement at the intercept of ses = 0 would also have a larger slope of ses. If the covariance is negative, then a higher intercept value is associated with a lower slope. In our example, schools with higher intercepts of math achievement at ses = 0 would have lower slopes for ses. We’ll look at the actual random effect covariance in a moment. To estimate a random slope effect in lme4, you place the predictor for which you want a random slope before the | in the code as follows: ses_l1_random &lt;- lmer(math ~ ses + (1 + ses|schcode), data = data, REML = TRUE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(ses_l1_random) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ ses + (1 + ses | schcode) ## Data: data ## ## REML criterion at convergence: 48190.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8578 -0.5553 0.1290 0.6437 5.7098 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schcode (Intercept) 3.2042 1.7900 ## ses 0.7794 0.8828 -1.00 ## Residual 62.5855 7.9111 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6959 0.1315 378.6378 438.78 &lt;2e-16 *** ## ses 3.9602 0.1408 1450.7730 28.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.284 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Note that the 1 indicates a random intercept term, and is a default setting so you can also estimate both a random intercept and slope with just (ses|schcode). If you want to exclude the random intercept from the model you need to write (0 + ses|schcode) to override the default. Let’s look at our fixed effects. Per the intercept, the average math achievement at mean SES (ses = 0) is 57.70. A one-standard-deviation increase in ses across all private schools is associated with a 3.96-point increase in math achievement. From our random effect variances, the variance term describing how schools vary around the intercept (at mean ses) is 3.20 (\\(\\tau_0^2\\)), the variance term describing how school SES slopes vary around the grand mean slope is 0.78 (\\(\\tau_1^2\\)), and the variance term describing how students vary around their school’s mean math achievement is 62.59 (\\(\\sigma^2\\)). We can find our random effect covariance by examining our Tau matrix. The Tau matrix is called a Tau matrix because it contains the estimates for our random effect variances, or Taus: \\(\\tau_0^2\\), \\(\\tau_1^2\\), etc. We have always been estimating a Tau matrix, but when we only had a random intercept it was just a 1-by-1 matrix of the random intercept term \\(\\tau_0^2\\). Matrix::bdiag(VarCorr(ses_l1_random)) ## 2 x 2 sparse Matrix of class &quot;dsCMatrix&quot; ## (Intercept) ses ## (Intercept) 3.204184 -1.5802590 ## ses -1.580259 0.7793617 The code looks a little busy, but there are two steps. First, we extract our random effects variance-covariance matrix (Tau matrix) with VarCorr(ses_l1_random). Then, we use the bdiag() function from the Matrix package to construct a matrix that’s easy for us to read at a glance. In the first row and first column, we have our intercept variance term \\(\\tau_0^2\\), 3.20. In the second row and second column, we have our slope variance term, \\(\\tau_1^2\\), 0.78. In the second row and first column OR in the first row and second column, we have our random effect covariance, -1.58. This negative covariance indicates that for higher intercepts, the slope value is lower: the relationship between SES and math achievement decreases as mean math achievement increases. The matrix we’ve shown here only includes the level-2 random effect variances organized in matrix form, which you may see in other software programs and which can be easier to read. Information about the relationship between random effect variances is also output by lme4 under the “Random effects:” section. The lme4 output includes all random effect variances in variance and standard deviation units, as well as the correlation (not covariance) between the intercept and slope variances. We can convert the covariance between \\(\\tau_0^2\\) and \\(\\tau_1^2\\) to the correlation using the standard deviations of each: \\[corr = \\frac{cov(X, Y)}{sd_x*sd_y}\\] We have our covariance from our Tau matrix: -1.58. We can see the standard deviations in the lme4 output: the standard deviation of the intercept variance is 1.79, the standard deviation of the slope variance 0.88. We can then compute the correlation: -1.58/(1.79*0.88) ## [1] -1.003047 So there is a correlation of -1 between the intercept variance and slope variance, which matches the printed output of -1.00 under the “Corr” column in the “Random effects” section of the lme4 output. Let’s visualize the relationship using Empirical Bayes estimates (see Chapter 4 for more on EB estimates) of the intercepts and slopes for each school; we expect to see a negative relationship between them. empirical_bayes_data &lt;- ranef(ses_l1_random) # extract random effects for each school empirical_bayes_intercepts &lt;- empirical_bayes_data$schcode[&quot;(Intercept)&quot;] empirical_bayes_slopes &lt;- empirical_bayes_data$schcode[&quot;ses&quot;] # extracts the SES/slope EB estimates from the list bind_cols(empirical_bayes_intercepts, empirical_bayes_slopes) %&gt;% # combine EB slopes and intercepts into a useable dataframe for graphing ggplot(mapping = aes(x = ses, y = `(Intercept)`)) + geom_point() Looks like we expect! That’s a covariance of -1.58 visualized. Finally, note that we get a convergence issue with this model: boundary (singular) fit: see help('isSingular'). For now, we’re going to ignore that. In Chapter 7 we will focus on estimation issues and troubleshooting. 6.2.3 MLM with Crosslevel Effect In Chapter 5, we added the level-2 variable of school type (public = 0 for public schools, public = 1 for private schools) as a predictor of the intercept to answer the question: how does school type affect math achievement scores when ses = 0? Do public schools have higher or lower intercepts than private schools? The following equations described that model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{10}public_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}public_{j} + \\gamma_{10}ses_{ij} + U_{0j} + R_{ij}\\) What if we want to know how school type affects the slope of ses, though? In other words, is there a difference in the effect of SES on math achievement in a private or public school? We can answer this question by adding school type as a predictor of SES slopes and create a cross-level interaction. This is an interaction because it allows us to estimate a different slope based on school type, whereas our previous model assumed the relationship between SES and math achievement was the same for both school types. The logic is the same as in regular regression. We can describe this model with the following equations. Note that we are also including our slope random effect variance (\\(\\tau_1^2\\) / \\(U_{1j}\\)), which allows the slopes to vary across schools. This is logically consistent with the idea that slopes might vary due to school type, but not required. We could run this model without random slopes. Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{10}public_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}public_j + U_{1j}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}public_{j} + \\gamma_{10}ses_{ij} + \\gamma_{11}ses_{ij}*public_j + U_{0j} + U_{1j}public_{j} + R_{ij}\\) With this model, we will be estimating 8 parameters — 4 fixed effects, 3 random effect variances, and a random effect covariance: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses and public; \\(\\gamma_{01}\\): the fixed effect for the slope of public, controlling for ses; \\(\\gamma_{10}\\): the fixed effect for the slope of ses, controlling for public; \\(\\gamma_{11}\\): the fixed effect for the effect of public on the slope of ses; \\(\\sigma^2\\): a random effect variance capturing the variance of students around their school’s mean math achievement, controlling for ses and public; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of schools around the intercept, controlling for ses and public; \\(\\tau_1^2\\): a random effect variance for the slope capturing variance of school slopes around the grand mean slope, controlling for ses and public; \\(\\tau_{01}\\): a random effect covariance capturing how the intercept variance and slope variance relate to each other. A cross-level interaction is interpreted like an interaction in regular regression: the effect of school type on the effect of SES on math achievement. Like in regular regression interactions, it can also be interpreted as the effect of SES on school type on math achievement. And as in regular regression, either interpretation is accurate, but one of the other might be more intuitive for a specific research question. Let’s run our model: crosslevel_model &lt;- lmer(math ~ 1 + ses + public + ses:public + (1 + ses|schcode), data = data, REML = TRUE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(crosslevel_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + public + ses:public + (1 + ses | schcode) ## Data: data ## ## REML criterion at convergence: 48187.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8509 -0.5593 0.1294 0.6412 5.6998 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schcode (Intercept) 3.2144 1.7929 ## ses 0.8013 0.8951 -1.00 ## Residual 62.5555 7.9092 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.72440 0.25183 382.39815 229.216 &lt;2e-16 *** ## ses 4.42383 0.27427 1283.55622 16.130 &lt;2e-16 *** ## public -0.02632 0.29472 387.41741 -0.089 0.9289 ## ses:public -0.62520 0.31957 1363.95273 -1.956 0.0506 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ses public ## ses -0.232 ## public -0.852 0.197 ## ses:public 0.198 -0.858 -0.250 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Note that we can include interactions in two ways. Here, we are using verbose code, listing the individual effects (ses and public) and indicating their interaction with ses:public. You can also capture all of this information with an asterisk: ses*public is equivalent to ses + public + ses:public. We have a convergence warning again: boundary (singular) fit: see help('isSingular'), and again we’re going to ignore it for now (see Chapter 7 for a deeper dive into these issues). Let’s look at our fixed effects. Per the intercept, the average math achievement across all private schools (public = 0) at mean SES (ses = 0) is 57.72. A one-standard-deviation increase in ses across all private schools is associated with a 4.42-point increase in math achievement. Public schools (public = 1) at mean ses have a -0.02-point decrease on average in math achievement relative to private schools. The effect of ses on math achievement is lower in public schools by -0.63 points on average, which quantifies the interaction. We can calculate the expected slope for SES in public schools by using these coefficients: 4.42 - 0.63 = 3.79, so a one-unit increase in SES in public schools is associated with a 3.79-unit increase in math achievement, less of an affect than at private schools. From our random effect variances, the variance term describing how schools vary around the intercept (at mean SES at public schools) is 3.21, the variance of school slopes around the grand mean is 0.80, and the variance term describing how students vary around their school means is 62.56. We can see our random effect covariance of -1.6 with our Tau matrix, indicating that schools with higher values of mean math achievement at the intercept of ses = 0 have lower slopes of ses. Matrix::bdiag(VarCorr(crosslevel_model)) ## 2 x 2 sparse Matrix of class &quot;dsCMatrix&quot; ## (Intercept) ses ## (Intercept) 3.214435 -1.6048897 ## ses -1.604890 0.8012825 Like in other chapters, you can also calculate variance reduced at level-1 and level-2 to examine the impact of adding our cross-level effect, which we leave as an exercise to the reader. 6.3 Conclusion In this chapter, we added random slope effects at level-1 and a cross-level interaction to our model, examined the Tau matrix, and interpreted random effect covariances. In doing so, we ran into some convergence issues (that ?isSingular warning). In Chapter 7, we’ll delve into model estimation options, problems, and troubleshooting. "],["07-module-7.html", "Chapter 7 Model Estimation Options, Problems, and Troubleshooting 7.1 Learning Objectives 7.2 Data Demonstration 7.3 Conclusion 7.4 Further Reading", " Chapter 7 Model Estimation Options, Problems, and Troubleshooting 7.1 Learning Objectives In this chapter, we will review common estimation options, problems that can arise, and how to troubleshoot those problems. The learning objectives for this chapter are: Differentiate between restricted maximum likelihood and full information maximum likelihood estimation options; Describe common causes of estimation errors; Understand the components of optimizer functions; Recognize estimation errors in R output and examine output to identify error sources; Build and compare models to address errors. All materials for this chapter are available for download here. 7.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the data. 7.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for graphing library(lme4) # for multilevel models library(lmerTest) # for p-values And the same dataset of students’ math achievement: data &lt;- read.csv(&#39;heck2011.csv&#39;) 7.2.2 Introduction to Estimation Problems In Chapter 6, we modelled the relationship between SES and math achievement with a random intercept and random slope as follows: ses_l1_random &lt;- lmer(math ~ 1 + ses + (1 + ses|schcode), data = data, REML = TRUE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) As indicated by the warning message from R, our model is singular (which we’ll define in a moment). In this chapter, we will examine estimation issues like this and how to troubleshoot them. This is one of the less interactive chapters in these materials, but if you want a reason to stick around, there is a fun puzzle analogy. We’ll begin with some notes on model estimation and then move onto possible issues and how to address them. 7.2.3 Estimation and Optimizers In linear regression, Ordinary Least Squares estimation is used to find a combination of parameters (intercepts and slopes) that minimize the residual sum of squares. If we imagine a simple linear regression with math achievement as an outcome and SES as a predictor, we have our regression line (line of best fit) and our actual data points around that line. data %&gt;% filter(schcode &lt;= 10) %&gt;% # subset data to make it easier to see ggplot(mapping = aes(x = ses, y = math)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; For a given value of SES on the x-axis, the distance between our prediction (regression line) and our actual observation (data point) is our residual, and if we sum all of the residuals (after squaring them so the negative residuals below the line and positive residuals above it don’t cancel out), we get our residual sum of squares. OLS regression will select the regression line with the smallest residuals, which is the line that is as close as possible to the data points. You can see this process and play around with it on this interactive website: https://seeing-theory.brown.edu/regression-analysis/index.html#section1 In multilevel modelling, we use maximum likelihood (ML) estimation instead of OLS estimation. In ML estimation, we have our data points and we want to find the combination of parameters (intercepts and slopes) that maximize the likelihood that we observed that data. This is an iterative process, where we select parameters that maximize the probability of getting our data (i.e., that maximize the likelihood). We select set after set of parameters, and eventually stop when the parameter sets aren’t getting better. You can play around with likelihood here: https://seeing-theory.brown.edu/bayesian-inference/index.html#section2 This video from Stat Quest walks through the concept: We have two options for ML estimation in multilevel modelling: restricted maximum likelihood (REML) and full information maximum likelihood (FIML or ML). The key difference between them is how the estimation methods handle the variance components. When using REML, there is a penalty applied to the degrees of freedom when estimating the variance components \\(\\sigma^2\\), \\(\\tau_1^2\\), etc. When using FIML, there is no such penalty and as a result the variance components are usually underestimated. A linear regression analogy might help clarify this point: the formula for population variance is \\(S = \\frac{\\Sigma(x_i - \\overline{x})^2}{n}\\). The formula for sample variance is \\(s = \\frac{\\Sigma(x_i - \\overline{x})^2}{n - 1}\\). The sample variance imposes a penalty of n - 1 and is a REML estimator, while the population variance formula is the corresponding FIML estimator. Because we want accurate information about our variance components, we will usually use REML. We will only use FIML when we want to compare two models with different fixed effects. We’ll discuss model comparison later in this chapter. 7.2.4 Non-Convergence In the embedded Stat Quest video above, the narrator describes the iterative process in ML estimation of finding the maximum likelihood estimate of a parameter, trying multiple different options before settling on one as the value that maximizes the likelihood of observing their data about mice weights. When you’re working with many predictors at once — for example, an intercept and a slope for SES and a slope for school type and variance terms for all of those fixed effects — it is harder to try all possible combinations. So, optimization algorithms (AKA optimizers) are used to try to find the ML estimates by examining a subset of possible combinations. However, these optimizers cannot always find the combination of parameters that maximizes the likelihood of observing your data; they can’t find a solution to the problem of “what paramaters maximize the likelihood of observing this data?”. When the optimizers cannot find a solution, the result is called non-convergence: the model did not converge on a solution. You should not use the parameter estimates from a non-converged solution. A non-convergence warning is the computing equivalent of being unable to put a puzzle together, jamming the pieces in where you can, and saying “I don’t know, this is my best guess about where these pieces go.” Sure, the puzzle might sort of look like the image on the box, but it doesn’t really match, a bunch of the pieces have been contorted and bent to fit. There are two main strategies to solve a non-convergence problem: change your optimizer or change your model. You can manipulate a few characteristics of your optimizer to try to get convergence: Number of iterations. If you increase the number of iterations, the algorithm will search for longer. This is the equivalent of getting our puzzle-doer to sit at the table for longer trying to assemble the puzzle, trying out different and more pieces. Algorithm: the algorithm determines how the optimizer chooses its next attempted solution. What strategy is our puzzle-doer using to fit pieces into the puzzle? Tolerance: this can get a bit technical and vary depending on context, so we suggest Brauer and Curtin, 2018 for more. But in our case, we can think of it as the algorithm’s tolerance for differences in solutions. Lower tolerance means slightly different solutions will be seen as different, whereas higher tolerance means two different solutions that are still kind of close will be treated as essentially the same. Maybe our puzzle-doer needs glasses; tolerance is like whether they’re wearing their glasses and can distinguish between two close-but-not-identical assembled puzzles. (We hope you enjoyed the puzzle analogy.) You can alter these elements of your optimizer to see if giving it more time, a different strategy, or more leeway to say “yes, this converged” will lead to convergence. Alternatively, you can trim your model, removing variables you think are less likely to matter. We will discuss some approaches to doing this below. 7.2.5 Singularity Singularity occurs when an element of your variance-covariance matrix is estimated as essentially zero as a result of extreme multicollinearity or because the parameter is actually essentially zero. You can find singularity by examining your variance-covariance estimates and the correlations between them. It will often show up as co/variances near zero or correlations between variances at -1 or 1. Let’s return to our example from Chapter 6, predicting math achievement from SES with a random slope: ses_l1_random &lt;- lmer(math ~ 1 + ses + (1 + ses|schcode), data = data, REML = TRUE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) As we can see, our output contains a helpful warning message notifying us that the model is singular. We can investigate this issue in three ways. First, we can look at our Tau matrix: Matrix::bdiag(VarCorr(ses_l1_random)) ## 2 x 2 sparse Matrix of class &quot;dsCMatrix&quot; ## (Intercept) ses ## (Intercept) 3.204184 -1.5802590 ## ses -1.580259 0.7793617 Things look okay here, no elements appear to be close to or zero. Our second method of investigation is looking at our overall output: summary(ses_l1_random) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + (1 + ses | schcode) ## Data: data ## ## REML criterion at convergence: 48190.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8578 -0.5553 0.1290 0.6437 5.7098 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schcode (Intercept) 3.2042 1.7900 ## ses 0.7794 0.8828 -1.00 ## Residual 62.5855 7.9111 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6959 0.1315 378.6378 438.78 &lt;2e-16 *** ## ses 3.9602 0.1408 1450.7730 28.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.284 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Here, in our random effects section, we can see that the correlation between our random effect variances is -1.00, a sign of perfect multicollinearity. We can dig into the confidence intervals of our estimates up close to confirm this: confint(ses_l1_random, oldNames = FALSE) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## sd_(Intercept)|schcode 1.4999340 2.0751034 ## cor_ses.(Intercept)|schcode -1.0000000 -0.6587309 ## sd_ses|schcode 0.5410211 1.2341712 ## sigma 7.7773576 8.0484788 ## (Intercept) 57.4380811 57.9532857 ## ses 3.6737845 4.2507467 Note that oldNames = FALSE just makes the output easier to read. This will take a moment to run, but when it does we can see that the 95% confidence interval for the correlation between our random effect variances spans -1 to 1 (i.e. the entire possible range). Our singularity issue started when we added the random slope effect, which added both a random slope variance \\(\\tau_1^2\\) and the random intercept-slope covariance \\(\\tau_{01}\\). Let’s see if we can fix the issue by removing that problematic covariance. ses_l1_random_cov0 &lt;- lmer(math ~ 1 + ses + (1|schcode) + (0 + ses|schcode), data = data, REML = TRUE) summary(ses_l1_random_cov0) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + (1 | schcode) + (0 + ses | schcode) ## Data: data ## ## REML criterion at convergence: 48213.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7791 -0.5526 0.1327 0.6466 5.7089 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.3222 1.8227 ## schcode.1 ses 0.7205 0.8488 ## Residual 62.5213 7.9070 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5888 0.1328 374.9738 433.55 &lt;2e-16 *** ## ses 3.8803 0.1435 377.2408 27.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.023 Here, we specify our random intercept (1|schcode) and random slope with no covariance (0 + ses|schcode) separately, and that fixed the singularity issue! If we print our Tau matrix we can see that the covariance is fixed to 0. Matrix::bdiag(VarCorr(ses_l1_random_cov0)) ## 2 x 2 sparse Matrix of class &quot;dsCMatrix&quot; ## ## [1,] 3.322166 . ## [2,] . 0.7204535 In general, it is best practice to build a maximal multilevel model, one that includes all possible fixed and random effects that are not zero (Barr et al., 2013). This maximal model will produce parameter estimates with the least amount of bias and provide the best shot at your model fitting the data. However, the maximal model that tries to estimate extreme random effect variances (those near zero or with high multicollinearity) will have trouble converging and produce estimation errors. When this happens, often an inspection of the random effect variances will reveal which parameters need to be removed from the model. It can be helpful, ahead of running your MLMs, to consider the key variables of interest, their random effects, and plan, if the maximal model has errors, which parameters should be removed and in what order. Overall, building MLMs is about balancing complexity with utility. Sometimes we do not have enough information in our data to estimate the complex model we planned, so having a plan for how to decrease complexity ahead of time can prevent getting lost in the garden of forking paths. 7.2.6 Deviance Testing for Model Comparison We removed the random effect covariance and our model is no longer singular (i.e., suffering from multicollinearity). That seems better! Now that we have a model without an error, let’s look at comparing the model with the random slope for SES (but no covariance, as we just removed that) and the model without the random slope for SES. If we want to formally test if a model fit is better or at least not worse, we can conduct a deviance test. You can find the “deviance” for your model under the “REML criterion at convergence” in your summary output. In short, deviance is bad and we don’t want more of it, so when we compare the model with and without the random slope for SES, we don’t want the model with the random slope to have more deviance. We want the same or less deviance. Note that deviance is based on the likelihood function for your model. Unlike probability, likelihood is not bound at 0 and 1. It can be any number. As a result, looking at likelihood or deviance in isolation is not informative, because it has no bounds. It is only useful for comparison between models, where less deviance indicates a better model (compared to the reference model). Here, we’re comparing models with the same fixed effects but different random effects so we can still use REML estimator that more accurately estimates random effects. We have our two model terms, ses_l1 and ses_l1_random_cov0, and we can compare the deviance of each using the built-in ANOVA function. Specifying refit = FALSE stops the function from refitting the models with FIML. If we were comparing models with different fixed effects, we would use FIML to estimate our models. # models ses_l1 &lt;- lmer(math ~ 1 + ses + (1|schcode), data = data, REML = TRUE) ses_l1_random_cov0 &lt;- lmer(math ~ 1 + ses + (1|schcode) + (0 + ses|schcode), data = data, REML = TRUE) # deviance test to compare model fit anova(ses_l1, ses_l1_random_cov0, refit = FALSE) ## Data: data ## Models: ## ses_l1: math ~ 1 + ses + (1 | schcode) ## ses_l1_random_cov0: math ~ 1 + ses + (1 | schcode) + (0 + ses | schcode) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## ses_l1 4 48223 48251 -24108 48215 ## ses_l1_random_cov0 5 48223 48258 -24107 48213 2.0825 1 0.149 Let’s read our output. We have seven columns: npar is the number of parameters estimated in the models. The only difference between the models is one has a random slope for SES and the other doesn’t, and you can see that one model estimates 4 parameters and the other 5 parameters. AIC: Akaike’s Information Criterion, one measure of goodness of fit BIC: Bayesian Information Criterion, another measure of goodness of fit logLik: log likelihood deviance: -2*logLik Chisq: the difference betwen our models’ deviances df: the degrees of freedom for the test, calculated as the difference in number of parameters between the models Pr(&gt;Chisq): the probability that we would find our chi-square value or greater if the null hypothesis that the models were the same was true There is no significant difference between our models’ deviance statistics: the model without the random slope has a deviance of 48215 and the model with the covariance has a deviance of 48213. The difference between these numbers is not significant, p = 0.149. Thus, there is no significant different in model fits and adding a random slope does not compromise model fit so we can add it if we think it’s informative. We’ll discuss model specification, fit, and comparison more in Chapter 11 when discussing effect sizes. In closing, when assessing model fit or troubleshooting estimation problems, it is preferable to pre-register what troubleshooting you expect to try or models you expect to estimate. At minimum, you should keep a record of changes you make and report all of them. 7.3 Conclusion In this chapter, we considered convergence options, how to diagnose and troubleshoot issues, and comparing model fits using deviance testing. In Chapter 8, we’ll consider different centering options in MLMs. 7.4 Further Reading Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 10.1016/j.jml.2012.11.001. https://doi.org/10.1016/j.jml.2012.11.001 Brauer, M., &amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. "],["08-module-8.html", "Chapter 8 Centering Options and Interpretations 8.1 Learning Objectives 8.2 Data Demonstration 8.3 Conclusion 8.4 Further Reading", " Chapter 8 Centering Options and Interpretations 8.1 Learning Objectives In this chapter, we will review options for and interpretations of centering variables in multilevel models. The examples are adapted from Dr. Dan McNeish’s lecture (thanks Dan!), and you can see an overview of his other work here. The learning objectives for this chapter are: Review centering options and interpretation in linear regression. Differentiate between total, within, between, and contextual effects. Understand the difference between within-cluster and grand-mean centering and when to use each strategy. Estimate and interpret models using both within-cluster and grand-mean centering. All materials for this chapter are available for download here. 8.2 Data Demonstration The data for this chapter were taken from chapter 3 of Heck, R. H., Thomas, S. L., &amp; Tabata, L. N. (2011). Multilevel and Longitudinal Modeling with IBM SPSS: Taylor &amp; Francis. Students are clustered within schools in the data. 8.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(magrittr) # for assignment pipe %&lt;&gt;% library(lme4) # for multilevel models library(lmerTest) # for p-values And the same dataset of students’ math achievement: data &lt;- read.csv(&#39;heck2011.csv&#39;) 8.2.2 Why Center Variables? If you’ve worked with single-level regression before, you’re probably already familiar with centering variables. Centering in regression facilitates interpretation of the intercept, which is the average value of your outcome variable when all predictors are set to zero. If your predictors do not have meaningful zero points, then the intercept can be non-sensical. For example, imagine we were predicting the number of goals scored by players in an adult hockey league based on their age in a simple regression: \\(goals_{i} = \\beta_{0} + \\beta_{1}age_{i} + \\epsilon_{i}\\) If we had age in years without centering it, the intercept would represent the average number of goals scored by players 0 years old. There are no players in adult hockey leagues that are zero years old, so this intercept is not really useful to us. If we instead centered age, the intercept would represent the average number of goals scored by players at the average league age (maybe something like 38 years old). This is a more meaningful interpretation that lies within the range of our data. Centering in multilevel models is also used to make coefficients more meaningful, but also changes the interpretation of coefficients and can be used to decompose total effects into estimates of the within, between, and contextual effects of a variable. Let’s dig more into those different effects. 8.2.3 Within, Between, and Contextual Effects To define within, between, and contextual effects, let’s think about Marsh’s Big Fish-Little Pond effect. This effect describes the situation in which high achieving students in a school that is low achieving on average, will feel better about their abilities than high achieving students in a school with higher average achievement. You may have experienced this feeling when you went from being one of the best undergraduates at your institution (i.e., you were the big fish in the little pond) to feeling less confident in your abilities when you went to graduate school (i.e., when you became a small or medium sized fish in a big pond). In the example data we have students in different schools and have measured their levels of academic achievement (their grades, for example) and academic self-concept (do they feel like they’re succeeding in school?). The within effect describes the relationship between students’ grades and their feelings of succeeding in school within a given school: how does a student’s grades affect their feelings of success? We might expect that higher grades are associated with stronger feelings of “yes, I am succeeding.” The between effect describes the relationship between a school’s average grades and the average of students’ feelings of success: how does a school’s average grade affect students’ average feelings of success? We might expect that in higher-performing schools, the students actually feel less successful on average. The contextual effect describes the difference in feelings of success for students with the same grades in schools with different average grades. What would happen if we plopped the same student into a different context (i.e., cluster)? We might expect that Student A with a grade of 80% in a school with an average grade of 60% feels great about their success, but what if we take that student with a grade of 80% and put them into a school with an average grade of 99%? They probably don’t feel as successful. This is the Big Fish-Little Pond effect, and it is a contextual effect: what effect does context have on the outcome variable? We can see this illustrated in the following graph adapted from Dan McNeish’s slides: Note how the effects relate to one another: \\(between\\ effect = within\\ effect + contextual\\ effect\\). One way of thinking of this is that the between effect shows the overall, average relationship, but using MLMs, we can decompose it into a within and contextual effect. To estimate these effects, We use different centering options! 8.2.4 Options for Centering in MLMs Let’s return to our example of SES predicting math achievement to understand if there is a contextual effect of students’ achievement from being in higher or lower on average SES schools. So far, we’ve been using the following model: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{10}ses_{ij} + U_{0j} + R_{ij}\\) model &lt;- lmer(math ~ 1 + ses + (1|schcode), data = data, REML = TRUE) summary(model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48215.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7733 -0.5540 0.1303 0.6469 5.6908 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 3.469 1.863 ## Residual 62.807 7.925 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5960 0.1329 375.6989 433.36 &lt;2e-16 *** ## ses 3.8739 0.1366 3914.6382 28.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.025 We’ve been using the variable ses which is Z-scored so 0 is equal to the mean across all students. This is a type of centering that also standardizes the units in our model. From this model, we’ve seen in previous chapters that SES has a 3.87-unit effect on achievement, meaning that math achievement is expected to increase by 3.87 units as SES increases by 1 standard deviation. This estimate cannot tell us about the effect within a school, or the contextual effect, it is an uninterpretable blend of both. We will walk through how to tease these different effects out using centering. If we want to get our within, between, and contextual effects, we have two options (Enders and Tofighi, 2007): Centering within cluster (CWC) Centering around grand mean (CGM). 8.2.4.1 Centering Within Cluster (CWC) Centering a variable within a cluster means each cluster will have a mean of zero. So, each school will have a mean of zero, and students’ scores on ses_cwc will reflect their variance around their school mean, not the grand mean of the whole dataset. To center SES within cluster, we first group our dataset by school and calculate the mean SES for each cluster: data %&lt;&gt;% # this symbol is an assignment operator and pipe, equivalent to data &lt;- data %&gt;% group_by(schcode) %&gt;% mutate(ses_mean = mean(ses)) Then, we subtract this cluster mean from every individual student’s SES value: \\(ses_{cwc} = ses - ses_{mean}\\). For the students at the mean, the resulting value will be 0. Students above the mean will have positive values, and students below the mean will have negative values. data %&lt;&gt;% mutate(ses_cwc = ses - ses_mean) The values of ses_cwc are distributed around the mean for the cluster. The mean of the centered values for each school is now (essentially) zero: data %&gt;% group_by(schcode) %&gt;% summarize( mean(ses_cwc) ) ## # A tibble: 419 × 2 ## schcode `mean(ses_cwc)` ## &lt;int&gt; &lt;dbl&gt; ## 1 1 6.48e-17 ## 2 2 -2.56e-17 ## 3 3 4.93e-17 ## 4 4 6.20e-17 ## 5 5 -1.31e-17 ## 6 6 -1.39e-17 ## 7 7 2.78e-17 ## 8 8 8.54e-18 ## 9 9 -1.31e-17 ## 10 10 -1.11e-17 ## # ℹ 409 more rows If we estimate a model with just ses_cwc, it would look like this: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses\\_cwc_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{10}ses\\_cwc_{ij} + U_{0j} + R_{ij}\\) Here, \\(\\gamma_{10}\\) represents how students vary around their school mean, which is our within effect. It only captures the effect of SES on achievement within schools, if we run this model we get an intercept of 57.67 and an effect for ses_cwc of 3.19 indicating that on average, within a school an increase in SES to one standard deviation above the mean is associated with an increase in math achievement of 3.19. model_cwc &lt;- lmer(math ~ 1 + ses_cwc + (1|schcode), data = data, REML = TRUE) summary(model_cwc) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses_cwc + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48482.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6436 -0.5628 0.1413 0.6375 5.6268 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 10.89 3.300 ## Residual 62.59 7.912 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6732 0.1883 416.1017 306.33 &lt;2e-16 *** ## ses_cwc 3.1903 0.1578 6451.7013 20.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## ses_cwc 0.000 This model is incomplete. If we want our between effect (i.e., how the school averages differ from each other), we can add the aggregate back in at level 2, which is the value we calculated for each school’s mean, ses_mean: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses\\_cwc_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}ses\\_mean_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}ses\\_mean_j + \\gamma_{10}ses\\_cwc_{ij} + U_{0j} + R_{ij}\\) With this model, we are estimating 5 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses_cwc and ses_mean (i.e., a student with school-average SES at the average school); \\(\\gamma_{01}\\): the fixed effect for ses_mean controlling for ses_cwc. This is our between effect, indicating the effect of a school’s average SES on a student’s math achievement; \\(\\gamma_{10}\\): the fixed effect for the slope of ses_cwc controlling for ses_mean. This is our within effect, indicating the effect of a student’s SES compared to the school average (at the average school, ses_mean = 0); \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of schools around the intercept, controlling for ses_cwc and ses_mean; \\(\\sigma^2\\): a random effect variance capturing the variance of students around their school mean math achievement, controlling for ses_cwc and ses_mean. Let’s estimate it: model_cwc_l2 &lt;- lmer(math ~ 1 + ses_cwc + ses_mean + (1|schcode), data = data, REML = TRUE) summary(model_cwc_l2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses_cwc + ses_mean + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48137.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7545 -0.5575 0.1312 0.6619 5.7512 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 2.511 1.585 ## Residual 62.628 7.914 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.5467 0.1239 401.3223 464.54 &lt;2e-16 *** ## ses_cwc 3.1903 0.1578 6448.7759 20.22 &lt;2e-16 *** ## ses_mean 5.8920 0.2529 385.3371 23.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ss_cwc ## ses_cwc 0.000 ## ses_mean -0.052 0.000 The average student math achievement at the average school is 57.55. A one-unit increase in SES within a school is associated with a 3.19-unit increase in math achievement. A one-unit increase in the school’s average SES is associated with a 5.89-unit increase in math achievement. Recall our earlier formula detailing the relationship between the within, between, and contextual effects: \\(between\\ effect = within\\ effect + contextual\\ effect\\). Armed with our between effect (\\(\\gamma_{01}\\)) and within effect (\\(\\gamma_{10}\\)), we can reorganize this equation and calculate our contextual effect: \\(contextual = between - within = \\gamma_{01} - \\gamma_{10} = 5.89 - 3.19 = 2.70\\), so with two hypothetical students with the same level of SES, the one in the school with higher average SES has 2.70-unit higher math achievement. This represents the contextual effect of a school’s SES on math achievement. 8.2.4.2 Centering Grand Mean (CGM) When we center a variable at the grand mean, we have information about how individuals vary around the mean of all individuals. So students’ scores on ses_cgm will reflect their variance around the mean of all students. To center SES at the grand mean, we first calculate the grand mean: data %&lt;&gt;% ungroup() %&gt;% # remove the grouping by school that we added in the CWC section mutate(ses_grand_mean = mean(ses)) Then, we subtract this grand mean from every individual student’s SES value: \\(ses_{cgm} = ses - ses\\_grand\\_mean\\). For the students at the grand mean, the resulting value will be 0. Students above the mean will have positive values, and students below the mean will have negative values. data %&lt;&gt;% mutate(ses_cgm = ses - ses_grand_mean) The values of ses_cgm are distributed around the grand mean. The mean of the students’ values of ses_cgm is now (essentially) zero. data %&gt;% summarize( mean(ses_cgm) ) ## # A tibble: 1 × 1 ## `mean(ses_cgm)` ## &lt;dbl&gt; ## 1 4.73e-17 If we estimate a model with just ses_cgm, it would look like this: Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses\\_cgm_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{10}ses\\_cgm_{ij} + U_{0j} + R_{ij}\\) Here, \\(\\gamma_{10}\\) represents how students vary around the grand mean. If we don’t factor out school means, then this value is an uninterpretable blend of within- and between-effects, sometimes referred to as the total effect. When we center at the grand mean, we must add the cluster mean back into into the model. (This is in contrast to centering within cluster, when we can just estimate the within effect, but to get the between effect we must add the cluster mean back into the model). Level Equation Level 1 \\(math_{ij} = \\beta_{0j} + \\beta_{1j}ses\\_cgm_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}ses\\_mean_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}ses\\_mean_j + \\gamma_{10}ses\\_cgm_{ij} + U_{0j} + R_{ij}\\) With this model, we are estimating 5 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for ses_cgm and ses_mean (i.e., a student with school-average SES at the average school); \\(\\gamma_{01}\\): the fixed effect for ses_mean controlling for ses_cgm. This is our contextual effect, indicating the effect of school average SES on student math achievement at the same level of student SES; \\(\\gamma_{10}\\): the fixed effect for the slope of ses_cgm controlling for ses_mean. This is our within effect, indicating the effect of a student’s SES compared to the school average (at the average school, ses_mean = 0); \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of schools around the intercept, controlling for ses_cgm and ses_mean; \\(\\sigma^2\\): a random effect variance capturing the variance of students around their school mean math achievement, controlling for ses_cgm and ses_mean. Let’s estimate it in R: cgm_model &lt;- lmer(math ~ 1 + ses_cgm + ses_mean + (1|schcode), data = data, REML = TRUE) summary(cgm_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: math ~ 1 + ses_cgm + ses_mean + (1 | schcode) ## Data: data ## ## REML criterion at convergence: 48137.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7545 -0.5575 0.1312 0.6619 5.7512 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schcode (Intercept) 2.511 1.585 ## Residual 62.628 7.914 ## Number of obs: 6871, groups: schcode, 419 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 57.6485 0.1240 402.6508 464.982 &lt;2e-16 *** ## ses_cgm 3.1903 0.1578 6448.7759 20.217 &lt;2e-16 *** ## ses_mean 2.7017 0.2981 738.7254 9.065 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ss_cgm ## ses_cgm 0.041 ## ses_mean -0.066 -0.529 The average student math achievement at the average school is 57.65. For two hypothetical students with the same level of SES, the one in the school with higher average SES has 2.70-unit higher math achievement. Within a school, a one-unit increase in SES relative to the grand mean is associated with a 3.19-unit increase in math achievement. Recall our earlier formula detailing the relationship between the within, between, and contextual effects: \\(between\\ effect = within\\ effect + contextual\\ effect\\). Armed with our contextual effect (\\(\\gamma_{01}\\)) and within effect (\\(\\gamma_{10}\\)), we can calculate our between effect: \\(between = within + contextual = \\gamma_{10} + \\gamma_{01} = 3.19 + 2.70 = 5.89\\), so an increase in a school’s average SES by one unit is associated with an increase of 5.89-unit math achievement. 8.2.5 What Kind of Centering Should You Use? Here is a reference table to keep track of what coefficients represent what effects in CWC or CGM models: Centering Option Contextual Parameter Within Parameter Between Parameter Centering Grand Mean (CGM) \\(\\gamma_{01}\\) \\(\\gamma_{10}\\) \\(\\gamma_{01} + \\gamma_{10}\\) Centering Within Cluster (CWC) \\(\\gamma_{01} - \\gamma_{10}\\) \\(\\gamma_{10}\\) \\(\\gamma_{01}\\) Let’s look at our example results this way: Centering Option Contextual Parameter Within Parameter Between Parameter Centering Grand Mean (CGM) 2.70 3.19 5.89 Centering Within Cluster (CWC) 2.70 5.89 3.19 If you’re not interested in contextual results, you can use the following shorthand for deciding whether to center around the grand mean or within cluster: if you’re interested in level-1 predictors, CWC is best because it gives an unbiased estimate of the within cluster effect and produces better estimates of the slope variance, though as we saw you can get an unbiased estimate of the within cluster effect with CGM if you add the aggregate back in at level 2. If you’re interested in level-2 predictors, but you have covariates at level-1 you want to control for, CGM is best because it controls for level-1 predictors by producing adjusted means. If you are interested in interactions (at level-1 or cross-level), use CWC to get an unbiased estimate of the within cluster slope and slope variance. See Enders &amp; Tofighi (2007) for a detailed discussion. 8.3 Conclusion In this chapter, we reviewed two options for centering variables in MLMs (centering within cluster and centering grand mean), when to use each option, and how to interpret coefficients under each option. In the first 8 chapters, we’ve covered a lot of the basics of MLMs. In Chapter 9, we’ll revisit a number of concepts we’ve already seen, but in the context of repeated-measure rather than the cross-sectional data we’ve been using to this point. 8.4 Further Reading Enders, C. K., &amp; Tofighi, D. (2007). Centering predictor variables in cross-sectional multilevel models: A new look at an old issue. Psychological Methods, 12(2), 121–138. https://doi.org/10.1037/1082-989X.12.2.121 "],["09-module-9.html", "Chapter 9 Multilevel Modelling with Repeated Measures Data 9.1 Learning Objectives 9.2 Data Demonstration 9.3 Conclusion", " Chapter 9 Multilevel Modelling with Repeated Measures Data 9.1 Learning Objectives In this chapter, we will review fitting MLMs for repeated measures data. The learning objectives for this chapter are: Review multilevel modelling concepts discussed so far; Recognize when data are repeated measures and in the correct format for multilevel modelling; Conduct multilevel modelling on repeated measures data; Interpret coefficients for repeated measures data. All materials for this chapter are available for download here. 9.2 Data Demonstration 9.2.1 Load Dependencies For this data demo, we will use the following packages: library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for ICC 9.2.2 Review of Multilevel Modelling Procedure Multilevel modelling in repeated measures data is a new application of the techniques we’ve covered so far, so let’s briefly review the steps in our modelling framework: Establish solid theory and measurement, decide whether you need MLMs for your question Run random-intercept-only (i.e., null) model to calculate ICC and quantify extent of clustering in data Build model incrementally adding fixed and random effects per your theory, considering centering and estimation (REML or FIML) choices Conduct deviance test to compare model fits If you run into estimation issues, change your optimizer or remove problematic effects Report results: coefficients, significance, plausible values ranges, any changes made to address estimation issues 9.2.3 Multilevel Models for Repeated Measures Thus far, we’ve been using a cross-sectional example of students clustered within schools. Our level-1 variables have been about traits that students vary on (e.g., age, gender, SES) while our level-2 variables have been about traits that schools vary on (e.g., whether they are public or private schools). With repeated measures data, measures are clustered within person rather than having people clustered within some organizational structure. The level-1 variables are about traits that the measures vary on (e.g., experimental manipulations) whereas level-2 variables are about traits that people vary on (e.g., age, gender, SES). For example, imagine we show participants pictures of lines and ask them to estimate the line length. We measure how long it takes them to rate each line, making their reaction time the dependent variable. A level-1 predictor variable would be the line length; each participant sees several lines of different lengths. A level-2 predictor variable would be something demographic like a participant’s age or gender; each person has the same value on the variable for the entire experiment. In this chapter, we’ll look at repeated measures data without time in the model. This approaches assumes that time is not related to the outcome in the model. In Chapter 10, we’ll look at repeated measures data with time in the model, i.e., longitudinal models, which focus on how an outcome changes over time. 9.2.3.1 Data Structures: Long vs Wide Imagine you were measuring weight and caloric intake. If you have historically worked with repeated measures data in an ANOVA framework, you are probably used to working with data in a “wide” format, i.e., one row per participant with different variables for different measurement instances. id weight1 weight2 calories1 calories2 1 200 190 3500 3300 2 150 160 3200 3100 In MLMs, you need to use data in a “long” format where one row is one measurement occasion: id weight calories measurement_occasion 1 200 3500 1 1 190 3300 2 2 150 3200 1 2 160 3100 2 This requires transposing your data, which you can read more about here. An aside: you might also be used to thinking listwise deletion deletes an entire participant, because listwise deletion deletes rows with any missing data and in wide data one row is one participant. In long data, listwise deletion means deleting one measurement instance, not necessarily an entire participant. For example, if a participant answers a questionnaire a first time, then at one follow-up, but not at the second follow-up, listwise deletion will only remove their third row full of NAs; you’ll keep their data from the first two questionnaires. 9.2.4 Our Data: Reaction Time The data used in this module are used as an example in Hoffman and Rovine (2007). The article and supporting materials can be found here: http://www.lesahoffman.com/Research/MLM.html data &lt;- read.csv(&#39;hoffman2007.csv&#39;) Let’s look at our data: head(data) ## X id sex age NAME rt_sec Item meaning salience lg_rt oldage yrs65 c_mean c_sal ## 1 1 1 1 20 rt_sec1 4.662 1 3.5 4.0 1.5394445 0 0 0.5 1.0 ## 2 2 1 1 20 rt_sec2 6.660 2 0.0 3.0 1.8961195 0 0 -3.0 0.0 ## 3 3 1 1 20 rt_sec3 6.602 3 4.0 2.0 1.8873726 0 0 1.0 -1.0 ## 4 4 1 1 20 rt_sec4 1.332 4 4.0 4.0 0.2866816 0 0 1.0 1.0 ## 5 5 1 1 20 rt_sec5 1.332 5 0.0 5.0 0.2866816 0 0 -3.0 2.0 ## 6 6 1 1 20 rt_sec7 1.302 7 3.5 4.5 0.2639015 0 0 0.5 1.5 For this data demo the outcome of interest is the log of reaction time for participants to detect a change during a picture viewing task (rt_sec). The pictures varied on two dimensions: how meaningful driving was to the picture (meaning) and how salient the change was in the picture (salient). For this analysis we will focus on the variables centered from the midpoint of the rating (3): c_mean and c_sal. One of the primary research questions was how age related to reaction time, given those differences in pictures. Participants were sampled in age categories: younger (oldage = 0, 40 and under) and older (oldage = 1, above 40). Repeated trials are nested within persons. 9.2.5 Random-Intercept-Only/Null Model Let’s estimate our null model with FIML as our estimator and calculate the ICC: null_model &lt;- lmer(lg_rt ~ 1 + (1|id), data = data, REML = FALSE) # note that REML = FALSE performance::icc(null_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.252 ## Unadjusted ICC: 0.252 With repeated measures data, the ICC is interpreted as the proportion of variance between people: How much of the variance stems from people being different from one another versus fluctuating within themselves? A large ICC means that most of the variability is between people, not from people varying in their answers to a set of questions (or in this case, reaction time). The ICC is 0.252, indicating that 25.2% of the variance in log reaction time is attributed to a person. (Some bonus fun: when responses to questions are nested within person, Cronbach’s alpha is equivalent to the ICC: a high alpha indicates high “reliability” of the scale because most of the scale variance is between people. By contrast, if individuals answered inconsistently, most of the scale variance would be within person and we would get a lower alpha.) 9.2.6 Adding Level-1 Fixed Effects Let’s add our level-1 predictors for picture meaning c_mean and picture salience c_sal to our model. This is represented with the following formulae: Level Equation Level 1 \\(lg\\_rt_{ij} = \\beta_{0j} + \\beta_{1j}c\\_mean_{ij} + \\beta_{2j}c\\_sal_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) \\(\\beta_{2j} = \\gamma_{20}\\) Combined \\(lg\\_rt_{ij} = \\gamma_{00} + \\gamma_{10}c\\_mean_{ij} + \\gamma_{20}c\\_sal_{ij} + U_{0j} + R_{ij}\\) With this model, we’re estimating 5 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for c_mean and c_sal; \\(\\gamma_{10}\\): the fixed effect for the slope of c_mean, controlling for c_sal. This represents how meaning affects a person’s reaction time — do people respond more quickly or slowly to photos with changes that are related to driving a car (perhaps because they’re often driving and are attuned to changes in the environment when operating a car)? \\(\\gamma_{20}\\): the fixed effect for the slope of c_sal, controlling for c_mean. This represents how salience affects a person’s reaction time — do people respond more quickly or slowly to photos with more obvious changes? \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of people around the intercept, controlling for c_mean and c_sal; \\(\\sigma^2\\): a random effect variance capturing the variance of people around their own mean log reaction time, controlling for c_mean and c_sal. Let’s run the model with FIML as our estimator: l1_model &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + (1|id), data = data, REML = FALSE) summary(l1_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + (1 | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16525.9 16560.6 -8257.9 16515.9 7641 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5224 -0.7291 -0.1090 0.6177 3.8884 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.1756 0.4191 ## Residual 0.4785 0.6917 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.605e+00 3.483e-02 1.533e+02 46.09 &lt;2e-16 *** ## c_mean -5.153e-02 4.304e-03 7.493e+03 -11.97 &lt;2e-16 *** ## c_sal -1.323e-01 7.435e-03 7.493e+03 -17.80 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean ## c_mean 0.044 ## c_sal -0.015 -0.219 The intercept of 1.61 is the mean log reaction time across all people at average values of meaning and salience. A one-unit increase in meaning is associated with a decrease in log reaction time of 0.05 (i.e., a faster reaction time), at the average level of salience. A one-unit increase in salience is associated with a decrease in log reaction time of 0.13 at the average level of meaning. All coefficients are significant. The term describing how people vary around the grand mean intercept is 0.18. The term describing how people vary around their own intercept is 0.48. Does this model have significantly less deviance (i.e., better fit) than the null model alone? Let’s use a deviance test to check. Note that we can compare the null and level-1 models because we used FIML as our estimator and they are nested (i.e., all variables in the level-1 model are in the null model). anova(null_model, l1_model) ## Data: data ## Models: ## null_model: lg_rt ~ 1 + (1 | id) ## l1_model: lg_rt ~ 1 + c_mean + c_sal + (1 | id) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## null_model 3 17082 17103 -8537.9 17076 ## l1_model 5 16526 16561 -8257.9 16516 559.94 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The level-1 model does have significantly less deviance (16516 compared to 17076 for the null model), so is a better model. Hooray! 9.2.7 Adding Random Slopes Let’s try adding random slopes for our level-1 variables of meaning and salience. This allows slopes to vary across people. Maybe some people have stronger relationships between salience and reaction time — such that when the change is more salient they really notice it, and when it’s less salient they notice it less — while other people have eagle eyes and notice the changes no matter their salience. Level Equation Level 1 \\(lg\\_rt_{ij} = \\beta_{0j} + \\beta_{1j}c\\_mean_{ij} + \\beta_{2j}c\\_sal_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + U_{1j}\\) \\(\\beta_{2j} = \\gamma_{20} + U_{2j}\\) Combined \\(lg\\_rt_{ij} = \\gamma_{00} + \\gamma_{10}c\\_mean_{ij} + \\gamma_{20}c\\_sal_{ij} + U_{0j} + U_{1j}c\\_mean_{ij} + U_{2j}c\\_sal_{ij} + R_{ij}\\) Let’s not estimate random effect covariances, so with this model, we’re estimating 7 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for c_mean and c_sal; \\(\\gamma_{10}\\): the fixed effect for the slope of c_mean, controlling for c_sal; \\(\\gamma_{20}\\): the fixed effect for the slope of c_sal, controlling for c_mean; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of people around the intercept, controlling for c_mean and c_sal; \\(\\tau_1^2\\): a random effect variance capturing how people’s slopes for c_mean vary around the grand mean slope, controlling for c_sal; \\(\\tau_2^2\\): a random effect variance capturing how people’s slopes for c_sal vary around the grand mean slope, controlling for c_mean; \\(\\sigma^2\\): a random effect variance capturing the variance of people around their own mean log reaction time, controlling for c_mean and c_sal. Let’s run our model with random slope effects (but no covariances) in R: l1_random &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + (1|id) + (0 + c_mean|id) + (0 + c_sal|id), data = data, REML = FALSE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(l1_random) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + (1 | id) + (0 + c_mean | id) + (0 + c_sal | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16529.4 16578.0 -8257.7 16515.4 7639 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5320 -0.7287 -0.1087 0.6142 3.8842 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 1.756e-01 4.191e-01 ## id.1 c_mean 3.766e-09 6.137e-05 ## id.2 c_sal 6.853e-04 2.618e-02 ## Residual 4.777e-01 6.911e-01 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.605e+00 3.483e-02 1.533e+02 46.09 &lt;2e-16 *** ## c_mean -5.154e-02 4.301e-03 7.337e+03 -11.98 &lt;2e-16 *** ## c_sal -1.323e-01 7.724e-03 1.690e+02 -17.13 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean ## c_mean 0.044 ## c_sal -0.014 -0.211 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) We get an estimation error! Recall that singularity occurs when a variance term is close to zero or a correlation between variance terms is near 1 (high multicollinearity). Looking at our output, the variance terms for meaning and salience both look quite small. Let’s try to address our estimation issue by removing the smaller of the two, the random effect for c_mean. l1_random_without_cmean &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + (1|id) + (0 + c_sal|id), data = data, REML = FALSE) summary(l1_random_without_cmean) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + (1 | id) + (0 + c_sal | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16527.4 16569.0 -8257.7 16515.4 7640 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5320 -0.7287 -0.1087 0.6142 3.8842 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.1756384 0.41909 ## id.1 c_sal 0.0006842 0.02616 ## Residual 0.4776817 0.69115 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.605e+00 3.483e-02 1.533e+02 46.09 &lt;2e-16 *** ## c_mean -5.154e-02 4.301e-03 7.352e+03 -11.98 &lt;2e-16 *** ## c_sal -1.323e-01 7.724e-03 1.690e+02 -17.13 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean ## c_mean 0.044 ## c_sal -0.014 -0.211 That took care of our singularity issue. However, the random effect variance for salience still looks very small. Let’s conduct a deviance test to see if including the random effect reduces deviance at all (that is, whether it is worth it to estimate). anova(l1_random, l1_random_without_cmean) ## Data: data ## Models: ## l1_random_without_cmean: lg_rt ~ 1 + c_mean + c_sal + (1 | id) + (0 + c_sal | id) ## l1_random: lg_rt ~ 1 + c_mean + c_sal + (1 | id) + (0 + c_mean | id) + (0 + c_sal | id) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l1_random_without_cmean 6 16527 16569 -8257.7 16515 ## l1_random 7 16529 16578 -8257.7 16515 0 1 1 There is no significant difference between these models, so there doesn’t seem to be much benefit to including the random effect for salience because the model fits just as well without it. 9.2.8 Adding Level-2 Fixed Effects The level-2 variables in our dataset are demographic variables about participants, which in this dataset are their sex, age in years, whether they 40 or older (oldage = 1) or younger than 40 (oldage = 0), or their age centered at 65 years old for those who are older than 40 (yrs65). Let’s add oldage and sex as level-2 predictors of the intercept. Level Equation Level 1 \\(lg\\_rt_{ij} = \\beta_{0j} + \\beta_{1j}c\\_mean_{ij} + \\beta_{2j}c\\_sal_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}oldage_j + \\gamma_{02}sex_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) \\(\\beta_{2j} = \\gamma_{20}\\) Combined \\(lg\\_rt_{ij} = \\gamma_{00} + \\gamma_{01}oldage_j + \\gamma_{02}sex_j + \\gamma_{10}c\\_mean_{ij} + \\gamma_{20}c\\_sal_{ij} + U_{0j} + R_{ij}\\) We’re estimating 7 effects: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for c_mean and c_sal; \\(\\gamma_{10}\\): the fixed effect for the slope of c_mean, controlling for c_sal; \\(\\gamma_{20}\\): the fixed effect for the slope of c_sal, controlling for c_mean; \\(\\gamma_{01}\\): the fixed effect for the slope of oldage, controlling for sex, c_mean, and c_sal; \\(\\gamma_{02}\\): the fixed effect for the slope of sex, controlling for oldage, c_mean and c_sal; \\(\\tau_0^2\\): a random effect variance capturing how people’s mean log reaction times vary around the grand mean log reaction time, controlling for c_mean, c_sal, oldage, and sex; \\(\\sigma^2\\): a random effect variance capturing the variance of people around their own mean log reaction time, controlling for c_mean, c_sal, oldage, and sex. l2_model &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + oldage + sex + (1|id), data = data, REML = FALSE) summary(l2_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + oldage + sex + (1 | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16269.8 16318.4 -8127.9 16255.8 7639 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6108 -0.7341 -0.1058 0.6151 3.9626 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.02424 0.1557 ## Residual 0.47852 0.6918 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.281e+00 2.566e-02 1.532e+02 49.915 &lt;2e-16 *** ## c_mean -5.150e-02 4.304e-03 7.494e+03 -11.965 &lt;2e-16 *** ## c_sal -1.323e-01 7.434e-03 7.495e+03 -17.801 &lt;2e-16 *** ## oldage 7.994e-01 3.087e-02 1.532e+02 25.891 &lt;2e-16 *** ## sex 4.467e-02 3.045e-02 1.525e+02 1.467 0.144 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean c_sal oldage ## c_mean 0.059 ## c_sal -0.017 -0.219 ## oldage -0.392 -0.002 -0.003 ## sex -0.681 0.001 -0.002 -0.075 The intercept of 1.28 is the mean log reaction time across all people at average values of meaning and salience for men (sex = 0) who are younger than 40 (oldage = 40). A one-unit increase in meaning is associated with a decrease in log reaction time of 0.05 (i.e., a faster reaction time), controlling for other variables. A one-unit increase in salience is associated with a decrease in log reaction time of 0.13 controlling for other variables. People older than 40 have 0.80 units longer of a log reaction time on average, controlling for other variables. Women have 0.04 unit-slower log reaction times on average, controlling for other variables. All coefficients are significant except for sex. The term describing how people vary around the grand mean intercept is 0.02. The term describing how people vary around their own intercept is 0.48. Let’s run a model without the non-significant sex predictor and conduct a deviance test to see if the model fit is negatively impacted. # model l2_model_no_sex &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + oldage + (1|id), data = data, REML = FALSE) # deviance test anova(l2_model, l2_model_no_sex) ## Data: data ## Models: ## l2_model_no_sex: lg_rt ~ 1 + c_mean + c_sal + oldage + (1 | id) ## l2_model: lg_rt ~ 1 + c_mean + c_sal + oldage + sex + (1 | id) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l2_model_no_sex 6 16270 16312 -8129.0 16258 ## l2_model 7 16270 16318 -8127.9 16256 2.1363 1 0.1438 There is no significant difference in deviance, so we don’t lose on the model fit front if we don’t estimate the effect of sex. 9.2.9 Adding Cross-Level Interactions For our final model, let’s remove the level-2 term for sex and look at a cross-level interaction between oldage and the slope of c_mean to consider the question: does age alter the effect of meaning on reaction times? Level Equation Level 1 \\(lg\\_rt_{ij} = \\beta_{0j} + \\beta_{1j}c\\_mean_{ij} + \\beta_{2j}c\\_sal_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}oldage_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}oldage_j\\) \\(\\beta_{2j} = \\gamma_{20}\\) Combined \\(lg\\_rt_{ij} = \\gamma_{00} + \\gamma_{01}oldage_j + \\gamma_{10}c\\_mean_{ij} + \\gamma_{20}c\\_sal_{ij} + \\gamma_{11}c\\_mean_{ij}*oldage_j + U_{0j} + R_{ij}\\) We’re estimating 7 effects: \\(\\gamma_{00}\\): the fixed effect for the intercept, controlling for c_mean, c_sal, and oldage; \\(\\gamma_{10}\\): the fixed effect for the slope of c_mean, controlling for c_sal and oldage; \\(\\gamma_{20}\\): the fixed effect for the slope of c_sal, controlling for c_mean and oldage; \\(\\gamma_{01}\\): the fixed effect for the slope of oldage, controlling for c_mean and c_sal; \\(\\gamma_{11}\\): the fixed effect for the cross-level interaction of oldage with c_mean, controlling for c_sal; \\(\\tau_0^2\\): a random effect variance capturing how people’s mean log reaction times vary around the grand mean log reaction time, controlling for c_mean, c_sal, and oldage; \\(\\sigma^2\\): a random effect variance capturing the variance of people around their own mean log reaction time, controlling for c_mean, c_sal, and `oldage``. crosslevel_model &lt;- lmer(lg_rt ~ 1 + c_mean + c_sal + oldage + oldage:c_mean + (1|id), data = data, REML = FALSE) summary(crosslevel_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: lg_rt ~ 1 + c_mean + c_sal + oldage + oldage:c_mean + (1 | id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 16253.8 16302.4 -8119.9 16239.8 7639 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6032 -0.7333 -0.0988 0.6125 3.9049 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.0247 0.1572 ## Residual 0.4774 0.6909 ## Number of obs: 7646, groups: id, 153 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1.301e+00 1.895e-02 1.545e+02 68.674 &lt; 2e-16 *** ## c_mean -6.499e-02 5.337e-03 7.493e+03 -12.176 &lt; 2e-16 *** ## c_sal -1.325e-01 7.425e-03 7.495e+03 -17.838 &lt; 2e-16 *** ## oldage 8.154e-01 3.113e-02 1.561e+02 26.194 &lt; 2e-16 *** ## c_mean:oldage 3.717e-02 8.722e-03 7.495e+03 4.261 2.06e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) c_mean c_sal oldage ## c_mean 0.100 ## c_sal -0.025 -0.174 ## oldage -0.608 -0.057 -0.004 ## c_mean:oldg -0.058 -0.593 -0.005 0.095 The interaction between oldage and c_mean is 0.04, suggesting that people older than 40 have 0.04 added to their (log) reaction times for more meaningful photos. Let’s do some quick calculations to emphasize that point. Remember that lower reaction time is better here (faster response). The intercept represents average log reaction time for someone under 40 with a stimulus at average salience and meaning: 1.30 For people over 40, the average reaction time increases by 0.82 (coefficient for oldage) to 1.30 + 0.82 = 2.12 When photos are related to driving, the log reaction time decreases by -0.06 (the coefficient for c_mean). For people under 40, that’s 1.30 - 0.06 = 1.24 But when people are also older the benefit of the meaning is offset by age it increases by 0.03: 1.30 + 0.82 - 0.06 + 0.03 = 2.09 In summary, older people have slower reaction times. Photos being related to driving helps offset the effect of age somewhat, but even when photos are more related to driving they still have slower reaction times. The other coefficient interpretations are similar to those we discussed in earlier models. 9.3 Conclusion In this chapter, we reviewed our MLM pipeline and applied it to repeated measures without time in the model. In short, MLMs on repeated measures data can be executed in the same way as organizational models, however, the interpretations shift from being about a person clustered within some unit, to being about responses clustered within a person. In an organizational model, level-1 variables tend to measure aspects of a person, whereas these variables are often level-2 variables in a repeated measures model. In Chapter 10, we will look at longitudinal models, i.e., repeated measures with time in the model. "],["10-module-10.html", "Chapter 10 Multilevel Modelling with Longitudinal Data 10.1 Learning Objectives 10.2 Data Demonstration 10.3 Conclusion 10.4 Further Reading", " Chapter 10 Multilevel Modelling with Longitudinal Data 10.1 Learning Objectives In Chapter 9, we discussed how to interpret multilevel models with repeated measures data without time in the model. In this chapter, we will review fitting MLMs for longitudinal data, i.e., repeated measures with time in the model. The learning objectives for this chapter are: Recognize when data are longitudinal and in the correct format for multilevel modelling; Conduct multilevel modelling on longitudinal data; Interpret coefficients for longitudinal data; Review evidence to decide whether to retain effects. All materials for this chapter are available for download here. 10.2 Data Demonstration 10.2.1 Load Dependencies For this data demo, we will use the following packages: library(dplyr) # for data manipulation library(ggplot2) # for graphing library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for ICC 10.2.2 Multilevel Models for Longitudinal Data When we use cross-sectional data, level-1 variables vary between individuals (e.g., age, gender, SES) whereas level-2 variables vary between clusters (e.g., whether schools are public or private, number of students in a classroom, school funding). With repeated measures data without time in the model, level-1 variables vary between measurement occasions (e.g., experimental manipulations, stimulus volume or brightness) while level-2 variables vary between people (e.g., age, gender, SES). In this chapter, we’re looking at longitudinal models, i.e., repeated measures with time in the model. With longitudinal data, level-1 variables vary over time (e.g., weight at time 1, time 2, time 3; Testosterone levels at 10 minutes into game, 20 minutes into game, 30 minutes into game). Level-1 variables are also called “time-varying covariates” because they vary with time. Level-2 variables are the same across time (also called “time invariant covariates”), for example age, gender, SES (assuming these things don’t change over the study period, if you wait long enough these will or may change). For more on the topic of time-varying covariates, see McCoach and Kaniskan (2010). 10.2.2.1 Data Structures: Long vs Wide As with repeated measures, you need your data in long format to conduct an MLM. Imagine you were measuring weight and caloric intake. If you have historically worked with repeated measures data in an ANOVA framework, you are probably used to working with data in a “wide” format, i.e., one row per participant with different variables for different measurement instances. id weight1 weight2 calories1 calories2 1 200 190 3500 3300 2 150 160 3200 3100 In MLMs, you need to use data in a “long” format where one row is one measurement occasion: id weight calories measurement_occasion 1 200 3500 1 1 190 3300 2 2 150 3200 1 2 160 3100 2 10.2.3 Visualizing Testosterone Levels Over Time The data used in this chapter are a simplified version of the data from Casto, K.V., &amp; Edwards, D.A. (2016). Before, during, and after: How phases of competition differentially affect testosterone, cortisol, and estradiol levels in women athletes. Adaptive Human Behavior and Physiology, 2, 11-25. https://doi.org/10.1007/s40750-015-0028-2. Dr. Kathleen Casto gave us permission to use these data for teaching purposes (thanks Kathleen!); any other use requires additional permission. You can see more of her work here. data &lt;- read.csv(&#39;casto2016.csv&#39;) head(data) ## Code position HormonCont Played minplayed game time Cortisol Testosterone Estradiol time0 ## 1 1224 1 0 1 90 1 2 0.456 84.541 3.706 0 ## 2 1224 1 0 1 90 1 3 0.575 118.942 5.631 1 ## 3 1224 1 0 1 90 1 4 0.303 104.803 3.372 2 ## 4 1348 2 0 1 90 1 2 0.649 39.547 2.620 0 ## 5 1348 2 0 1 90 1 3 0.323 44.291 3.010 1 ## 6 1348 2 0 1 90 1 4 0.879 62.774 3.650 2 These data include hormone levels of female athletes in a competition setting (soccer game), if they played in the game or not (Played), what position they played (position), how long they played (minplayed), their testosterone levels (Testosterone), and if they were taking birth control (HormonCont). We will focus on testosterone, if they played, and if they were taking birth control for this example. Graphs are very helpful for understanding patterns over time visually, which has two immediate implications for our model-building. First, we can get a sense of whether predictors are valuable additions to our model. Second, they can be used to determine the functional form of a model (e.g., is the relationship between variables over time linear? exponential? something else?). Note that in our example, there are three time points: before a game starts, during the game, and after the game. These are indicated in the below graphs with x-axis labels 0, 1, and 2, respectively. Three is the minimal number of points required to estimate a slope for time in a longitudinal analysis, because with only two time points you are just looking at the difference between two time points, not a trend over time. See McCoach and Kaniskan (2010) for more. Let’s start by visualizing testosterone levels over time for the entire sample: data %&gt;% group_by(time0) %&gt;% mutate(tmean = mean(Testosterone)) %&gt;% # mean testosterone per timepoint ggplot(mapping = aes(x = time0, y = tmean)) + geom_line() + labs(title = &quot;Testosterone Over Time for Entire Sample&quot;) + scale_x_discrete(limits = c(0, 1, 2)) # adjust x-axis labels The relationship between time and testosterone doesn’t look exactly linear. Let’s check if there are different relationships between people who played and people who didn’t. data %&gt;% group_by(time0, Played) %&gt;% # group by timepoint and played mutate(tmean = mean(Testosterone)) %&gt;% ggplot(mapping = aes(x = time0, y = tmean, colour = factor(Played))) + geom_line() + labs(title = &quot;Testosterone Over Time, Played vs. Did Not Play&quot;) + scale_x_discrete(limits = c(0, 1, 2)) Those who played have a pretty linear increase in testosterone over time, while those who did not play go up from time 0 to 1 and then back down between 1 and 2, so it looks like playing or not might be an important level-2 predictor for our model intercepts and slopes. Finally, let’s check whether birth control seems like an important predictor of intercepts and slopes: data %&gt;% group_by(time0, HormonCont) %&gt;% # group by timepoint and birth control mutate(tmean = mean(Testosterone)) %&gt;% ggplot(mapping = aes(x = time0, y = tmean, colour = factor(HormonCont))) + geom_line() + labs(title = &quot;Testosterone Over Time, Birth Control or Not&quot;) + scale_x_discrete(limits = c(0, 1, 2)) Looks like another variable we might want to consider in our model! Let’s begin with our null model and incrementally build from there. 10.2.4 Random-Intercept-Only/Null Model Let’s estimate our null model with FIML as our estimator and calculate the ICC. Why FIML? We’re going to be adding fixed effects (played, birth control) and comparing model fits. Level Equation Level 1 \\(testosterone_{ij} = \\beta_{0j} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) Combined \\(testosterone_{ij} = \\gamma_{00} + U_{0j} + R_{ij}\\) Here, we’re estimating three parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean testosterone levels across all people; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of people’s average testosterone levels around the intercept; \\(\\sigma^2\\): a random effect variance capturing the variance of people around their own average testosterone level. null_model &lt;- lmer(Testosterone ~ 1 + (1|Code), data = data, REML = FALSE) summary(null_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Testosterone ~ 1 + (1 | Code) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 634.5 641.4 -314.3 628.5 70 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6055 -0.5563 -0.2220 0.4901 2.8619 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Code (Intercept) 252.9 15.90 ## Residual 186.1 13.64 ## Number of obs: 73, groups: Code, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 59.643 3.569 24.876 16.71 4.94e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 performance::icc(null_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.576 ## Unadjusted ICC: 0.576 With longitudinal data, as with repeated measures without time, the ICC is interpreted as the proportion of variance between people: How much of the variance stems from people being different from one another versus fluctuating within themselves? A large ICC means that most of the variability is between people, not from people varying in their testosterone levels. The ICC is 0.576, indicating that 57.6% of the variance in testosterone is attributed to a person. 10.2.5 Adding Level-1 Fixed and Random Effects Let’s add our level-1 predictor, time0, to our model. Note that time0 is coded such that 0 is the first measurement occasion at the beginning of the game, 1 is mid-game, and 2 is at the end of the game. Level Equation Level 1 \\(testosterone_{ij} = \\beta_{0j} + \\beta_{1j}time0_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + U_{1j}\\) Combined \\(testosterone_{ij} = \\gamma_{00} + \\gamma_{10}time0_{ij} + U_{0j} + U_{1j}time0_{ij} + R_{ij}\\) With this model, we’re estimating 6 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean testosterone levels at time 0 across all people; \\(\\gamma_{10}\\): the fixed effect for time0, effect of time on testosterone levels; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of people’s average testosterone levels around the intercept; \\(\\tau_1^2\\): a random effect variance for the slope of time0 capturing the variance of people’s slopes around the grand mean slope; \\(\\sigma^2\\): a random effect variance capturing the variance of people around their own average testosterone level. \\(\\tau_{01}\\): the covariance between the random intercept and random slope. Do people who have higher values of testosterone at time0 have particularly lower or higher slopes? Let’s run the model with FIML as our estimator: l1_model &lt;- lmer(Testosterone ~ 1 + time0 + (time0|Code), data = data, REML = FALSE) summary(l1_model) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Testosterone ~ 1 + time0 + (time0 | Code) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 601.1 614.8 -294.5 589.1 67 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.65381 -0.55990 -0.07896 0.50762 2.39316 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Code (Intercept) 154.67 12.437 ## time0 24.12 4.911 0.87 ## Residual 78.58 8.865 ## Number of obs: 73, groups: Code, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 50.665 2.971 25.027 17.054 2.73e-15 *** ## time0 9.092 1.625 24.030 5.595 9.26e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## time0 0.114 as.matrix(Matrix::bdiag(VarCorr(l1_model))) ## (Intercept) time0 ## (Intercept) 154.67317 53.07523 ## time0 53.07523 24.11744 Looking at our fixed effects, the intercept of 50.67 is the average testosterone level across people at time 0. The slope of 9.09 indicates the average increase in testosterone levels as time increases by one. The variance term describing how people vary around the intercept is 154.67. The variance term describing how people’s slopes vary is 24.12. The covariance between the random intercept and random slope is 53.07 (correlation of 0.87), indicating that those with higher initial levels of testosterone also have higher slopes (more intense increase in testosterone over time). 10.2.6 Evidence for Retaining Effects The fixed effect for time seems both statistically and practically significant. Variances are difficult to interpret in isolation, so we can consider a few sources of evidence when examining whether to keep the random effect for slope: deviance testing, 95% confidence interval, 95% plausible values range, and visualizing variance. We can use the built-in anova() function to conduct our deviance test for the model with and without the slope covariance: l1_model_no_U1j &lt;- lmer(Testosterone ~ 1 + time0 + (1|Code), data = data, REML = FALSE) anova(l1_model, l1_model_no_U1j) ## Data: data ## Models: ## l1_model_no_U1j: Testosterone ~ 1 + time0 + (1 | Code) ## l1_model: Testosterone ~ 1 + time0 + (time0 | Code) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l1_model_no_U1j 4 607.23 616.39 -299.61 599.23 ## l1_model 6 601.09 614.83 -294.54 589.09 10.14 2 0.006281 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is significantly less deviance in the model with the slope variance term. Let’s look at 95% confidence intervals for all effects: confint(l1_model, oldNames = F) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## sd_(Intercept)|Code 8.08885766 18.508662 ## cor_time0.(Intercept)|Code -0.02405199 1.000000 ## sd_time0|Code 1.82944849 9.035275 ## sigma 6.84599241 11.178406 ## (Intercept) 44.60763264 56.714968 ## time0 5.76797005 12.405566 We’ve suppressed the warnings in this output for brevity, but if you run this code yourself you’ll see a lot of warnings! That’s likely because of the covariance term, given that the correlation cor_time0.(Intercept)|Code runs up against the upper bound of 1.00 within the confidence interval. Let’s remove that term. l1_model_cov0 &lt;- lmer(Testosterone ~ 1 + time0 + (1|Code) + (0 + time0|Code), data = data, REML = FALSE) anova(l1_model, l1_model_cov0) ## Data: data ## Models: ## l1_model_cov0: Testosterone ~ 1 + time0 + (1 | Code) + (0 + time0 | Code) ## l1_model: Testosterone ~ 1 + time0 + (time0 | Code) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l1_model_cov0 5 602.67 614.12 -296.33 592.67 ## l1_model 6 601.09 614.83 -294.54 589.09 3.5849 1 0.05831 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our model fit is not significantly different after removing the covariance term, so let’s proceed without it and look at our confidence intervals again: confint(l1_model_cov0, oldNames = F) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## sd_(Intercept)|Code 10.123069 20.33901 ## sd_time0|Code 2.941503 10.19590 ## sigma 6.549504 10.66364 ## (Intercept) 44.090984 57.21964 ## time0 5.589114 12.80020 The confidence interval for our slope variance, sd_time0|Code, does not contain 0, so it is significant. Finally, one way to visualize variability is with a graph: what are the different slope values across people? This is the same as our Empirical Bayes plotting exercise in Chapter 4. # Extract Empirical Bayes estimates and graph them as_tibble(coef(l1_model)$Code) %&gt;% ggplot(mapping = aes(x = time0)) + geom_histogram(bins = 5) To summarize our evidence: the model with the random slope effect fits better, the random effect confidence interval does not contain zero (i.e., it is significant), and we can see that slopes for time vary, many people’s slopes around 5 or 10 but some as high as ~20. All signs point to keeping the random effect. 10.2.7 Adding Level-2 Fixed Effects Let’s start adding the level-2 effects we graphed at the beginning: playing and birth control. First, let’s add whether someone played or not. Level Equation Level 1 \\(testosterone_{ij} = \\beta_{0j} + \\beta_{1j}time0_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}played_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}played_j + U_{1j}\\) Combined \\(testosterone_{ij} = \\gamma_{00} + \\gamma_{01}*played_j + \\gamma_{10}time0_{ij} + \\gamma_{11}time0_{ij}*played_j + U_{0j} + U_{1j}time0_{ij} + R_{ij}\\) With this model, we’re estimating 7 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean testosterone levels at time 0 across those who did not play (played = 0); \\(\\gamma_{10}\\): the fixed effect for time0, effect of time on testosterone levels controlling for playing; \\(\\gamma_{01}\\): the fixed effect for playing, effect of playing on testosterone at time0; \\(\\gamma_{11}\\): interaction term between playing and time, the effect of playing on the effect of time on testosterone levels; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of people’s average testosterone levels around the intercept controlling for time and playing; \\(\\tau_1^2\\): a random effect variance for the slope of time0 capturing the variance of people’s slopes around the grand mean slope controlling for playing; \\(\\sigma^2\\): a random effect variance capturing the variance of people around their own average testosterone level, controlling for time and playing; We are not estimating a covariance term. l2_played &lt;- lmer(Testosterone ~ 1 + time0 + Played + Played:time0 + (1|Code) + (0 + time0|Code), data = data, REML = FALSE) summary(l2_played) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Testosterone ~ 1 + time0 + Played + Played:time0 + (1 | Code) + (0 + time0 | Code) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 599.1 615.1 -292.5 585.1 66 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.5068 -0.5964 -0.1015 0.3243 2.4437 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Code (Intercept) 182.24 13.500 ## Code.1 time0 27.22 5.217 ## Residual 67.89 8.239 ## Number of obs: 73, groups: Code, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 44.526 5.479 26.886 8.126 1.03e-08 *** ## time0 3.929 2.942 25.406 1.335 0.1936 ## Played 9.004 6.638 26.811 1.356 0.1863 ## time0:Played 7.389 3.500 25.476 2.111 0.0448 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) time0 Played ## time0 -0.265 ## Played -0.825 0.218 ## time0:Playd 0.222 -0.840 -0.270 Looking at our fixed effects, the intercept of 44.53 is the average testosterone level at time 0 across people who didn’t play. The slope of time, 3.93, indicates the average increase in testosterone levels over one unit of time controlling for playing. The slope of having played, 9.00, indicates that those who played had on average 9 more units of Testosterone than those who didn’t at time 0. The interaction between time and having played indicates that those who played had a 7.39-unit higher slope on average than those who didn’t. The variance term describing how people vary around the intercept is 182.24. The variance term describing how people’s time slopes vary is 27.22. Let’s do a quick deviance test to see if including playing decreases deviance (i.e., improves model fit): anova(l1_model_cov0, l2_played) ## Data: data ## Models: ## l1_model_cov0: Testosterone ~ 1 + time0 + (1 | Code) + (0 + time0 | Code) ## l2_played: Testosterone ~ 1 + time0 + Played + Played:time0 + (1 | Code) + (0 + time0 | Code) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l1_model_cov0 5 602.67 614.12 -296.33 592.67 ## l2_played 7 599.09 615.12 -292.54 585.09 7.583 2 0.02256 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model with played does fit significantly better. Finally, let’s add a fixed effect for birth control. Level Equation Level 1 \\(testosterone_{ij} = \\beta_{0j} + \\beta_{1j}time0_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}played_j + \\gamma_{02}birth\\_control_j + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}played_j + \\gamma_{12}birth\\_control_j + U_{1j}\\) Combined \\(testosterone_{ij} = \\gamma_{00} + \\gamma_{01}*played_j + \\gamma_{02}*birth\\_control_j + \\gamma_{10}time0_{ij} + \\gamma_{11}time0_{ij}*played_j + \\gamma_{12}time0_{ij}*birth\\_control_j + U_{0j} + U_{1j}time0_{ij} + R_{ij}\\) Here, we’re estimating 9 effects, the 7 previously described plus: \\(\\gamma_{02}\\): the fixed effect for birth control, effect of taking birth control on baseline testosterone levels controlling for playing; \\(\\gamma_{12}\\): interaction term between playing and birth control. l2_played_birthcontrol &lt;- lmer(Testosterone ~ 1 + time0 + Played + time0:Played + HormonCont + time0:HormonCont + (1|Code) + (0 + time0|Code), data = data, REML = FALSE) summary(l2_played_birthcontrol) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Testosterone ~ 1 + time0 + Played + time0:Played + HormonCont + ## time0:HormonCont + (1 | Code) + (0 + time0 | Code) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 600.4 621.0 -291.2 582.4 64 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.5413 -0.5573 -0.1349 0.3886 2.4161 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Code (Intercept) 159.27 12.620 ## Code.1 time0 27.99 5.291 ## Residual 67.37 8.208 ## Number of obs: 73, groups: Code, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 50.5745 6.4512 26.8271 7.840 2.07e-08 *** ## time0 4.0889 3.5168 25.6439 1.163 0.2557 ## Played 9.2374 6.3078 26.9286 1.464 0.1547 ## HormonCont -9.7086 6.1205 26.8019 -1.586 0.1244 ## time0:Played 7.4842 3.5225 25.5433 2.125 0.0435 * ## time0:HormonCont -0.3946 3.3160 25.6701 -0.119 0.9062 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) time0 Played HrmnCn tm0:Pl ## time0 -0.287 ## Played -0.652 0.187 ## HormonCont -0.591 0.169 -0.024 ## time0:Playd 0.183 -0.668 -0.280 0.007 ## tm0:HrmnCnt 0.171 -0.543 0.007 -0.289 -0.067 Matrix::bdiag(VarCorr(l2_played_birthcontrol)) ## 2 x 2 sparse Matrix of class &quot;dsCMatrix&quot; ## ## [1,] 159.2723 . ## [2,] . 27.99435 anova(l2_played, l2_played_birthcontrol) ## Data: data ## Models: ## l2_played: Testosterone ~ 1 + time0 + Played + Played:time0 + (1 | Code) + (0 + time0 | Code) ## l2_played_birthcontrol: Testosterone ~ 1 + time0 + Played + time0:Played + HormonCont + time0:HormonCont + (1 | Code) + (0 + time0 | Code) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## l2_played 7 599.09 615.12 -292.54 585.09 ## l2_played_birthcontrol 9 600.36 620.97 -291.18 582.36 2.7264 2 0.2558 Taking birth control is associated with a drop in Testosterone by 9.71 units, controlling for all other variables, and the interaction between time and birth control was -0.39, indicating that the slope for time is 4.09 - 0.39 = 3.70 for people taking birth control. Neither effect was statistically significant. 10.3 Conclusion In this chapter, we estimated and interpreted models for longitudinal data and reviewed some of the evidence available to us in making model construction decisions: deviance testing, 95% confidence intervals, and visualizing Empirical Bayes estimates. Longitudinal models are complex and an entire course could be spent on them. This was an introduction to a simple example case. For more, we recommend Dr. Lesa Hoffman’s textbook as a starting point. In Chapter 11, we will review another source of evidence for model-building and interpretation: effect sizes and R-squared in multilevel models. 10.4 Further Reading Hoffman, L. (2015). Longitudinal analysis: Modeling within-person fluctuation and change. Routledge/Taylor &amp; Francis Group. McCoach, D. B., &amp; Kaniskan, B. (2010). Using time-varying covariates in multilevel growth models. Frontiers in psychology, 1, 17. https://doi.org/10.3389/fpsyg.2010.00017 "],["11-module-11.html", "Chapter 11 Effect Sizes in Multilevel Models 11.1 Learning Objectives 11.2 Data Demonstration 11.3 Conclusion 11.4 Additional Reading", " Chapter 11 Effect Sizes in Multilevel Models 11.1 Learning Objectives In this chapter we will discuss effect sizes in multilevel models, with a particular focus on R-squared. Note that this is an adaptation of a published work, Shaw, Rights, Sterba, and Flake (2022). The learning objectives for this chapter are: Define effect sizes; Understand the components of Rights &amp; Sterba’s (2019) framework for R-squared in MLMs; Implement and interpret R-squared results for single models, via automatic and manual entry; Select models to compare for a given research question and interpret comparison output. All materials for this chapter are available for download here. 11.2 Data Demonstration 11.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(r2mlm) # for R-squared values library(lme4) # for multilevel models library(lmerTest) # for p-values library(performance) # for ICC We will be using the teachsat dataset included in the r2mlm package. This is simulated data about teacher job satisfaction. Teachers are clustered within schools. The level-1 variables are school-centered salary and control over curriculum, and the level-2 variables are school average salary, school average control over curriculum, and student-teacher ratio. data(teachsat) 11.2.2 Defining Effect Sizes Effect sizes are important for contextualizing the magnitude of results from your model. Effect sizes go beyond statistical significance to ask “what is the practical significance of this result? Do we care about the effect of this predictor?” Effect sizes can be standardized or unstandardized (expressed in the units of the dependent variable). Regression coefficients are one example of an unstandardized effect size that we’ve already seen in earlier chapters: for a one-unit increase in SES, math scores increase/decrease by some amount. If that coefficient is very small, like 0.01 points, even if it’s statistically significant we’re probably not that interested in a 0.01% increase in test scores. Unstandardized effect sizes like this are useful when the units are interpretable, like with math test scores. Standardized effect sizes measure magnitude without units, and as such can be particularly useful when original metrics are not particularly interpretable (e.g., log reaction times, scores on Likert scales that don’t have inherent meaning). 11.2.3 R-squared in Multilevel Models One example of a standardized effect size is R-squared, or proportion of variance in the outcome explained by a model. In single-level regression, it is calculated as the outcome variance explained by the model divided by the total outcome variance: \\[R^2 = \\frac{explained\\ variance}{total\\ variance}\\] This yields an intuitive variance explained measure ranging from 0 to 1, with 0 indicating 0% explained and 1 indicating 100% explained. In multilevel regression, a single R-squared term cannot accurately capture variance explained because there are multiple sources of variance and kinds of predictors explaining that variance. As we have discussed, we partition total variance into within and between variance, so we have three possible denominators: total outcome variance, outcome variance within a cluster, and outcome variance between clusters. Further, we have multiple sources of explained variance, so we have four possible unique numerators: fixed effects at level-1, fixed effects at level-2, a random intercept effect, and random slope effects. With all these sources of variance and options for explaining variance, a single R-squared value cannot capture variance explained for MLMs. Rights &amp; Sterba (2019) first detailed a comprehensive framework that accounts for all these sources of variance and variance explained and can be referenced for a more detailed explanation of the framework. Here, we provide an overview of the 12 R-squared terms in the framework. 11.2.3.1 Within Variance Explained At the within level of the model, there are three possible sources of variance: the level-1 predictors via the fixed effects (shorthand: “f1”), the level-1 predictors via the random effects (shorthand: “v”), and the level-1 residuals (shorthand: resid). Our denominator for within R-squareds contains these three terms, because the sum of all three represents the total within variance: \\[R^2_{within} = \\frac{explained\\ variance}{var_{f1} + var_{v} + var_{resid}}\\] You can then calculate two distinct effect sizes from this. The first is within variance explained by level 1 predictors via fixed effects: \\[R^{2(f1)}_{within} = \\frac{var_{f1}}{var_{f1} + var_{v} + var_{resid}}\\] The second is within variance explained by random slopes: \\[R^{2(v)}_{within} = \\frac{var_{v}}{var_{f1} + var_{v} + var_{resid}}\\] You can also calculate a higher-level term for variance explained by BOTH level-1 fixed effects and random slopes: \\[R^{2(f1v)}_{within} = R^{2(f1)}_{within} + R^{2(v)}_{within} = \\frac{var_{f1} + var_{v}}{var_{f1} + var_{v} + var_{resid}}\\] We’re not calculating an \\(R^{2(resid)}_{within}\\) with \\(var_{resid}\\) in the numerator because the residuals are all variance left after accounting for all predictors, so it doesn’t make sense to have “variance explained by residual variance.” Regarding notation: a given R-squared is described by two elements: a subscript and a superscript. The subscripts indicate at what level variance is being explained: “within” for within-cluster, “between” for between-cluster, and “total” for total. The superscripts indicate what potential sources of variance are contributing to variance explained: “f1” for level 1 predictors via fixed effects, “f2” for level-2 predictors via fixed effects, and so on. 11.2.3.2 Between Variance Explained Between variance is composed of the contribution of level 2 predictors via fixed effects (shorthand: “f2”) and cluster-specific means via intercept variation (shorthand: “m”): \\[R^2_{between} = \\frac{explained\\ variance}{var_{f2} + var_{m}}\\] We have two R-squared terms here. The first is for between-variance explained by level-2 fixed effects: \\[R^{2(f2)}_{between} = \\frac{var_{f2}}{var_{f2} + var_{m}}\\] The second is for between-variance explained by random intercept variation: \\[R^{2(m)}_{between} = \\frac{var_{m}}{var_{f2} + var_{m}}\\] We are not calculating a higher-level term \\[R^{2(f2m)}_{between} = \\frac{var_{f2} + var_{m}}{var_{f2} + var_{m}}\\] because all variance between clusters is captured by fixed-effects at level-2 and random intercept variation, so this term would always be equal to 1. 11.2.3.3 Total Variance Explained Total variance then is the combination of within and between variance explained, and thus total R-squared measures take the following form: \\[R^2_{total} = \\frac{explained\\ variance}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] There are four possible unique R-squared terms here. The first is total variance explained by fixed effects at level-1: \\[R^{2(f1)}_{total} = \\frac{var_{f1}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] The second is total variance explained by fixed effects at level-2: \\[R^{2(f2)}_{total} = \\frac{var_{f2}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] The third is total variance explained by random slope variation: \\[R^{2(v)}_{total} = \\frac{var_{v}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] And the fourth is total variance explained by random intercept variation: \\[R^{2(m)}_{total} = \\frac{var_{m}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] There are also three higher-level terms. The first is total variance explained by fixed effects at both level-1 and level-2: \\[R^{2(f)}_{total} = R^{2(f1)}_{total} + R^{2(f2)}_{total} = \\frac{var_{f1} + var_{f2}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] The second is total variance explained by fixed effects at both level-1 and level-2 and random slope variation: \\[R^{2(fv)}_{total} = R^{2(f1)}_{total} + R^{2(f2)}_{total} + R^{2(v)}_{total} = \\frac{var_{f1} + var_{f2} + var_v}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] The third and final higher-level term is total variance explained by fixed effects at level-1 and level-2, random slope variation, and random intercept variation: \\(R^{2(fvm)}_{total} = R^{2(f1)}_{total} + R^{2(f2)}_{total} + R^{2(v)}_{total} + R^{2(m)}_{total}= \\frac{var_{f1} + var_{f2} + var_v + var_m}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\) You could also calculate higher-level terms \\(R^{2(f1v)}_{total}\\), \\(R^{2(f2v)}_{total}\\), \\(R^{2(f1m)}_{total}\\), \\(R^{2(f2m)}_{total}\\), \\(R^{2(f1vm)}_{total}\\), and \\(R^{2(f2vm)}_{total}\\) if you wanted by summing the appropriate individual terms, but those are not automatically calculated for you in the r2mlm function we’ll see in a moment because they’re not as widely substantively interesting as the other combinations. In all, we have 12 R-squared measures: 3 within measures, 2 between measures, and 7 total measures. Here is a table for your quick reference: Measure Definition/Interpretation \\[R^{2(f1)}_{total} = \\frac{var_{f1}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by level-1 predictors via fixed slopes \\[R^{2(f2)}_{total} = \\frac{var_{f2}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by level-2 predictors via fixed slopes \\[R^{2(v)}_{total} = \\frac{var_{v}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by level-1 predictors via random slope variation/covariation \\[R^{2(m)}_{total} = \\frac{var_{m}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by cluster-specific outcome means via random intercept variation \\[R^{2(f)}_{total} = \\frac{var_{f1} + var_{f2}}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by all predictors via fixed slopes \\[R^{2(fv)}_{total} = \\frac{var_{f1} + var_{f2} + var_v}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by predictors via fixed slopes and random slope variation/covariation \\[R^{2(fvm)}_{total} = \\frac{var_{f1} + var_{f2} + var_v + var_m}{var_{f1} + var_{f2} + var_{v} + var_{m} + var_{resid}}\\] Proportion of total outcome variance explained by predictors via fixed slopes and random slope variation/covariation and by cluster-specific outcome means via random intercept variation \\[R^{2(f1)}_{within} = \\frac{var_{f1}}{var_{f1} + var_{v} + var_{resid}}\\] Proportion of within-cluster outcome variance explained by level-1 predictors via fixed slopes \\[R^{2(v)}_{within} = \\frac{var_{v}}{var_{f1} + var_{v} + var_{resid}}\\] Proportion of within-cluster outcome variance explained by level-1 predictors via random slope variation/covariation \\[R^{2(f1v)}_{within} = \\frac{var_{f1} + var_{v}}{var_{f1} + var_{v} + var_{resid}}\\] Proportion of within-cluster outcome variance explained by level-1 predictors via fixed slopes and random slope variation/covariation \\[R^{2(f2)}_{between} = \\frac{var_{f2}}{var_{f2} + var_{m}}\\] Proportion of between-cluster outcome variance explained by level-2 predictors via fixed slopes \\[R^{2(m)}_{between} = \\frac{var_{m}}{var_{f2} + var_{m}}\\] Proportion of between-cluster outcome variance explained by cluster-specific outcome means via random intercept variation Let’s look at an example with our teacher job satisfaction data to develop our intuition for using this framework and interpreting its results. 11.2.4 Single Model, Automatic Entry Let’s begin with a null model with teacher job satisfaction (satisfaction) as the outcome with school (schoolID) as the clustering variable and REML as the estimator. Level Equation Level 1 \\(satisfaction_{ij} = \\beta_{0j} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + U_{0j}\\) Combined \\(satisfaction_{ij} = \\gamma_{00} + U_{0j} + R_{ij}\\) We’re estimating 3 parameters here: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean teacher job satisfaction across all schools; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of school’s average job satisfaction levels around the intercept; \\(\\sigma^2\\): a random effect variance capturing the variance of teachers around their school average job satisfaction. null_model &lt;- lmer(satisfaction ~ 1 + (1|schoolID), data = teachsat, REML = TRUE) summary(null_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: satisfaction ~ 1 + (1 | schoolID) ## Data: teachsat ## ## REML criterion at convergence: 29426.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.8004 -0.6500 -0.0083 0.6370 3.4825 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schoolID (Intercept) 0.8376 0.9152 ## Residual 1.3955 1.1813 ## Number of obs: 9000, groups: schoolID, 300 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 5.99852 0.05429 298.99999 110.5 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The fixed effect for our intercept is 6.00, indicating the average teacher job satisfaction across all schools. The random effect variance describing how schools vary around that average is 0.70, and the random effect variance describing how teachers vary around their school averages is 1.52. Let’s use the automatic r2mlm() function for the first time to calculate how much variance in teacher job satisfaction is explained by the null model. r2mlm(null_model) ## $Decompositions ## total within between ## fixed, within 0.0000000 0 NA ## fixed, between 0.0000000 NA 0 ## slope variation 0.0000000 0 NA ## mean variation 0.3750827 NA 1 ## sigma2 0.6249173 1 NA ## ## $R2s ## total within between ## f1 0.0000000 0 NA ## f2 0.0000000 NA 0 ## v 0.0000000 0 NA ## m 0.3750827 NA 1 ## f 0.0000000 NA NA ## fv 0.0000000 0 NA ## fvm 0.3750827 NA NA There are three parts to our output: Decompositions, R2s, and a graph. The decompositions output gives us all of our unique R-squared estimates. The R2s output gives us the unique output plus the higher-level combination output: (1) variance explained by all fixed effects at both level-1 and level-2 (represented by “f”), (2) variance explained by all fixed effects and random slope variances (“fv”), and (3) variance explained by all fixed effects, random slope variances, and the random intercept variance (“fvm”). The graph visualizes the unique R-squared estimates from the Decompositions output and includes a legend. Looking at our output, we can see that the variance explained by our random intercept, “m”, is 0.316, or 31.6%. What is this number? It’s the ICC, the variance attributed to school membership! We can use our familiar ICC function to see that it matches our r2mlm output: performance::icc(null_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.375 ## Unadjusted ICC: 0.375 Let’s now use r2mlm to fit a larger model predicting job satisfaction from salary (with a random slope), control over curriculum, and student-teacher ratio: Level Equation Level 1 \\(Y_{satisfaction} = \\beta_{0j} + \\beta_{1j}*salary\\_c_{ij} + \\beta_{2j}*control\\_c_{ij} + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}*s\\_t\\_ratio_{j} + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10}\\) \\(\\beta_{2j} = \\gamma_{20} + U_{2j}\\) Combined \\(Y_{satisfaction} = \\gamma_{00} + \\gamma_{01}*s\\_t\\_ratio_{j} + \\gamma_{10}*salary\\_c_{ij} + \\gamma_{20}*control\\_c_{ij} + U_{0j} + U_{2j}*control\\_c_{ij} + R_{ij}\\) We will be estimating 8 parameters: \\(\\gamma_{00}\\): the fixed effect for the intercept, mean job satisfaction levels controlling for salary, curriculum-control, and student-teacher ratio; \\(\\gamma_{01}\\): the fixed effect for student-teacher ratio, effect of school-wide class size on job satisfaction controlling for salary and curriculum; \\(\\gamma_{10}\\): the fixed effect for salary_c, effect of salary on job satisfaction within a school, controlling for curriculum and student-teacher ratio; \\(\\gamma_{20}\\): the fixed effect for control_c, effect of control over curriculum on job satisfaction within a school, controlling for salary and student-teacher ratio; \\(\\tau_0^2\\): a random effect variance for the intercept capturing the variance of schools’ average job satisfaction around the intercept controlling for salary, curriculum, and student-teacher ratio; \\(\\tau_2^2\\): a random effect variance for the slope of curriculum control capturing the variance of schools’ curriculum slopes around the grand mean slope controlling for salary and student-teacher ratio; \\(\\sigma^2\\): a random effect variance capturing the variance of teachers around their school average job satisfaction, controlling for salary, curriculum, and student-teacher ratio; \\(\\tau_{02}\\): the covariance between the random intercept and random slope. Do schools with higher job satisfaction intercepts have higher/lower effects of curriculum? full_model &lt;- lmer(satisfaction ~ 1 + control_c + salary_c + s_t_ratio + (1 + control_c | schoolID), data = teachsat, REML = TRUE) summary(full_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: satisfaction ~ 1 + control_c + salary_c + s_t_ratio + (1 + control_c | schoolID) ## Data: teachsat ## ## REML criterion at convergence: 24144.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8197 -0.6599 -0.0026 0.6555 3.8699 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schoolID (Intercept) 0.66952 0.8182 ## control_c 0.02962 0.1721 0.06 ## Residual 0.72895 0.8538 ## Number of obs: 9000, groups: schoolID, 300 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 7.364e+00 1.573e-01 2.981e+02 46.819 &lt;2e-16 *** ## control_c 2.742e-01 1.149e-02 2.957e+02 23.865 &lt;2e-16 *** ## salary_c 7.295e-02 1.055e-03 8.531e+03 69.115 &lt;2e-16 *** ## s_t_ratio -4.119e-02 4.518e-03 2.980e+02 -9.116 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) cntrl_ slry_c ## control_c 0.016 ## salary_c 0.000 0.008 ## s_t_ratio -0.952 0.000 0.000 Matrix::bdiag(VarCorr(full_model)) ## 2 x 2 sparse Matrix of class &quot;dsCMatrix&quot; ## (Intercept) control_c ## (Intercept) 0.669517927 0.008950089 ## control_c 0.008950089 0.029619351 The average teaching satisfaction across all schools at average levels of curriculum control and salary and a student-teacher ratio of 0 is 7.19. (Note: this is an example of an uninterpretable zero point because there is no meaning behind a student-teacher ratio of zero. However, for demonstration purposes we’re going to keep going and ignore that issue.) A one-unit increase in control over the curriculum within a school is associated with a 0.31-unit increase in teacher job satisfaction (on the 1-10 scale), controlling for salary and student-teacher ratio. A one-unit increase in salary within school is associated with a 0.07-unit increase in job satisfaction, controlling for curriculum control and student-teacher ratio. A one-unit increase in student-teacher ratio (i.e., one more student per class; all classes in one school have the same number of students) is associated with a 0.03-unit decrease in job satisfaction, at school average salary and control over curriculum. Looking at random effect variances, the term describing how the intercepts vary across schools is 0.57. The term describing how schools’ curriculum-control slopes vary around the grand mean is 0.03. The term describing how teachers’ intercepts vary around the grand mean intercept is 0.77. The covariance between the random intercept and random slope of curriculum is 0.01, so there is a negligible relationship between intercept and slope values. Recall that interpreting these coefficients in this way — a one-unit change is associated with an X-unit change in the outcome — is also an embodiment of effect sizes. Now let’s also calculate and interpret R-squared for this model. r2mlm(full_model) ## $Decompositions ## total within between ## fixed, within 0.25683985 0.41735597 NA ## fixed, between 0.08559698 NA 0.2225596 ## slope variation 0.03301136 0.05364234 NA ## mean variation 0.29900545 NA 0.7774404 ## sigma2 0.32554636 0.52900169 NA ## ## $R2s ## total within between ## f1 0.25683985 0.41735597 NA ## f2 0.08559698 NA 0.2225596 ## v 0.03301136 0.05364234 NA ## m 0.29900545 NA 0.7774404 ## f 0.34243683 NA NA ## fv 0.37544820 0.47099831 NA ## fvm 0.67445364 NA NA With our larger model, our R-squared output is filling out. Let’s look at our Decompositions output, the top of the output printed to the console. The first column represents total variance explained. Reading from top to bottom, we can see that for this model: Fixed effects at level-1 explain 29.6% of total variance in teacher job satisfaction Fixed effects at level-2 explain 6.8% of total variance in teacher job satisfaction Random slope variation explains 3.2% of total variance in teacher job satisfaction Random intercept variation explains 25.9% of total variance in teacher job satisfaction Remaining residual variance is 34.5% of total variance in teacher job satisfaction The second column represents within variance explained. Reading from top to bottom, we can see that for this model: Fixed effects at level-1 explain 44% of within variance in teacher job satisfaction Random slope variation explains 4.7% of within variance in teacher job satisfaction Remaining residual variance is 51.2 of within variance in teacher job satisfaction The third column represents between variance explained. Reading from top to bottom, we can see that for this model: Fixed effects at level-2 explain 20.7% of between variance in teacher job satisfaction Random intercept variation explains 79.3% of between variance in teacher job satisfaction The second output printed to console, R2s, repeats this information using the f1, f2, f, v, m notation discussed above and includes the higher-level R-squared combinations. In the first column: All fixed effects at level-1 and level-2 (“f”) explain 36.4% of total variance in teacher job satisfaction All fixed effects plus random slope variation (“fv”) accounts for 40.0% of total variance in teacher job satisfaction All fixed effects and random slope variation plus random intercept variation (“fvm”) account for 65.5% of total variance in teacher job satisfaction In the second column: Fixed effects at level-2 and random slope variation (“f1v”) account for 48.8% of within variance in teacher job satisfaction A graph is printed to visualize this information and facilitate understanding and interpreting it. 11.2.5 Single Model, Manual Entry It is convenient but not always possible to use automatic entry where you feed r2mlm your model name and it extracts the necessary information and calculates the R-squareds. For example, perhaps you estimated your model in a different software like MPlus or SPSS and don’t have a model object to give the function. Or, you might have a complex model that doesn’t work with the automatic entry function (for example, containing higher-order terms created with the I() function). In cases where using the automatic function isn’t possible, there is r2mlm_manual(). It takes the following arguments: Parameter Definition For Our Example data your dataset teachsat within_covs list of numbers or variable names corresponding to the column numbers/names in your dataset for level-1 predictors our level-1 predictors are control_c and salary_c, which are in the fourth and fifth columns of our dataset, so c(4, 5) between_covs list of numbers or variable names corresponding to the column numbers/names in your dataset for level-2 predictors our level-2 predictor is student-teacher ratio, which is the eighth column in our dataset, so c(8) random_covs list of numbers or variable names corresponding to the column numbers/names in your dataset for level-1 predictors with random effects control has a random effect, so c(4) gamma_w list of fixed slope estimates for level-1 predictors in the order listed in within_covs our fixed slope estimates for the level-1 predictors are c(0.311, 0.074) gamma_b list of intercept estimate (if applicable) followed by fixed slope estimates for level-2 predictors in the order listed in between_covs our intercept and level-2 fixed slope estimates are c(7.186, -0.037) Tau random effect covariance matrix. The first row/column denotes the intercept variances and covariances; set to 0 if intercept is fixed. Subsequent rows/columns denote random slope variances and covariances in the order listed in random_covs our Tau matrix is matrix(c(0.575, 0.009, 0.009, 0.028), 2, 2) sigma2 level-1 residual variance the level-1 residual variance is 0.766 has_intercept true/false indicating whether your model estimates an intercept; default value of true our model does have an intercept, so TRUE clustermeancentered true/false indicating whether your level-1 predictors are centered-within-cluster; default value of true our level-1 predictors are centered within cluster, so TRUE bargraph indicate whether you want a bar graph we’d like to see the graph, so TRUE If we enter these values into r2mlm_manual(), we’ll get the same results as from our automatic function: r2mlm_manual(data = teachsat, within_covs = c(4, 5), between_covs = c(8), random_covs = c(4), gamma_w = c(0.311, 0.074), gamma_b = c(7.186, -0.037), Tau = matrix(c(0.575, 0.009, 0.009, 0.028), 2, 2), sigma2 = 0.766, has_intercept = TRUE, clustermeancentered = TRUE) ## $Decompositions ## total within between ## fixed, within 0.28983748 0.43323407 NA ## fixed, between 0.07015859 NA 0.2119652 ## slope variation 0.03169734 0.04737954 NA ## mean variation 0.26083243 NA 0.7880348 ## sigma2 0.34747416 0.51938639 NA ## ## $R2s ## total within between ## f1 0.28983748 0.43323407 NA ## f2 0.07015859 NA 0.2119652 ## v 0.03169734 0.04737954 NA ## m 0.26083243 NA 0.7880348 ## f 0.35999607 NA NA ## fv 0.39169341 0.48061361 NA ## fvm 0.65252584 NA NA 11.2.6 Model Comparison In some earlier chapters, we calculated proportion of variance reduced to get a sense of the impact of adding or removing a predictor using the equation \\(\\frac{variance\\ model\\ A - variance\\ model\\ B}{variance\\ model\\ A}\\). The issue with calculating variance reduced this way is it can yield impossible negative values. Model B might have “more” variance than model A at a given level. This would suggest that variance increases when adding a predictor to a model, when really the variance has been re-allocated elsewhere. We can more accurately quantify differences between models using r2mlm. To do so, we run two models and give them to the r2mlm_comp() function. Let’s compare our full model with a model without the level-2 fixed effect of student-teacher ratio. First, create a model object for this reduced-size model: reduced_model &lt;- lmer(satisfaction ~ 1 + control_c + salary_c + (1 + control_c | schoolID), data = teachsat, REML = TRUE) Then, we run r2mlm_comp(): r2mlm_comp(full_model, reduced_model) ## $`Model A R2s` ## total within between ## f1 0.25683985 0.41735597 NA ## f2 0.08559698 NA 0.2225596 ## v 0.03301136 0.05364234 NA ## m 0.29900545 NA 0.7774404 ## f 0.34243683 NA NA ## fv 0.37544820 0.47099831 NA ## fvm 0.67445364 NA NA ## ## $`Model B R2s` ## total within between ## f1 0.2569912 0.41735356 NA ## f2 0.0000000 NA 0 ## v 0.0330319 0.05364379 NA ## m 0.3842362 NA 1 ## f 0.2569912 NA NA ## fv 0.2900231 0.47099736 NA ## fvm 0.6742593 NA NA ## ## $`R2 differences, Model B - Model A` ## total within between ## f1 1.513418e-04 -2.412125e-06 NA ## f2 -8.559698e-02 NA -0.2225596 ## v 2.054085e-05 1.458584e-06 NA ## m 8.523080e-02 NA 0.2225596 ## f -8.544564e-02 NA NA ## fv -8.542509e-02 -9.535413e-07 NA ## fvm -1.942966e-04 NA NA This output contains three groups of R2s: The R2s for Model A The R2s for Model B The differences between Model A and Model B — these are what we reference to see variance explained differences between models Let’s look at the “R2 differences, Model B - Model A” output, the third group of R2s printed to the console. The first column is for the differences in total variance explained. There are minuscule differences between the models in total variance explained by level-1 fixed effects (“f1”), and random slope variation (“v”). There is a -6.8% difference in total variance explained by level-2 fixed effects (“f2”); because these differences are calculated as Model B - Model A and the result is negative, Model B explains 6.8% less total variance with level-2 fixed effects, which makes sense because we got rid of the level-2 fixed effect student-teacher ratio. That 6.8% is instead allocated to random intercept variation (“m”). Note that this is one example where our old method \\(\\frac{variance\\ model\\ A - variance\\ model\\ B}{variance\\ model\\ A}\\) might make it look like variance increases in model B, when really it is being redistributed. The second column is for within variance explained, and there are functionally no differences between models, which makes sense given that we didn’t touch our level-1 predictors that explain within-school variance, school_c and control_c. The third column represents differences in explained job satisfaction variance between schools. Model B explains 20.7% less due to level-2 fixed effects (“f2”), which again is because we removed that predictor. That variance is re-allocated to intercept variation (“m”). For visualizing these results, five graphs are also printed: A full decomposition for Model A A full decomposition for Model B A side-by-side of total variance decompositions for Model A and Model B A side-by-side of within variance decompositions for Model A and Model B A side-by-side of between variance decompositions for Model A and Model B In this case, Model A and Model B are nested. That is, all terms in Model B are in Model A, Model B is just missing one. If we were comparing two unnessted models (say, one with just salary_c as a predictor and one with just control_c as a predictor, neither model is nested within the other), we would have to provide a data argument to the function: r2mlm_comp(modelA, modelB, data). Like with the single-model r2mlm() function, there is a manual function for model comparison as well: r2mlm_comp_manual. 11.3 Conclusion In this chapter, we reviewed the definition of “effect size,” detailed a comprehensive framework for R-squared in MLMs, and practiced calculating and interpreting R-squareds using the r2mlm function. In the next (final!) chapter, we will look at the assumptions that underpin all multilevel modelling. 11.4 Additional Reading Rights, J. D., &amp; Sterba, S. K. (2019). Quantifying explained variance in multilevel models: An integrative framework for defining R-squared measures. Psychological Methods, 24(3), 309–338. https://doi.org/10.1037/met0000184 Rights, J. D., &amp; Sterba, S. K. (2020). New recommendations on the use of r-squared differences in multilevel model comparisons. Multivariate Behavioral Research, 55(4), 568–599. https://doi.org/10.1080/00273171.2019.1660605 Shaw, M., Rights, J. D., Sterba, S. K., &amp; Flake, J. K. (in-press). r2mlm: An R Package Calculating R-Squared for Multilevel Models. Behavior Research Methods. "],["12-module-12.html", "Chapter 12 Assumptions 12.1 Learning Objectives 12.2 Data Demonstration 12.3 Conclusion 12.4 Further Reading", " Chapter 12 Assumptions 12.1 Learning Objectives In this chapter we will review assumptions of MLMs and some ways to check them in R. The learning objectives for this chapter are: Understand the assumptions underpinning MLMs; Develop and interpret R code and output for testing assumptions. All materials for this chapter are available for download here. 12.2 Data Demonstration 12.2.1 Load Data and Dependencies For this data demo, we will use the following packages: library(lme4) # for multilevel models library(lmerTest) # for p-values library(dplyr) # for data manipulation library(ggplot2) # for graphing The data for chapter are a subsample from the 1982 High School Beyond data collected by the National Center for Educational Statistics, used as example data throughout Raudenbush, S. W., &amp; Bryk, A. S. (2002). Hierarchical Linear Models: Applications and Data Analysis Methods: SAGE Publications. These data are related to student math achievement. data &lt;- read.csv(&#39;rb2002.csv&#39;) 12.2.2 Assumptions of MLMs In brief, the assumptions underlying MLMs are as follows: The model is correctly specified (i.e., all the predictors associated with the outcome and relevant random effects are included); The functional form is correct (e.g., the relationship between the predictors and outcome is linear if using a linear model); Level-1 residuals are independent and normally distributed; Level-2 residuals are independent and multivariate normally distributed; Residuals at level-1 and level-2 are unrelated; Predictors at one level are not related to errors at another level (homoscedasticity). In the data demonstration that follows, we will be showing some techniques for checking these assumptions. However, we note that there are many other ways to conduct similar checks. Assumption checks are data exploration at the end of the day. If you know and prefer other methods of exploring the same facets of your data, go for it! 12.2.3 Assumption 1: Model Specification Specifying your model means determining which predictors should be included and what parameters to estimate. A correctly-specified model is one that includes everything associated with your outcome and nothing not-associated. This is impossible to know for certain and most researchers are constrained by the resources they have to get data and, ultimately, by the data they have. A general approach for considering this is trying to include all possible relevant IVs when predicting a DV. We recommend a three step approach. (1) Start with a null model to check the extent of clustering. (2) Run your “core” model, what you expect to be included according to theory. (3) Run a maximal model with all possible random effects, and trim down as you see effects are zero. Comparing models with and without effects and reviewing the literature for important variables to consider are decent strategies here. Get experts to read your results and their ideas on if anything is missing! 12.2.4 Assumption 2: Functional Form is Correct The functional form of your model refers to the function connecting your predictors and outcome. In linear models like MLMs, that form is (you guessed it!) linear. However, you can use MLMs for non-linear data by specifying other functions, which are not covered in the current set of modules. In our example data, let’s look at the relationship between an outcome of math achievement (mathach) and a predictor of socioeconomic status (ses). We can get a sense of whether that relationship is linear with a scatterplot: data %&gt;% ggplot(mapping = aes(x = ses, y = mathach)) + geom_point() ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`). The scatterplot is hard to read because we have over 11,000 observations in our data. One option to improve readability is reducing the opacity of each point to get a clearer picture of how many points there are: data %&gt;% ggplot(mapping = aes(x = ses, y = mathach)) + geom_point(alpha = .2) ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`). It looks loosely linear. Before we continue, for teaching purposes we’ll take a subset of 30 schools to make plotting easier for the rest of this data demo. For your data, you should consider all observations; this is just to make examples clearer. data &lt;- data %&gt;% filter(SCHOOL %in% head(unique(SCHOOL), n = 30)) Let’s take a look at the relationship between SES and math achievement for each of the 30 schools in our subsetted data. data %&gt;% ggplot() + geom_point(mapping = aes(x = ses, y = mathach)) + facet_wrap(~ SCHOOL) Looks like we have vaguely linear relationships; nothing looks quadratic. This is a subjective determination, be open to other, non-linear functions if a straight line doesn’t appear to do the job. Finally, let’s graph all points on one plane again but with regression lines overlaid for each school. data %&gt;% group_by(SCHOOL) %&gt;% ggplot(mapping = aes(x = ses, y = mathach, colour = factor(SCHOOL))) + geom_point(show.legend = FALSE) + geom_smooth(method = lm, se = FALSE, show.legend = FALSE, fullrange = TRUE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; This isn’t an explicit test of functional form, but it looks like there is variance in intercepts and slopes across schools that merits multilevel modelling (or cluster-robust standard errors, if you don’t have any multilevel research questions). 12.2.5 An Aside: Extracting Residuals In linear regression, we have the assumption of homoscedasticity. That is, the variance of residuals is roughly the same at any value of of your predictor(s). In multilevel models, we have two classes of residuals: level-1 residuals and level-2 residuals. The remaining assumptions all deal with these residuals: that they are normally distributed, that they don’t relate to each other, that they don’t relate to predictors at the same level or at the other level. We’re going to look at a model with math achievement predicted by SES centered within cluster (school) and mean SES predicting the intercept and slope of SES centered within cluster. We’ll estimate a random effect variance for the slope of SES centered within cluster. We’re not estimating the random effect covariance. Level Equation Level 1 \\(mathach_{ij} = \\beta_{0j} + \\beta_{1j}cwc\\_ses + R_{ij}\\) Level 2 \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{10}ses\\_mean + U_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}ses\\_mean + U_{1j}\\) Combined \\(math_{ij} = \\gamma_{00} + \\gamma_{01}ses\\_mean + \\gamma_{10}cwc\\_ses + \\gamma_{11}cwc\\_ses*ses\\_mean + U_{0j} + U_{1j}cwc\\_ses + R_{ij}\\) model &lt;- lmer(mathach ~ CWCses*ses_mean + (1|SCHOOL) + (0 + CWCses|SCHOOL), data = data, REML = TRUE) summary(model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: mathach ~ CWCses * ses_mean + (1 | SCHOOL) + (0 + CWCses | SCHOOL) ## Data: data ## ## REML criterion at convergence: 8555.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.87105 -0.70607 -0.03647 0.76336 2.77351 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SCHOOL (Intercept) 1.990 1.411 ## SCHOOL.1 CWCses 1.379 1.174 ## Residual 35.315 5.943 ## Number of obs: 1329, groups: SCHOOL, 30 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.8243 0.3078 28.9279 41.661 &lt; 2e-16 *** ## CWCses 1.9581 0.3412 27.3088 5.739 4.05e-06 *** ## ses_mean 6.5269 0.6499 28.8676 10.044 6.28e-11 *** ## CWCses:ses_mean 1.5239 0.7253 27.7253 2.101 0.0448 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) CWCses ses_mn ## CWCses 0.000 ## ses_mean -0.026 0.000 ## CWCss:ss_mn 0.000 0.030 0.000 Let’s extract the level-1 residuals and add them to our data to work with moving forward: data$l1resid &lt;- residuals(model) head(data$l1resid) ## [1] -1.9478535 10.1937749 10.7268788 -0.5893637 7.6105198 -6.0281684 We’ll extract the level-2 residuals shortly. 12.2.6 Assumption 3: Level-1 Residuals are Independent and Normally Distributed 12.2.6.1 Independent Our level-1 residuals should be independent of our level-1 predictors. For our example, if our level-1 residuals (\\(\\sigma^2\\)) correlate to SES centered within cluster, we have heteroscedasticity, which we don’t want. We can check this visually by creating a scatterplot of the predictor and the residuals: data %&gt;% ggplot(mapping = aes(x = CWCses, y = l1resid)) + geom_point() + labs(x = &quot;CWCses&quot;, y = &quot;residuals&quot;) Those don’t look correlated; they look like a blob. We can check this statistically by calculating the correlation. cor.test(data$l1resid, data$CWCses) ## ## Pearson&#39;s product-moment correlation ## ## data: data$l1resid and data$CWCses ## t = -5.7568e-15, df = 1327, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.05377213 0.05377213 ## sample estimates: ## cor ## -1.580333e-16 That correlation is essentially zero, meaning that our level-1 residuals and level-1 predictor are indeed uncorrelated! 12.2.6.2 Normally Distributed We can check whether our level-1 residuals are normally distributed with a histogram: data %&gt;% ggplot(mapping = aes(x = l1resid)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Those look normal-ish — there’s maybe something odd going on in that the distribution is bimodal, but that can be influenced by the number of bins. For example, with fewer bins: data %&gt;% ggplot(mapping = aes(x = l1resid)) + geom_histogram(bins = 15) Let’s clear this up with a quantile-quantile (QQ) plot: data %&gt;% ggplot(mapping = aes(sample = l1resid)) + stat_qq() This plots quantiles for two distributions against each other: what proportion of points in our distribution is in a given quantile (y-axis), and how does that compare to the proportion of points in a given quantile for our theoretical normal distribution (x-axis). The line is straight, which indicates that our distribution is roughly normal. 12.2.7 Assumption 4: Level-2 Residuals are Independent and Multivariate Normal To check our level-2 residual assumptions, we need to extract the level-2 residuals and add them to our data. This requires a little more work than extracting the level-1 residuals. We could easily add level-1 residuals to our dataset because every row is a person and had a residual. Level-2 residuals are at the school level, so the vector of residuals cannot be appended as easily. Let’s create a level-2 dataset to work with instead. l2_data &lt;- data %&gt;% group_by(SCHOOL) %&gt;% # group data by clustering variable, school mutate( mathach_mean = mean(mathach) # create mean math achievement per school ) %&gt;% select(SCHOOL, ses_mean, mathach_mean) %&gt;% unique() # select unique rows (rather than having school, ses_mean, and mathach_mean repeating over and over again) Then let’s add the level-2 residuals to this dataset. We get two columns of residuals with the ranef(model)$SCHOOL command; the first column is intercept residuals \\(U_{0j}\\)s, the second column the slope residuals \\(U_{1j}\\)s. Matrices are indexed as [row, column], so we use the [, 1] code to get all rows from column 1 and [, 2] to get all rows from column 2. l2_data$intercept_resid = ranef(model)$SCHOOL[, 1] l2_data$slope_resid = ranef(model)$SCHOOL[, 2] 12.2.7.1 Independent Like with level-1 residuals, we want our level-2 residuals to be independent from level-2 predictors. We also want both our level-2 residuals — intercept and slope residuals — to be independent of each other (which was not an issue at level-1, given that we only had one category of residual, \\(\\sigma^2\\)). We can check whether our level-2 residuals are independent from the level-2 predictors and from one another as we did before: with a scatterplot and/or by calculating the correlation among them. First, let’s look at the relationship between the intercept residuals and the level-2 variable of mean SES. l2_data %&gt;% ggplot(mapping = aes(x = intercept_resid, y = ses_mean)) + geom_point() And the correlation: cor.test(l2_data$ses_mean, l2_data$intercept_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: l2_data$ses_mean and l2_data$intercept_resid ## t = 1.3316e-14, df = 28, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3602692 0.3602692 ## sample estimates: ## cor ## 2.516581e-15 Those look uncorrelated! Next, let’s check that the slope residuals are independent from the predictor of mean SES: l2_data %&gt;% ggplot(mapping = aes(x = slope_resid, y =ses_mean)) + geom_point() cor.test(l2_data$ses_mean, l2_data$slope_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: l2_data$ses_mean and l2_data$slope_resid ## t = 5.6841e-15, df = 28, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3602692 0.3602692 ## sample estimates: ## cor ## 1.074192e-15 Those also look uncorrelated! Finally, let’s check whether the level-2 residuals are independent of each other: l2_data %&gt;% ggplot(mapping = aes(x = slope_resid, y = intercept_resid)) + geom_point() cor.test(l2_data$intercept_resid, l2_data$slope_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: l2_data$intercept_resid and l2_data$slope_resid ## t = -3.9046, df = 28, p-value = 0.0005424 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.7859072 -0.2970177 ## sample estimates: ## cor ## -0.5937517 Our model assumes that intercept and slope residuals are independent, but that doesn’t seem to be the case. We could relax this by adding the covariance back in. 12.2.7.2 Multivariate Normal As we did with our level-1 residuals, we can check the normality of our level-2 residuals with histograms and QQ plots. First, intercept residuals: l2_data %&gt;% ggplot(mapping = aes(x = intercept_resid)) + geom_histogram(binwidth = .75) l2_data %&gt;% ggplot(mapping = aes(sample = intercept_resid)) + stat_qq() Those look a bit wonky. When residuals are not normal, the assumption that the data are random doesn’t hold, so that’s a big issue. You might need to revisit your sampling strategy or your model: are predictors missing? You can read more in Pek, Wong, and Wong (2018). Next, our slope residuals: l2_data %&gt;% ggplot(mapping = aes(x = slope_resid)) + geom_histogram(binwidth = .50) l2_data %&gt;% ggplot(mapping = aes(sample = slope_resid)) + stat_qq() Similar issue: wonky and potentially concerning. 12.2.8 Assumption 5: Residuals at Level-1 and Level-2 are Independent Our residuals at level-1 and level-2 should not be related to each other. We can check that with scatterplots and calculating correlations. To do so, we first need to add our level-2 residuals to our level-1 dataset. As mentioned, each school only has one level-2 intercept residual and one level-2 slope residual, but as many level-1 residuals as there are students in that school To add our level-2 residuals to the dataset, then, we need to replicate them X times, where X is the number of students in a school. For example, if a school has 10 students, we need to repeat the level-2 residuals 10 times. (This is how level-2 predictors are encoded, too: for a school with 10 students, the mean values of SES for that school repeats 10 times in the dataset.) To do that, let’s use the replication function rep with the times argument to repeat each value X times. We can find the number of students per school as follows: n_per_school &lt;- data %&gt;% group_by(SCHOOL) %&gt;% # group by school select(SCHOOL) %&gt;% # we just want to count schools count() %&gt;% ungroup() %&gt;% select(n) %&gt;% unlist() Then let’s create vectors of level-1 and level-2 residuals that repeat X times: data$intercept_resid &lt;- rep(l2_data$intercept_resid, times = n_per_school) data$slope_resid &lt;- rep(l2_data$slope_resid, times = n_per_school) Now that we have a dataset with all of our residuals, let’s assess whether the level-1 and level-2 residuals are correlated with our trusty scatterplots and correlations. First, level-2 intercept residuals and level-1 residuals. data %&gt;% ggplot(mapping = aes(x = l1resid, y = intercept_resid)) + geom_point() cor.test(data$l1resid, data$intercept_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: data$l1resid and data$intercept_resid ## t = 2.7251, df = 1327, p-value = 0.006512 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.02091225 0.12785972 ## sample estimates: ## cor ## 0.07460049 This looks odd at first, but the striation is because for any one level-2 residual there are multiple level-1 residuals. This looks decent; for each level-2 residual, there seems to be a decent spread of level-1 residuals. We would be concerned if level-1 residuals were clustering so that, say, for lower level-2 residuals there were also lower level-1 residuals or vice versa. Our correlation is also quite small, emphasizing that things seem fine. Next, level-1 residuals with level-2 slope residuals: data %&gt;% ggplot(mapping = aes(x = l1resid, y = slope_resid)) + geom_point() cor.test(data$l1resid, data$slope_resid) ## ## Pearson&#39;s product-moment correlation ## ## data: data$l1resid and data$slope_resid ## t = -1.666, df = 1327, p-value = 0.09595 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.099214249 0.008106189 ## sample estimates: ## cor ## -0.04568585 This similarly looks fine. 12.2.9 Assumption 6: Level-1 Residuals Independent of Level-2 Predictors, Level-2 Residuals Independent of Level-1 Predictors We’ve tested that our residuals are independent of predictors at their same level — level-1 residuals with ses_cwc and level-2 residuals with ses_mean. We also want residuals to be independent of predictors at the other level — level-1 residuals independent of ses_mean and level-2 residuals independent of ses_cwc. How do we test this? Scatterplots and correlations, of course. Let’s check level-1 residuals first: data %&gt;% ggplot(mapping = aes(x = l1resid, y = ses_mean)) + geom_point() cor.test(data$l1resid, data$ses_mean) ## ## Pearson&#39;s product-moment correlation ## ## data: data$l1resid and data$ses_mean ## t = 1.1447e-14, df = 1327, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.05377213 0.05377213 ## sample estimates: ## cor ## 3.142438e-16 The scatterplot looks like there might be a pattern where lower values on ses_mean are associated with higher residuals, but the correlation belies this. Next level-2 intercept residuals: data %&gt;% ggplot(mapping = aes(x = intercept_resid, y = CWCses)) + geom_point() cor.test(data$intercept_resid, data$CWCses) ## ## Pearson&#39;s product-moment correlation ## ## data: data$intercept_resid and data$CWCses ## t = -2.3158e-15, df = 1327, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.05377213 0.05377213 ## sample estimates: ## cor ## -6.357262e-17 Looks good! And finally level-2 slope residuals: data %&gt;% ggplot(mapping = aes(x = slope_resid, y = CWCses)) + geom_point() cor.test(data$slope_resid, data$CWCses) ## ## Pearson&#39;s product-moment correlation ## ## data: data$slope_resid and data$CWCses ## t = -2.0627e-15, df = 1327, p-value = 1 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.05377213 0.05377213 ## sample estimates: ## cor ## -5.662342e-17 Looks great! 12.3 Conclusion In this chapter, we reviewed the assumptions underlying multilevel models and examined some methods to assess them. This is the last chapter of the book! We’ve covered the fundamentals: when and why you would use a multilevel model, models of varying complexity and completeness, and considerations as you build your models like estimation, various data structures, and interpreting effect sizes. Thank you for reading. If you have any suggestions or feedback, feel free to get in touch on Twitter (Mairead, Jessica) or on GitHub. 12.4 Further Reading Pek, J., Wong, O., &amp; Wong, A. C. M. (2018). How to Address Non-normality: A Taxonomy of Approaches, Reviewed, and Illustrated. Frontiers in Psychology, 9. doi:10.3389/fpsyg.2018.02104 Snijders, T. A. B., &amp; Bosker, R. J. (2011). Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling: SAGE Publications. "],["13-appendix.html", "A Download Materials", " A Download Materials Chapter Data R Script Worksheet 2 heck2011.csv 02-multiple-regression.R Linear Regression Review 3 heck2011.csv 03-module-3.R Approaches to Multilevel Data 4 heck2011.csv 04-module-4.R Our First MLM: The Null Model 5 heck2011.csv 05-module-5.R Adding Fixed Predictors 6 heck2011.csv 06-module-6.R Random Effects and Cross-level Interactions 7 heck2011.csv 07-module-7.R Estimation Options and Troubleshooting 8 heck2011.csv 08-module-8.R Centering 9 hoffman2007.csv 09-module-9.R Repeated Measures 10 casto2016.csv 10-module-10.R Longitudinal Measures 11 teachsat.csv 11-module-11.R Effect Sizes in MLMs 12 rb2002.csv 12-module-12.R Assumptions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
